{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082587ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e00daea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep this here for https://github.com/ray-project/ray/issues/11547\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Replace above line with:\n",
    "from ray.tune.sklearn import TuneGridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff29d44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.kernel_approximation import Nystroem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c36154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.model_selection import PredefinedSplit\n",
    "# X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "# y = np.array([0, 0, 1, 1])\n",
    "# test_fold = [-1, -1, -1, 1]\n",
    "# ps = PredefinedSplit(test_fold)\n",
    "# ps.get_n_splits()\n",
    "\n",
    "# print(ps)\n",
    "\n",
    "# for train_index, test_index in ps.split():\n",
    "#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "#     X_train, X_test = X[train_index], X[test_index]\n",
    "#     y_train, y_test = y[train_index], y[test_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3151c880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineer(data):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    data: pandas.DataFrame that must have specific columns.\n",
    "\n",
    "    \"\"\"\n",
    "    # Bid-Ask spread: (Ask - Bid) / Ask\n",
    "    data[\"best_bid\"] = (data[\"best_offer\"] - data[\"best_bid\"]) / (data[\"best_offer\"])\n",
    "    data = data.rename(columns={\"best_bid\": \"ba_spread_option\"}).drop([\"best_offer\"], axis=1)\n",
    "\n",
    "    # Gamma: multiply by spotprice and divide by 100\n",
    "    data[\"gamma\"] = data[\"gamma\"] * data[\"spotprice\"] / 100 #following Bali et al. (2021)\n",
    "\n",
    "    # Theta: scale by spotprice\n",
    "    data[\"theta\"] = data[\"theta\"] / data[\"spotprice\"] #following Bali et al. (2021)\n",
    "\n",
    "    # Vega: scale by spotprice\n",
    "    data[\"vega\"] = data[\"vega\"] / data[\"spotprice\"] #following Bali et al. (2021)\n",
    "\n",
    "    # Time to Maturity: cale by number of days in year: 365\n",
    "    data[\"days_to_exp\"] = data[\"days_to_exp\"] / 365\n",
    "\n",
    "    # Moneyness: Strike / Spot (K / S)\n",
    "    data[\"strike_price\"] = data[\"strike_price\"] / data[\"spotprice\"] # K / S\n",
    "    data = data.rename(columns={\"strike_price\": \"moneyness\"})\n",
    "\n",
    "    # Forward Price ratio: Forward / Spot\n",
    "    data[\"forwardprice\"] = data[\"forwardprice\"] / data[\"spotprice\"]\n",
    "\n",
    "    # Drop redundant/ unimportant columns\n",
    "    data = data.drop([\"cfadj\", \"days_no_trading\", \"spotprice\", \"adj_spot\"], axis=1)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd83d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary y label generator\n",
    "def binary_categorize(y):\n",
    "    \"\"\"\n",
    "    Input: continuous target variable \n",
    "\n",
    "    Output: 1 for positive returns, \n",
    "            0 for negative returns\n",
    "    \"\"\"\n",
    "    if y > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "# multiclass y label generator\n",
    "def multi_categorize(y):\n",
    "    \"\"\"\n",
    "    Input: continuous target variable\n",
    "    CAREFUL: classes have to be between [0, C) for F.crossentropyloss.\n",
    "    \n",
    "    Output: multi class\n",
    "    \"\"\"\n",
    "    if y > 0.05:\n",
    "        return 2\n",
    "    elif y < -0.05:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaea3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVSplitter:\n",
    "    \"\"\" Generator for data splits\n",
    "    Args:\n",
    "    dates: pandas.Series of datetime,\n",
    "    init_train_length: int,\n",
    "    val_length: int\n",
    "    \"\"\"\n",
    "    def __init__(self, dates, init_train_length=1, val_length=2, test_length=1):\n",
    "        # find indeces where years change (will ignore last year end in dates)\n",
    "        self.val_length = val_length\n",
    "        self.test_length = test_length\n",
    "        self.eoy_idx =  np.where((dates.dt.year.diff() == 1))[0]\n",
    "        self.eoy_idx = np.append(self.eoy_idx, len(dates)) #append end of year of last year in dates\n",
    "\n",
    "        assert init_train_length + val_length + test_length <= len(self.eoy_idx) + 1, \\\n",
    "        \"defined train and val are larger than number of years in dataset\"\n",
    "        assert init_train_length > 0, \"init_train_length must be strictly greater than 0\"\n",
    "\n",
    "        # align the 4th idx to be the end of the 5th year...\n",
    "        self.train_start_idx = init_train_length - 1\n",
    "\n",
    "        self.train_eoy = self.eoy_idx[self.train_start_idx:-(val_length+test_length)]\n",
    "        self.val_eoy = self.eoy_idx[self.train_start_idx + val_length:-test_length]\n",
    "        # For generate_idx():\n",
    "        self.test_eoy = self.eoy_idx[self.train_start_idx + val_length + test_length:]\n",
    "\n",
    "    def generate(self):\n",
    "        for i in range(len(self.eoy_idx) - (self.train_start_idx + self.val_length)):\n",
    "            yield (list(range(self.train_eoy[i])),\n",
    "                   list(range(self.train_eoy[i], self.val_eoy[i])))\n",
    "\n",
    "    def generate_idx(self):\n",
    "        for i in range(len(self.eoy_idx) - (self.train_start_idx + self.val_length \n",
    "                        + self.test_length)):\n",
    "            yield ({\"train\": self.train_eoy[i], \n",
    "                    \"val\": self.val_eoy[i], \n",
    "                    \"test\": self.test_eoy[i]}\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787b79c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from disk\n",
    "path = Path(r\"C:\\Users\\Mathiass\\OneDrive - Universität Zürich UZH\\Documents\\mt_literature\\data\")\n",
    "\n",
    "class Dataset():\n",
    "    def __init__(self, path=path, year_idx=0, dataset=\"small\", init_train_length=20, val_length=2, label_fn=\"binary\"):\n",
    "        if dataset == \"small\":\n",
    "            self.data = pd.read_parquet(path/\"final_df_filledmean_small.parquet\")\n",
    "        elif dataset == \"big\":\n",
    "            self.data = pd.read_parquet(path/\"final_df_filledmean.parquet\")\n",
    "        else:\n",
    "            raise ValueError(\"Specify dataset as either 'small' or 'big'\")\n",
    "\n",
    "        # get splits\n",
    "        splitter = CVSplitter(self.data[\"date\"], init_train_length=init_train_length, \n",
    "                                val_length=val_length, test_length=1)\n",
    "        eoy_indeces = list(splitter.generate_idx())\n",
    "        self.eoy_train = eoy_indeces[year_idx][\"train\"]\n",
    "        self.eoy_val = eoy_indeces[year_idx][\"val\"]\n",
    "        self.eoy_test = eoy_indeces[year_idx][\"test\"]\n",
    "        \n",
    "        # Truncate data\n",
    "        self.data = self.data.iloc[:self.eoy_test]\n",
    "        assert len(self.data) == self.eoy_test, \"length of data is not equal to eoy_test\"\n",
    "            \n",
    "        # feature engineer data\n",
    "        self.data = feature_engineer(self.data)\n",
    "        \n",
    "        # create y\n",
    "        self.y = self.data[\"option_ret\"]\n",
    "        # make classification problem\n",
    "        if label_fn == \"binary\":\n",
    "            self.y = self.y.apply(binary_categorize)\n",
    "        elif label_fn == \"multi\":\n",
    "            self.y = self.y.apply(multi_categorize)\n",
    "        else:\n",
    "            raise ValueError(\"Specify label_fn as either 'binary' or 'multi'\")\n",
    "        # create X\n",
    "        self.X = self.data.drop([\"option_ret\"], axis=1)\n",
    "        \n",
    "        # save dates and drop\n",
    "        self.dates = self.X[\"date\"]\n",
    "        self.X = self.X.drop([\"date\"], axis=1)\n",
    "        \n",
    "#         # to torch Tensor\n",
    "#         self.X = torch.from_numpy(self.X.values).float() #-> will be standardized in setup, so do it there.\n",
    "#         self.y = torch.from_numpy(self.y.values)\n",
    "\n",
    "        # to numpy\n",
    "        self.X = self.X.values #-> will be standardized in setup, so do it there.\n",
    "        self.y = self.y.values\n",
    "    \n",
    "        ############################### setup #########################################################\n",
    "        # train\n",
    "        self.X_train = self.X[:self.eoy_train]\n",
    "        self.y_train = self.y[:len(self.X_train)]\n",
    "        \n",
    "        #val\n",
    "        self.X_val = self.X[self.eoy_train:self.eoy_val]\n",
    "        self.y_val = self.y[len(self.X_train):len(self.X_train)+len(self.X_val)]\n",
    "        \n",
    "        # test\n",
    "        self.X_test = self.X[self.eoy_val:self.eoy_test]\n",
    "        self.y_test = self.y[-len(self.X_test):]\n",
    "        \n",
    "        assert (len(self.X_train)+len(self.X_val)+len(self.X_test)) == len(self.data), \\\n",
    "            \"sum of X train, val, test is not equal length of dataset\"\n",
    "        assert (len(self.y_train)+len(self.y_val)+len(self.y_test) == len(self.data)), \\\n",
    "        \"sum of y train, val, test is not equal to length of dataset\"\n",
    "        \n",
    "#         #standardize X_train\n",
    "#         mean = torch.mean(self.X_train, axis=0)\n",
    "#         std = torch.std(self.X_train, axis=0)\n",
    "        \n",
    "#         # Standardize X_train, X_val and X_test with mean/std from X_train\n",
    "#         self.X_train = (self.X_train - mean) / std\n",
    "#         self.X_val = (self.X_val - mean) / std\n",
    "#         self.X_test = (self.X_test - mean) / std\n",
    "\n",
    "        # Save variables\n",
    "        # input dim\n",
    "        self.input_dim = self.X_train.shape[1]\n",
    "        # number of classes\n",
    "        self.num_classes = len(np.unique(self.y_train))\n",
    "#         class weights\n",
    "        self.class_weights = len(self.y_train) / np.unique(self.y_train, return_counts=True)[1]\n",
    "        \n",
    "        print(\"*****************************************************************************************\")\n",
    "        print(\"Current dataset information:\")\n",
    "        print(\"---\")\n",
    "        print(\"class_weights:\", self.class_weights)\n",
    "        print(\"---\")\n",
    "        print(f\"# of input data: {len(self.data)} with shape: {self.data.shape}\")\n",
    "        print(f\"# of training samples: {len(self.y_train)} with X_train of shape: {self.X_train.shape}\")\n",
    "        print(f\"# of validation samples: {len(self.y_val)} with X_val of shape: {self.X_val.shape}\")\n",
    "        print(f\"# of test samples: {len(self.y_test)} with X_test of shape: {self.X_test.shape}\")\n",
    "        print(\"---\")\n",
    "        print(f\"train start date: \", self.dates.iloc[0].strftime(\"%Y-%m-%d\"), \n",
    "              \", train end date: \", self.dates.iloc[:self.eoy_train].iloc[-1].strftime(\"%Y-%m-%d\"))\n",
    "        print(f\"val start date: \", self.dates.iloc[self.eoy_train:self.eoy_val].iloc[0].strftime(\"%Y-%m-%d\"), \n",
    "              \", val end date: \", self.dates.iloc[self.eoy_train:self.eoy_val].iloc[-1].strftime(\"%Y-%m-%d\"))\n",
    "        print(f\"test start date: \", self.dates.iloc[self.eoy_val:self.eoy_test].iloc[0].strftime(\"%Y-%m-%d\"), \n",
    "              \", test end date: \", self.dates.iloc[self.eoy_val:self.eoy_test].iloc[-1].strftime(\"%Y-%m-%d\"))\n",
    "        print(\"*****************************************************************************************\")\n",
    "        \n",
    "    def get_datasets(self):\n",
    "        return self.X_train, self.X_val, self.X_test\n",
    "    \n",
    "    def get_cv_data(self):\n",
    "        # careful: if predicting on X_val later... -> cheating\n",
    "        X = np.concatenate((self.X_train, self.X_val))\n",
    "        y = np.concatenate((self.y_train, self.y_val))\n",
    "        ps = PredefinedSplit(np.concatenate((np.zeros(len(self.X_train)) - 1, np.ones(len(self.X_val)))))\n",
    "        \n",
    "        assert (self.X_train.shape[0] + self.X_val.shape[0] == X.shape[0] and (self.X_train.shape[1] == self.X_val.shape[1] == X.shape[1]))\n",
    "        assert ps.get_n_splits() == 1, \"more than one train/ val split in PredefinedSplit\"\n",
    "        \n",
    "        return X, y, ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47345518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data instance\n",
    "data = Dataset()\n",
    "# get X, y and train val split\n",
    "X_train_val, y_train_val, train_val_split = data.get_cv_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce64cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aabeedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ceil(10**6 / 3428810)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af23735",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9879047",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c15e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = \"passthrough\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b3835f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "weights = compute_class_weight('balanced', classes=np.unique(data.y_train), y=data.y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d2a250",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.unique(data.y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3fd67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate \"balanced\" class weights manually (class_weight=\"balanced\" not possible for TuneSearch)\n",
    "class_weights = {}\n",
    "for i in range(len(labels)):\n",
    "    class_weights[labels[i]] = weights[i] \n",
    "\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be008387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # class CustomSGDClassifier(SGDClassifier):\n",
    "# #     def __init__(self, *args, **kwargs):\n",
    "#         super().__init__(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42723d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SGDClassifier(\n",
    "    random_state=42,\n",
    "#     class_weight=\"balanced\", #string method not implemented in ray tune\n",
    "    class_weight=class_weights,\n",
    "    max_iter=500000, # ignored as well?\n",
    "#     tol=10000000000000000,\n",
    "#     n_iter_no_change=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9818d0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "nystroem = Nystroem(gamma=0.2, random_state=1, n_components=100, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70873e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = LogisticRegression(random_state=0, \n",
    "#                          class_weight=\"balanced\",\n",
    "#                            max_iter=1000,\n",
    "# #                          n_jobs=-1,\n",
    "# #                          C=0.0001,\n",
    "#                         )\n",
    "\n",
    "# clf = SVC(random_state=0,\n",
    "#          class_weight=\"balanced\",\n",
    "#          cache_size=2000,\n",
    "#          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a60c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d1ed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "starttime = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "log_dir = \"./logs/tune/lr_loops\"\n",
    "train_year_end = 1996 + 10 + 0 - 1\n",
    "val_year_end = train_year_end + 2\n",
    "years = f\"train{train_year_end}_val{val_year_end}\"\n",
    "name = starttime+\"\\\\\"+ years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9d31f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = Pipeline([\n",
    "    ('scaler', scaler),\n",
    "    (\"nystroem\", nystroem),\n",
    "    ('pca' , pca),\n",
    "    ('clf', clf),\n",
    "])\n",
    "\n",
    "# Example parameters to tune from SGDClassifier\n",
    "parameter_grid = [{\n",
    "#     \"pca\": [\"passthrough\", PCA(), PCA(10)],\n",
    "    \"clf__loss\": [\"hinge\"],\n",
    "    \"nystroem__n_components\": [20],\n",
    "    \"pca__n_components\": [5, 10],\n",
    "#     \"clf__alpha\": [1e-6, 1e-3, 1, 100, 10000], \n",
    "#     \"clf__C\": np.logspace(-5, 5, 5),\n",
    "#     \"clf__epsilon\": [0.01, 0.1]\n",
    "}, \n",
    "# {\n",
    "#     \"clf__loss\": [\"hinge\"],\n",
    "#     \"nystroem__n_components\": [20],\n",
    "#     \"pca__n_components\": [5],\n",
    "# }\n",
    "]\n",
    "\n",
    "tune_search = TuneGridSearchCV(\n",
    "    clf,\n",
    "    parameter_grid,\n",
    "    cv=train_val_split,\n",
    "    early_stopping=True, # early stopping of ASHA\n",
    "    max_iters=1,\n",
    "    scoring=[\"accuracy\", \"balanced_accuracy\"],\n",
    "    refit=\"balanced_accuracy\",\n",
    "    n_jobs=2, #how many trials in parallel\n",
    "    verbose=2,\n",
    "    local_dir=log_dir,\n",
    "    name=name,\n",
    "    return_train_score=True # can be comp. expensive\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "tune_search.fit(X_train_val, y_train_val)\n",
    "end = time.time()\n",
    "print(\"Tune GridSearch Fit Time:\", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d62f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 022-07-18 15:00:39,894\tINFO tune.py:747 -- Total run time: 16.08 seconds (14.64 seconds for the tuning loop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5dc231",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tune_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b763eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'params': [{'clf__loss': 'hinge',\n",
    "#    'nystroem__n_components': 20,\n",
    "#    'pca__n_components': 10},\n",
    "#   {'clf__loss': 'hinge',\n",
    "#    'nystroem__n_components': 20,\n",
    "#    'pca__n_components': 5}],\n",
    "#  'split0_test_accuracy': array([0.45120242, 0.42342386]),\n",
    "#  'mean_test_accuracy': array([0.45120242, 0.42342386]),\n",
    "#  'std_test_accuracy': array([0., 0.]),\n",
    "#  'rank_test_accuracy': array([1, 2]),\n",
    "#  'split0_test_balanced_accuracy': array([0.52078709, 0.51714468]),\n",
    "#  'mean_test_balanced_accuracy': array([0.52078709, 0.51714468]),\n",
    "#  'std_test_balanced_accuracy': array([0., 0.]),\n",
    "#  'rank_test_balanced_accuracy': array([1, 2]),\n",
    "#  'split0_train_accuracy': array([0.44659375, 0.42329739]),\n",
    "#  'mean_train_accuracy': array([0.44659375, 0.42329739]),\n",
    "#  'std_train_accuracy': array([0., 0.]),\n",
    "#  'rank_train_accuracy': array([1, 2]),\n",
    "#  'split0_train_balanced_accuracy': array([0.52470044, 0.51906457]),\n",
    "#  'mean_train_balanced_accuracy': array([0.52470044, 0.51906457]),\n",
    "#  'std_train_balanced_accuracy': array([0., 0.]),\n",
    "#  'rank_train_balanced_accuracy': array([1, 2]),\n",
    "#  'time_total_s': array([23.16746497, 19.01607275]),\n",
    "#  'training_iteration': array([1, 1], dtype=int64),\n",
    "#  'param_clf__loss': masked_array(data=['hinge', 'hinge'],\n",
    "#               mask=[False, False],\n",
    "#         fill_value='?',\n",
    "#              dtype=object),\n",
    "#  'param_nystroem__n_components': masked_array(data=[20, 20],\n",
    "#               mask=[False, False],\n",
    "#         fill_value='?',\n",
    "#              dtype=object),\n",
    "#  'param_pca__n_components': masked_array(data=[10, 5],\n",
    "#               mask=[False, False],\n",
    "#         fill_value='?',\n",
    "#              dtype=object)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3670b4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf72f03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_best_score(gs):\n",
    "#     \"\"\"returns best scores of gridsearch object in dictionary\"\"\"\n",
    "#     dic = {}\n",
    "#     dic[\"val_acc\"] = gs.cv_results_[\"mean_test_accuracy\"][gs.best_index_]\n",
    "#     dic[\"val_bal_acc\"] = gs.cv_results_[\"mean_test_balanced_accuracy\"][gs.best_index_]\n",
    "    \n",
    "#     if \"mean_train_accuracy\" in gs.cv_results_:\n",
    "#         dic[\"train_acc\"] = gs.cv_results_[\"mean_train_accuracy\"][gs.best_index_]\n",
    "#     if \"mean_train_balanced_accuracy\" in gs.cv_results_:\n",
    "#         dic[\"train_bal_acc\"] = gs.cv_results_[\"mean_train_balanced_accuracy\"][gs.best_index_]\n",
    "    \n",
    "#     return dic\n",
    "        \n",
    "\n",
    "# def get_metric_order(nested_dic):\n",
    "#     order = list(nested_dic[list(nested_dic.keys())[0]].keys())\n",
    "#     metric_order = [\"val_bal_acc\", \"train_bal_acc\", \"val_acc\", \"train_acc\", \"val_loss\", \"train_loss\"]\n",
    "#     for m in metric_order[::-1]:\n",
    "#         if m in order:\n",
    "#             order.remove(m)\n",
    "#             order.insert(0, m)\n",
    "#     return order\n",
    "\n",
    "# collect = {}\n",
    "\n",
    "# collect[\"lr_2008\"] = get_best_score(tune_search)\n",
    "\n",
    "# collect\n",
    "\n",
    "# order = get_metric_order(collect)\n",
    "\n",
    "# # collect = collections.OrderedDict(collect)\n",
    "\n",
    "# # val_summary = pd.DataFrame(collect)\n",
    "\n",
    "# val_summary = pd.DataFrame(collect, index=order)\n",
    "# val_summary_floats = val_summary.apply(pd.to_numeric, axis=0, errors=\"coerce\")\n",
    "# val_summary.insert(loc=0, column=\"std\", value=val_summary_floats.std(axis=1))\n",
    "# val_summary.insert(loc=0, column=\"mean\", value=val_summary_floats.mean(axis=1))\n",
    "\n",
    "# val_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c343f530",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1365bbc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331a4a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206b35b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae8eb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_search.best_index_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f09eb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f31a9f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd11b09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1561b72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tune_search.predict(data.X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6910432",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad63cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_accuracy_score(data.y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083a457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(y_pred) / len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8250983",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trivial = np.zeros_like(data.y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447a2965",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(data.y_val, y_trivial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e134475b",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(data.y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a6edb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_accuracy_score(data.y_val, y_trivial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ba9c01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347192a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de3eb82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054bf8bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d796fe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c2b1bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0e5bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3f277e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
