{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a947ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f1bdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dir = Path(r\"C:\\Users\\Mathiass\\Documents\\Projects\\master-thesis\\logs\\tune\\nn_loops\\20220906163324\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f002e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_exp = pd.read_json(exp_dir/\"args.json\", typ=\"series\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094db1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yearidx_bestmodelpaths(exp_path: Path) -> list:\n",
    "    \"\"\"Search exp_path for 'best_ckpt????' files and save their paths into a list.\n",
    "    Then sort the path and enumerate it to get a list of tuples (idx, best_ckpt_path).\n",
    "\n",
    "        Returns:\n",
    "            idx_bestmodelpaths (list): Enumerated best checkpoint paths in the\n",
    "                                        experiment path.\n",
    "    \"\"\"\n",
    "    best_ckpt_paths = []\n",
    "    for directory in exp_path.iterdir():\n",
    "        if directory.is_dir() and directory.name != \"predictions\" and directory.name != \"portfolios\":\n",
    "            # See https://docs.python.org/3/library/fnmatch.html#module-fnmatch\n",
    "            # for filename pattern matching below.\n",
    "            for file in directory.glob(\"best_ckpt????\"):\n",
    "                # If files do not exist in 'predictions' folder yet\n",
    "                best_ckpt_paths.append(file.resolve())\n",
    "    # IMPORTANT: Sort best ckpt paths that were read in, in ascending order.\n",
    "    best_ckpt_paths = sorted(best_ckpt_paths, key=lambda x: int(str(x)[-4:]))\n",
    "    # Append corresponding year_idx to each best_model_path.\n",
    "    idx_bestmodelpaths = list(enumerate(best_ckpt_paths))\n",
    "    # Check if year_idx and bestckpts are in the correct order.\n",
    "    prev_year = -9999\n",
    "    for yearidx, bestckpt_path in idx_bestmodelpaths:\n",
    "        year = int(str(bestckpt_path)[-4:])\n",
    "        if year > prev_year:\n",
    "            print(f\"({yearidx}, {year})\\t is the correct (year_idx, year) tuple!\")\n",
    "            continue\n",
    "        else:\n",
    "            raise ValueError(\"(year_idx, bestmodel_ckpt_path) are not in the \"\n",
    "                             \"correct ascending order.\")\n",
    "    return idx_bestmodelpaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c60dde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "yearidx_target_list = get_yearidx_bestmodelpaths(exp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be177af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "yearidx_target_list[0][1].stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd3d644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path: Path, dataset: str):\n",
    "    \"\"\"Loads specific dataset from path, depending on specified size.\"\"\"\n",
    "    if dataset == \"small\":\n",
    "        return pd.read_parquet(data_path/\"final_df_call_cao_small.parquet\")\n",
    "    elif dataset == \"medium\":\n",
    "        return pd.read_parquet(data_path/\"final_df_call_cao_med_fillmean.parquet\")\n",
    "    elif dataset == \"big\":\n",
    "        return pd.read_parquet(data_path/\"final_df_call_cao_big_fillmean.parquet\")\n",
    "    else:\n",
    "        raise ValueError(\"Specify dataset as either 'small', 'medium' or big'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e42417",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4d8d9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "type(args_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c39d514",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = Path(r\"C:\\Users\\Mathiass\\Documents\\Projects\\master-thesis\\data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08b2a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = load_data(path_data, args_exp.dataset).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba184d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols.remove(\"date\")\n",
    "cols.remove(\"option_ret\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa9d9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1999d796",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a36938",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82235d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = [\"date\", \"option_ret\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04da1866",
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i in cols if i not in to_drop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a4a00f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f849f4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cols:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeb0322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aab9883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49abaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# import pdb\n",
    "\n",
    "class FFN(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                input_dim: int,\n",
    "                num_classes: int,\n",
    "                class_weights: torch.Tensor,\n",
    "                no_class_weights: bool,\n",
    "                learning_rate: float,\n",
    "                hidden_dim: int,\n",
    "                n_hidden: int,\n",
    "                batch_norm: bool,\n",
    "                dropout: bool,\n",
    "                drop_prob: float,\n",
    "                # config: dict = None,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        # Init variables are saved, so that model can be reloaded cleanly if necessary\n",
    "        # self.save_hyperparameters(ignore=[\"class_weights\"])\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        middle_layers = []\n",
    "        for _ in range(self.hparams.n_hidden):\n",
    "            middle_layers.append(nn.Linear(self.hparams.hidden_dim, self.hparams.hidden_dim))\n",
    "            if self.hparams.batch_norm:\n",
    "                middle_layers.append(nn.BatchNorm1d(self.hparams.hidden_dim))\n",
    "            middle_layers.append(nn.ReLU(inplace=True))\n",
    "            if self.hparams.dropout:\n",
    "                middle_layers.append(nn.Dropout(p=self.hparams.drop_prob))\n",
    "\n",
    "        #model\n",
    "        self.first = nn.Sequential(nn.Linear(self.hparams.input_dim, self.hparams.hidden_dim), \n",
    "                                    nn.ReLU(inplace=True))\n",
    "        self.middle = nn.Sequential(*middle_layers)  \n",
    "        self.last = nn.Linear(self.hparams.hidden_dim, self.hparams.num_classes)\n",
    "        \n",
    "        #sample weights\n",
    "        if not self.hparams.no_class_weights:\n",
    "            self.class_weights = class_weights\n",
    "            self.class_weights = self.class_weights.cuda() # Move to cuda, otherwise mismatch of devices # in train/val\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "        print(\"---\")\n",
    "        print(\"class_weights:\", self.class_weights)\n",
    "        print(\"device of class_weights:\", self.class_weights.device)\n",
    "        print(\"device of class:\", self.device)\n",
    "        print(\"---\")\n",
    "\n",
    "        #metrics\n",
    "        self.train_acc = torchmetrics.Accuracy()\n",
    "        self.train_bal_acc = torchmetrics.Accuracy(\n",
    "        num_classes=self.hparams.num_classes, average=\"macro\") # should be equal to sklearn bal. acc.\n",
    "\n",
    "        self.val_acc = torchmetrics.Accuracy()\n",
    "        self.val_bal_acc= torchmetrics.Accuracy(\n",
    "            num_classes=self.hparams.num_classes, average=\"macro\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first(x)\n",
    "        x = self.middle(x)\n",
    "        x = self.last(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x) #logits\n",
    "        \n",
    "        loss = F.cross_entropy(y_hat, y, weight=self.class_weights)\n",
    "        # Logging is done \"log_every_n_steps\" times (default=50 steps)\n",
    "        self.log(\"loss/loss\", loss, on_step=True, on_epoch=False, prog_bar=True)\n",
    "        \n",
    "        self.train_acc(y_hat, y)\n",
    "        self.log(\"acc/train\", self.train_acc, on_step=False, on_epoch=True)\n",
    "        \n",
    "        self.train_bal_acc(y_hat, y)\n",
    "        self.log(\"bal_acc/train\", self.train_bal_acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x) #logits\n",
    "        \n",
    "#         self.log(\"hp_metric\", torch.mean(y_hat.argmax(dim=-1).float()).item(), prog_bar=True) # average prediction class\n",
    "        self.log(\"mean_pred\", torch.mean(y_hat.argmax(dim=-1).float()).item(), prog_bar=True)\n",
    "        \n",
    "        loss = F.cross_entropy(y_hat, y, weight=self.class_weights)\n",
    "        self.log(\"loss/val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        self.val_acc(y_hat, y)\n",
    "        self.log(\"acc/val\", self.val_acc, on_step=False, on_epoch=True)\n",
    "        \n",
    "        self.val_bal_acc(y_hat, y)\n",
    "        self.log(\"bal_acc/val\", self.val_bal_acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return {\"val_loss\": loss}\n",
    "    \n",
    "    def on_train_start(self):\n",
    "        self.st_total = time.time()\n",
    "\n",
    "    def on_train_epoch_start(self):\n",
    "        self.st = time.time()\n",
    "        self.steps = self.global_step\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        elapsed = time.time() - self.st\n",
    "        steps_done = self.global_step - self.steps\n",
    "        self.log(\"time/step\", elapsed / steps_done)\n",
    "\n",
    "    def on_train_end(self):\n",
    "        elapsed = time.time() - self.st_total\n",
    "        print(f\"Total Training Time: {time.strftime('%H:%M:%S', time.gmtime(elapsed))}\")\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y, weight=self.class_weights)\n",
    "\n",
    "        self.log(\"loss/test_loss\", loss, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        return self(batch)\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):\n",
    "        parser = parent_parser.add_argument_group(\"FFN\")\n",
    "        parser.add_argument(\"--no_class_weights\", action='store_true')\n",
    "        parser.add_argument(\"--hidden_dim\", type=int, default=100)\n",
    "        parser.add_argument(\"-lr\", \"--learning_rate\", type=float, default=1e-2)\n",
    "        parser.add_argument(\"--n_hidden\", type=int, default=0)\n",
    "        parser.add_argument(\"--no_batch_norm\", action='store_false')\n",
    "        parser.add_argument(\"--no_dropout\", action='store_false')\n",
    "        parser.add_argument(\"--drop_prob\", type=float, default=0.5)\n",
    "\n",
    "        return parent_parser\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ee2243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineer(data):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    data: pandas.DataFrame that must have specific columns.\n",
    "\n",
    "    \"\"\"\n",
    "    # Bid-Ask spread: (Ask - Bid) / Ask\n",
    "    data[\"best_bid\"] = (data[\"best_offer\"] - data[\"best_bid\"]) / (data[\"best_offer\"])\n",
    "    data = data.rename(columns={\"best_bid\": \"ba_spread_option\"}).drop([\"best_offer\"], axis=1)\n",
    "\n",
    "    # Gamma: multiply by spotprice and divide by 100\n",
    "    data[\"gamma\"] = data[\"gamma\"] * data[\"spotprice\"] / 100 #following Bali et al. (2021)\n",
    "\n",
    "    # Theta: scale by spotprice\n",
    "    data[\"theta\"] = data[\"theta\"] / data[\"spotprice\"] #following Bali et al. (2021)\n",
    "\n",
    "    # Vega: scale by spotprice\n",
    "    data[\"vega\"] = data[\"vega\"] / data[\"spotprice\"] #following Bali et al. (2021)\n",
    "\n",
    "    # Time to Maturity: scale by number of days in year: 365\n",
    "    data[\"days_to_exp\"] = data[\"days_to_exp\"] / 365\n",
    "\n",
    "    # Moneyness: Strike / Spot (K / S)\n",
    "    data[\"strike_price\"] = data[\"strike_price\"] / data[\"spotprice\"] # K / S\n",
    "    data = data.rename(columns={\"strike_price\": \"moneyness\"})\n",
    "\n",
    "    # Forward Price ratio: Forward / Spot\n",
    "    data[\"forwardprice\"] = data[\"forwardprice\"] / data[\"spotprice\"]\n",
    "\n",
    "    # Drop redundant/ unimportant columns\n",
    "    data = data.drop([\"cfadj\", \"days_no_trading\", \"spotprice\", \"adj_spot\"], axis=1)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Binary y label generator.\n",
    "def binary_categorize(y):\n",
    "    \"\"\"\n",
    "    Input: continuous target variable \n",
    "\n",
    "    Output: 1 for positive returns, \n",
    "            0 for negative returns\n",
    "    \"\"\"\n",
    "    # threshold 0%\n",
    "    if y > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "# Multiclass y label generator.\n",
    "def multi_categorize(y: float, classes: int):\n",
    "    \"\"\"\n",
    "    Creates categorical labels from continuous values.\n",
    "\n",
    "        Args:\n",
    "            y (float):      continuous target variable (option return)\n",
    "            classes (int):  number of classes to create\n",
    "        Returns:\n",
    "            (int):          class assignment\n",
    "        CAREFUL: classes have to be between [0, C) for F.crossentropyloss.\n",
    "    \"\"\"\n",
    "    if classes == 3:\n",
    "        # thresholds: +/- 5%\n",
    "        if y > 0.05:\n",
    "            return 2\n",
    "        elif y < -0.05:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    elif classes == 5:\n",
    "        # thresholds: +/- 2.5% and +/- 5%\n",
    "        if y > 0.05:\n",
    "            return 4\n",
    "        elif (y > 0.025 and y <= 0.05):\n",
    "            return 3\n",
    "        elif (y >= -0.05 and y < -0.025):\n",
    "            return 1\n",
    "        elif (y < -0.05):\n",
    "            return 0\n",
    "        else:\n",
    "            return 2 # all returns \\elin [-0.025, 0.025]\n",
    "    # elif classes==10:\n",
    "    #     if y > 0.05:\n",
    "    #         return 9\n",
    "    #     elif (y > 0.04 and y <= 0.05):\n",
    "    #         return 8\n",
    "    #     elif (y > 0.03 and y <= 0.04):\n",
    "    #         return 7\n",
    "    #     elif (y > 0.02 and y <= 0.03):\n",
    "    #         return 6\n",
    "    #     elif (y > 0.01 and y <= 0.02):\n",
    "    #         return 5\n",
    "    #     elif (y >= -0.02 and y < -0.01):\n",
    "    #         return 3\n",
    "    #     elif (y >= -0.03 and y < -0.02):\n",
    "    #         return 2\n",
    "    #     elif (y >= -0.04 and y < -0.03):\n",
    "    #         return 1\n",
    "    #     elif (y >= -0.05 and y < -0.05):\n",
    "    #         return 0\n",
    "    #     else:\n",
    "    #         return 4\n",
    "    else:\n",
    "        raise ValueError(\"Only multi for 3 or 5 classes implemented right now.\")\n",
    "\n",
    "\n",
    "class YearEndIndeces:\n",
    "    \"\"\"Generator for indices where years change.\n",
    "\n",
    "        Args:\n",
    "            dates (pandas.Series):      series of datetimes,\n",
    "            init_train_length (int):    initial train length,\n",
    "            val_length (int):           validation length\n",
    "    \"\"\"\n",
    "    def __init__(self, dates, init_train_length, val_length, test_length):\n",
    "        # Find indeces where years change.\n",
    "        self.val_length = val_length\n",
    "        self.test_length = test_length\n",
    "        # Get end of month indeces for slicing.\n",
    "        # TECHNICALLY its start of month indeces, i.e. first row of January 31,\n",
    "        # but because for slicing [:idx], idx is not included, we name it end of\n",
    "        # year here.\n",
    "        self.eoy_idx =  np.where((dates.dt.year.diff() == 1))[0]\n",
    "        # Append last row as end of year of last year.\n",
    "        self.eoy_idx = np.append(self.eoy_idx, len(dates))\n",
    "\n",
    "        assert init_train_length + val_length + test_length <= len(self.eoy_idx), \\\n",
    "            (\"defined train and val are larger than eoy_indeces generated\")\n",
    "        assert init_train_length > 0, \"init_train_length must be strictly greater than 0\"\n",
    "\n",
    "        # The 4th idx in eoy_idx is the end of year 5. -> Subtract 1.\n",
    "        self.train_length_zeroindex = init_train_length - 1\n",
    "\n",
    "        self.train_eoy = self.eoy_idx[self.train_length_zeroindex:-(val_length+test_length)]\n",
    "        self.val_eoy = self.eoy_idx[self.train_length_zeroindex + val_length:-test_length]\n",
    "        # For generate_idx():\n",
    "        self.test_eoy = self.eoy_idx[self.train_length_zeroindex + val_length + test_length:]\n",
    "\n",
    "    # def generate(self):\n",
    "    #     for i in range(len(self.eoy_idx) - (self.train_start_idx + self.val_length)):\n",
    "    #         yield (list(range(self.train_eoy[i])),\n",
    "    #                list(range(self.train_eoy[i], self.val_eoy[i])))\n",
    "\n",
    "    def generate_idx(self):\n",
    "        for i in range(len(self.eoy_idx) - (self.train_length_zeroindex + self.val_length \n",
    "                        + self.test_length)):\n",
    "            yield ({\"train\": self.train_eoy[i], \n",
    "                    \"val\": self.val_eoy[i], \n",
    "                    \"test\": self.test_eoy[i]}\n",
    "                )\n",
    "\n",
    "\n",
    "class YearMonthEndIndeces:\n",
    "    \"\"\"Generator for indices where months change.\n",
    "\n",
    "        Args:\n",
    "            dates (pandas.Series):      series of datetimes,\n",
    "            init_train_length (int):    initial train length,\n",
    "            val_length (int):           validation length\n",
    "    \"\"\"\n",
    "    def __init__(self, dates, init_train_length, val_length, test_length):\n",
    "        # self.val_length = val_length\n",
    "        # self.test_length = test_length\n",
    "        # Get end of month indeces for slicing.\n",
    "        # TECHNICALLY its start of month indeces, i.e. first row of January 31,\n",
    "        # but because for slicing [:idx], idx is not included, we name it end of\n",
    "        # year here.\n",
    "        self.eom_idx =  np.concatenate([\n",
    "                        np.where((dates.dt.month.diff() == 1))[0], \n",
    "                        np.where((dates.dt.month.diff() == -11))[0] #Dec->Jan\n",
    "                        ])\n",
    "        # Sort, since Dec->Jan months indeces are only concatenated at the end.\n",
    "        self.eom_idx.sort()\n",
    "        # Append last row as end of month of last month.\n",
    "        self.eom_idx = np.append(self.eom_idx, len(dates))\n",
    "\n",
    "        # End of year indeces\n",
    "        self.eoy_idx =  np.where((dates.dt.year.diff() == 1))[0]\n",
    "        self.eoy_idx = np.append(self.eoy_idx, len(dates))\n",
    "\n",
    "        # Careful: -2 because November (-> cao (2021) return calc.) and December 2021 is not in dataset.\n",
    "        assert (26 * 12 - 2 == len(self.eom_idx)), (\"Some end of month indeces are missing.\")\n",
    "        assert init_train_length > 0, \"init_train_length must be strictly greater than 0.\"\n",
    "\n",
    "        # The 4th idx in eoy_idx is the end of year 5. -> Subtract 1.\n",
    "        self.train_length_zeroindex = init_train_length - 1\n",
    "\n",
    "        # Get eoy indeces where we predicted on AND FIRST ENTRY IS EOY_VAL == SOY_TEST\n",
    "        self.test_eoy = self.eoy_idx[self.train_length_zeroindex + val_length:]\n",
    "\n",
    "        # Get first end of month idx of year X until first end of month of year Y\n",
    "        # a prediction was made.\n",
    "        years_predicted = np.arange(1996 + init_train_length + val_length, 2021 + 1) #upper limit not included.\n",
    "        self.month_idx_per_year = {}\n",
    "        for i, eoy_idx in enumerate(self.test_eoy[:-1]): #-1 because [last_index:last_index+13] not needed.\n",
    "            idx_in_idx = np.where(np.in1d(self.eom_idx, eoy_idx))[0].item() #only one eom_idx equals one eoy_idx\n",
    "            # + 13, so that slicing is from start of year until +12 months (end of year).\n",
    "            self.month_idx_per_year[years_predicted[i]] = self.eom_idx[idx_in_idx:idx_in_idx+13]\n",
    "\n",
    "        # Check that dictionary years are correct and that months are consecutive.\n",
    "        assert check_month_years(self.month_idx_per_year, dates=dates), (\"Years of end \"\n",
    "        \"of month indices are wrong or the months are not consecutive.\")\n",
    "\n",
    "    def get_indeces(self):\n",
    "        # Return Tuple.\n",
    "        return (self.test_eoy, self.month_idx_per_year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1053f56a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cb737c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pathlib import Path\n",
    "import pdb\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "def load_data(path: Path, dataset: str):\n",
    "    \"\"\"Loads dataset from path, depending on specified size.\"\"\"\n",
    "    if dataset == \"small\":\n",
    "        return pd.read_parquet(path/\"final_df_call_cao_small.parquet\")\n",
    "    elif dataset == \"medium\":\n",
    "        return pd.read_parquet(path/\"final_df_call_cao_med_fillmean.parquet\")\n",
    "    elif dataset == \"big\":\n",
    "        return pd.read_parquet(path/\"final_df_call_cao_big_fillmean.parquet\")\n",
    "    elif dataset == \"predict\":\n",
    "        return pd.read_parquet(path)\n",
    "    else:\n",
    "        raise ValueError(\"Specify dataset as either 'small', 'medium' or 'custom'. For 'custom' \"\n",
    "                         \"the absolute path is required.\")\n",
    "\n",
    "class DataModule(pl.LightningDataModule):\n",
    "    \"\"\"Dataset Loader for Pytorch Lightning (Neural Network).\"\"\"\n",
    "    def __init__(self,\n",
    "                 path: str, # will be converted to Path in __init__\n",
    "                 year_idx: int,\n",
    "                 dataset: str,\n",
    "                 batch_size: int,\n",
    "                 init_train_length: int,\n",
    "                 val_length: int,\n",
    "                 test_length: int,\n",
    "                #  start_val: str, \n",
    "                #  start_test: str,\n",
    "                 label_fn: str,\n",
    "                 custom_data: pd.DataFrame = None,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=[\"path\"])\n",
    "        \n",
    "        # read data from disk ########################################### ADDED\n",
    "        if custom_data is not None:\n",
    "            self.data = custom_data\n",
    "        else:\n",
    "            path = Path(path)\n",
    "            self.data = load_data(path, dataset)\n",
    "        ###########################################################################\n",
    "\n",
    "        # get splits\n",
    "        splitter = YearEndIndeces(\n",
    "                                self.data[\"date\"], \n",
    "                                init_train_length=init_train_length, \n",
    "                                val_length=val_length,\n",
    "                                test_length=test_length,\n",
    "                                )\n",
    "        eoy_indeces = list(splitter.generate_idx())\n",
    "        self.eoy_train = eoy_indeces[year_idx][\"train\"]\n",
    "        self.eoy_val = eoy_indeces[year_idx][\"val\"]\n",
    "        self.eoy_test = eoy_indeces[year_idx][\"test\"]\n",
    "\n",
    "        # Truncate data\n",
    "        self.data = self.data.iloc[:self.eoy_test]\n",
    "        assert len(self.data) == self.eoy_test, \"length of data is not equal to eoy_test\"\n",
    "            \n",
    "#         # feature engineer data\n",
    "#         self.data = feature_engineer(self.data)\n",
    "        \n",
    "        # create y\n",
    "        self.y = self.data[\"option_ret\"]\n",
    "        # make classification problem\n",
    "        if label_fn == \"binary\":\n",
    "            self.y = self.y.apply(binary_categorize)\n",
    "        elif label_fn == \"multi3\":\n",
    "            self.y = self.y.apply(multi_categorize, classes=3)\n",
    "        elif label_fn == \"multi5\":\n",
    "            self.y = self.y.apply(multi_categorize, classes=5)\n",
    "        else:\n",
    "            raise ValueError(\"Specify label_fn as either 'binary' or 'multi'\")\n",
    "        # create X\n",
    "        self.X = self.data.drop([\"option_ret\"], axis=1)\n",
    "        \n",
    "        # save dates and drop\n",
    "        self.dates = self.X[\"date\"]\n",
    "        self.X = self.X.drop([\"date\"], axis=1)\n",
    "        \n",
    "        # to torch Tensor\n",
    "        self.X = torch.from_numpy(self.X.values).float() #-> will be standardized in setup, so do it there.\n",
    "        self.y = torch.from_numpy(self.y.values)\n",
    "        \n",
    "    def setup(self, stage: str = None):\n",
    "        # train\n",
    "        # self.X_train = self.X[self.dates < self.hparams.start_val]\n",
    "        self.X_train = self.X[:self.eoy_train]\n",
    "        self.y_train = self.y[:len(self.X_train)]\n",
    "        \n",
    "        #val\n",
    "        # mask = (self.dates >= self.hparams.start_val) & (self.dates < self.hparams.start_test)\n",
    "        # self.X_val = self.X[mask]\n",
    "        self.X_val = self.X[self.eoy_train:self.eoy_val]\n",
    "        self.y_val = self.y[len(self.X_train):len(self.X_train)+len(self.X_val)]\n",
    "        \n",
    "        # test\n",
    "        self.X_test = self.X[self.eoy_val:self.eoy_test]\n",
    "        self.y_test = self.y[-len(self.X_test):]\n",
    "        \n",
    "        assert (len(self.X_train)+len(self.X_val)+len(self.X_test)) == len(self.data), \\\n",
    "            \"sum of X train, val, test is not equal length of dataset\"\n",
    "        assert (len(self.y_train)+len(self.y_val)+len(self.y_test) == len(self.data)), \\\n",
    "        \"sum of y train, val, test is not equal to length of dataset\"\n",
    "        \n",
    "        #standardize X_train\n",
    "        mean = torch.mean(self.X_train, axis=0)\n",
    "        std = torch.std(self.X_train, axis=0)\n",
    "        \n",
    "        # Standardize X_train, X_val and X_test with mean/std from X_train\n",
    "        self.X_train = (self.X_train - mean) / std\n",
    "        self.X_val = (self.X_val - mean) / std\n",
    "        self.X_test = (self.X_test - mean) / std\n",
    "\n",
    "        # Save variables to pass to model class\n",
    "        # input dim\n",
    "        self.input_dim = self.X_train.shape[1]\n",
    "        # number of classes\n",
    "        self.num_classes = len(self.y_train.unique())\n",
    "        # class weights\n",
    "        self.class_weights = len(self.y_train) / self.y_train.unique(return_counts=True)[1]\n",
    "\n",
    "        print(\"*****************************************************************************************\")\n",
    "        print(\"Current TORCH dataset information:\")\n",
    "        print(\"---\")\n",
    "        print(\"class counts: \", self.y_train.unique(return_counts=True))\n",
    "        print(\"class_weights:\", self.class_weights)\n",
    "        print(\"device of class_weights:\", self.class_weights.device)\n",
    "        print(\"---\")\n",
    "        print(f\"# of input data: {len(self.data)} with shape: {self.data.shape}\")\n",
    "        print(f\"# of training samples: {len(self.y_train)} with X_train of shape: {self.X_train.shape}\")\n",
    "        print(f\"# of validation samples: {len(self.y_val)} with X_val of shape: {self.X_val.shape}\")\n",
    "        print(f\"# of test samples: {len(self.y_test)} with X_test of shape: {self.X_test.shape}\")\n",
    "        print(\"---\")\n",
    "        print(f\"train start date: \", self.dates.iloc[0].strftime(\"%Y-%m-%d\"), \n",
    "              \", train end date: \", self.dates.iloc[:self.eoy_train].iloc[-1].strftime(\"%Y-%m-%d\"))\n",
    "        print(f\"val start date: \", self.dates.iloc[self.eoy_train:self.eoy_val].iloc[0].strftime(\"%Y-%m-%d\"), \n",
    "              \", val end date: \", self.dates.iloc[self.eoy_train:self.eoy_val].iloc[-1].strftime(\"%Y-%m-%d\"))\n",
    "        print(f\"test start date: \", self.dates.iloc[self.eoy_val:self.eoy_test].iloc[0].strftime(\"%Y-%m-%d\"), \n",
    "              \", test end date: \", self.dates.iloc[self.eoy_val:self.eoy_test].iloc[-1].strftime(\"%Y-%m-%d\"))\n",
    "        print(\"*****************************************************************************************\")\n",
    "\n",
    "\n",
    "    def example(self):\n",
    "        \"\"\"Returns a random training example.\"\"\"        \n",
    "        idx = np.random.randint(0, len(self.X_train))\n",
    "        x, y = self.X_train[idx], self.y_train[idx]\n",
    "        return (x, y)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        dataset = TensorDataset(self.X_train, self.y_train)\n",
    "        return DataLoader(dataset, batch_size=self.hparams.batch_size,\n",
    "                         num_workers=0, #uses just the main worker, see https://stackoverflow.com/questions/71713719/runtimeerror-dataloader-worker-pids-15876-2756-exited-unexpectedly\n",
    "                         # there are issues occuring on windows where PID workers exit unexpectedly.\n",
    "                         pin_memory=True,\n",
    "                         shuffle=True, #shuffle training data\n",
    "                         )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        dataset = TensorDataset(self.X_val, self.y_val)\n",
    "        return DataLoader(dataset, batch_size=self.hparams.batch_size,\n",
    "                         num_workers=0,\n",
    "                         pin_memory=True,\n",
    "                         shuffle=False,\n",
    "                         )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        dataset = TensorDataset(self.X_test, self.y_test)\n",
    "        return DataLoader(dataset, batch_size=self.hparams.batch_size,\n",
    "                         num_workers=0,\n",
    "                         pin_memory=True,\n",
    "                         shuffle=False, #must not shuffle here!\n",
    "                         )\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        dataset = self.X_test # predict_step expects tensor not a list\n",
    "        return DataLoader(dataset, batch_size=self.hparams.batch_size,\n",
    "                        num_workers=0,\n",
    "                        pin_memory=True,\n",
    "                        shuffle=False, #must not shuffle here!\n",
    "                        )\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):\n",
    "        parser = parent_parser.add_argument_group(\"DataModule for Lightning\")\n",
    "        parser.add_argument(\"--batch_size\", type=int, default=512)\n",
    "        return parent_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dd9ea0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load best model\n",
    "yearidx_target = yearidx_target_list[-1]\n",
    "\n",
    "best_path = yearidx_target[1] ######## IMPORTANT ############################################\n",
    "\n",
    "# RANDOMIZED DATA\n",
    "rand_data = pd.read_parquet(Path(args_exp.path_data)/\"final_df_call_cao_small.parquet\")\n",
    "\n",
    "# Copy best model checkpoint to loop folder for later analysis.\n",
    "# test_year_end = val_year_end + args.test_length\n",
    "# shutil.copy2(best_path, loop_path/f\"best_ckpt{test_year_end}\")\n",
    "print(f\"Loading model to predict from path: {best_path}\")\n",
    "model = FFN.load_from_checkpoint(best_path)\n",
    "dm = DataModule(\n",
    "    path=args_exp.path_data, ########################## SET TO NONE\n",
    "    year_idx=yearidx_target[0], ######## IMPORTANT ############################################\n",
    "    dataset=\"small\", ############################ SET TO NONE\n",
    "    batch_size=128, # #################TAKE FIXED BATCH SIZE, SHOULD AFFECT PREDICTIONS?\n",
    "    init_train_length=args_exp.init_train_length,\n",
    "    val_length=args_exp.val_length,\n",
    "    test_length=args_exp.test_length,\n",
    "    label_fn=args_exp.label_fn,\n",
    "    custom_data=None, #################################### NEW\n",
    "    # config=model.hparams.config, # so that config is not hyperparam search again\n",
    ")\n",
    "trainer = pl.Trainer(\n",
    "    deterministic=True,\n",
    "    gpus=1, #fractional gpus here not possible.\n",
    "    logger=False, #deactivate logging for prediction\n",
    ")\n",
    "# predict\n",
    "preds = trainer.predict(model=model, datamodule=dm) #returns list of batch predictions.\n",
    "preds = torch.cat(preds) #preds is a list already of [batch_size, num_classes]. \n",
    "preds_argmax = preds.argmax(dim=1).numpy()\n",
    "preds_argmax_df_old = pd.DataFrame(preds_argmax, columns=[\"pred\"])\n",
    "# # prediction path\n",
    "# save_to_dir = loop_path/f\"prediction{test_year_end}.csv\"\n",
    "#         preds_argmax_df.to_csv(save_to_dir, index_label=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c09fdc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load best model\n",
    "best_path = yearidx_target[1] ######## IMPORTANT ############################################\n",
    "\n",
    "# RANDOMIZED DATA\n",
    "rand_data = pd.read_parquet(Path(args_exp.path_data)/\"final_df_call_cao_small.parquet\")\n",
    "\n",
    "# Copy best model checkpoint to loop folder for later analysis.\n",
    "# test_year_end = val_year_end + args.test_length\n",
    "# shutil.copy2(best_path, loop_path/f\"best_ckpt{test_year_end}\")\n",
    "print(f\"Loading model to predict from path: {best_path}\")\n",
    "model = FFN.load_from_checkpoint(best_path)\n",
    "dm = DataModule(\n",
    "    path=None, ########################## SET TO NONE\n",
    "    year_idx=yearidx_target[0], ######## IMPORTANT ############################################\n",
    "    dataset=None, ############################ SET TO NONE\n",
    "    batch_size=128, # #################TAKE FIXED BATCH SIZE, SHOULD AFFECT PREDICTIONS?\n",
    "    init_train_length=args_exp.init_train_length,\n",
    "    val_length=args_exp.val_length,\n",
    "    test_length=args_exp.test_length,\n",
    "    label_fn=args_exp.label_fn,\n",
    "    custom_data=rand_data, #################################### NEW\n",
    "    # config=model.hparams.config, # so that config is not hyperparam search again\n",
    ")\n",
    "trainer = pl.Trainer(\n",
    "    deterministic=True,\n",
    "    gpus=1, #fractional gpus here not possible.\n",
    "    logger=False, #deactivate logging for prediction\n",
    ")\n",
    "# predict\n",
    "preds = trainer.predict(model=model, datamodule=dm) #returns list of batch predictions.\n",
    "preds = torch.cat(preds) #preds is a list already of [batch_size, num_classes]. \n",
    "preds_argmax = preds.argmax(dim=1).numpy()\n",
    "preds_argmax_df_new = pd.DataFrame(preds_argmax, columns=[\"pred\"])\n",
    "# # prediction path\n",
    "# save_to_dir = loop_path/f\"prediction{test_year_end}.csv\"\n",
    "#         preds_argmax_df.to_csv(save_to_dir, index_label=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e38cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ecb4d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc8a384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800e07e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CHECK IF METHODS ARE EQUAL, when loading data from path vs. inputting them.\n",
    "\n",
    "# # load best model\n",
    "# list_equal = []\n",
    "\n",
    "# for yearidx_target in tqdm(yearidx_target_list):\n",
    "\n",
    "#     best_path = yearidx_target[1] ######## IMPORTANT ############################################\n",
    "\n",
    "#     # RANDOMIZED DATA\n",
    "#     rand_data = pd.read_parquet(Path(args.path_data)/\"final_df_call_cao_small.parquet\")\n",
    "\n",
    "#     # Copy best model checkpoint to loop folder for later analysis.\n",
    "#     # test_year_end = val_year_end + args.test_length\n",
    "#     # shutil.copy2(best_path, loop_path/f\"best_ckpt{test_year_end}\")\n",
    "#     print(f\"Loading model to predict from path: {best_path}\")\n",
    "#     model = FFN.load_from_checkpoint(best_path)\n",
    "#     dm = DataModule(\n",
    "#         path=args.path_data, ########################## SET TO NONE\n",
    "#         year_idx=yearidx_target[0], ######## IMPORTANT ############################################\n",
    "#         dataset=\"small\", ############################ SET TO NONE\n",
    "#         batch_size=128, # #################TAKE FIXED BATCH SIZE, SHOULD AFFECT PREDICTIONS?\n",
    "#         init_train_length=args.init_train_length,\n",
    "#         val_length=args.val_length,\n",
    "#         test_length=args.test_length,\n",
    "#         label_fn=args.label_fn,\n",
    "#         custom_data=None, #################################### NEW\n",
    "#         # config=model.hparams.config, # so that config is not hyperparam search again\n",
    "#     )\n",
    "#     trainer = pl.Trainer(\n",
    "#         deterministic=True,\n",
    "#         gpus=math.ceil(args.gpus_per_trial), #fractional gpus here not possible.\n",
    "#         logger=False, #deactivate logging for prediction\n",
    "#     )\n",
    "#     # predict\n",
    "#     preds = trainer.predict(model=model, datamodule=dm) #returns list of batch predictions.\n",
    "#     preds = torch.cat(preds) #preds is a list already of [batch_size, num_classes]. \n",
    "#     preds_argmax = preds.argmax(dim=1).numpy()\n",
    "#     preds_argmax_df_old = pd.DataFrame(preds_argmax, columns=[\"pred\"])\n",
    "#     # # prediction path\n",
    "#     # save_to_dir = loop_path/f\"prediction{test_year_end}.csv\"\n",
    "#     #         preds_argmax_df.to_csv(save_to_dir, index_label=\"id\")\n",
    "    \n",
    "#     # load best model\n",
    "#     best_path = yearidx_target[1] ######## IMPORTANT ############################################\n",
    "\n",
    "#     # RANDOMIZED DATA\n",
    "#     rand_data = pd.read_parquet(Path(args.path_data)/\"final_df_call_cao_small.parquet\")\n",
    "\n",
    "#     # Copy best model checkpoint to loop folder for later analysis.\n",
    "#     # test_year_end = val_year_end + args.test_length\n",
    "#     # shutil.copy2(best_path, loop_path/f\"best_ckpt{test_year_end}\")\n",
    "#     print(f\"Loading model to predict from path: {best_path}\")\n",
    "#     model = FFN.load_from_checkpoint(best_path)\n",
    "#     dm = DataModule(\n",
    "#         path=None, ########################## SET TO NONE\n",
    "#         year_idx=yearidx_target[0], ######## IMPORTANT ############################################\n",
    "#         dataset=None, ############################ SET TO NONE\n",
    "#         batch_size=128, # #################TAKE FIXED BATCH SIZE, SHOULD AFFECT PREDICTIONS?\n",
    "#         init_train_length=args.init_train_length,\n",
    "#         val_length=args.val_length,\n",
    "#         test_length=args.test_length,\n",
    "#         label_fn=args.label_fn,\n",
    "#         custom_data=rand_data, #################################### NEW\n",
    "#         # config=model.hparams.config, # so that config is not hyperparam search again\n",
    "#     )\n",
    "#     trainer = pl.Trainer(\n",
    "#         deterministic=True,\n",
    "#         gpus=math.ceil(args.gpus_per_trial), #fractional gpus here not possible.\n",
    "#         logger=False, #deactivate logging for prediction\n",
    "#     )\n",
    "#     # predict\n",
    "#     preds = trainer.predict(model=model, datamodule=dm) #returns list of batch predictions.\n",
    "#     preds = torch.cat(preds) #preds is a list already of [batch_size, num_classes]. \n",
    "#     preds_argmax = preds.argmax(dim=1).numpy()\n",
    "#     preds_argmax_df_new = pd.DataFrame(preds_argmax, columns=[\"pred\"])\n",
    "#     # # prediction path\n",
    "#     # save_to_dir = loop_path/f\"prediction{test_year_end}.csv\"\n",
    "#     #         preds_argmax_df.to_csv(save_to_dir, index_label=\"id\")\n",
    "    \n",
    "#     list_equal.append((yearidx_target[0], (preds_argmax_df_old == preds_argmax_df_new).all().all()))\n",
    "#     print(yearidx_target[0], (preds_argmax_df_old == preds_argmax_df_new).all().all())\n",
    "\n",
    "# list_equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e457fffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "(preds_argmax_df_old == preds_argmax_df_new).all().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab46489",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be714c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e047828",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ALTERNATIVE PREDICTION\n",
    "preds = model(dm.X_test) #returns list of batch predictions.\n",
    "# preds = torch.cat(preds) #preds is a list already of [batch_size, num_classes]. \n",
    "preds_argmax = preds.argmax(dim=1).numpy()\n",
    "preds_argmax_df2 = pd.DataFrame(preds_argmax, columns=[\"pred\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1ec8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34e820e",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240fd8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "def collect_preds(exp_dir: Path) -> None:\n",
    "    \"\"\"Copies all predictions????.csv to a 'predictions' folder within the\n",
    "    experiment_directory.\"\"\"\n",
    "    preds_dir = exp_dir/\"predictions\"\n",
    "    preds_dir.mkdir(exist_ok=True, parents=True)\n",
    "    # For all objects in exp_dir.\n",
    "    for dir in exp_dir.iterdir():\n",
    "        if dir.is_dir() and dir.name != \"predictions\":\n",
    "            # See https://docs.python.org/3/library/fnmatch.html#module-fnmatch\n",
    "            # for filename pattern matching below.\n",
    "            for file in dir.glob(\"prediction[1,2]???.csv\"): #[1,2]??? for years 1995,..,2000,..\n",
    "                # If files do not exist in 'predictions' folder yet\n",
    "                if not (preds_dir/(file.name)).is_file():\n",
    "#                     print(f\"Copy file: '{file.relative_to(Path.cwd())}'\"\n",
    "#                         f\"to '{preds_dir.relative_to(Path.cwd())}'\")\n",
    "                    try:\n",
    "                        shutil.copy2(file, preds_dir)\n",
    "                    except shutil.SameFileError:\n",
    "                        print(\"Source and Destination are the same file...\")\n",
    "                else:\n",
    "                    print(f\"File {file.name} already exists in '{preds_dir.name}' folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32706993",
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_preds(exp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a7392f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read from loop folder directly.\n",
    "read_preds = pd.read_csv(best_path.parent/f\"prediction{str(yearidx_target[1])[-4:]}.csv\", index_col=\"id\")\n",
    "# Read from collected 'predictions' folder.\n",
    "read_preds2 = pd.read_csv(exp_dir/\"predictions\"/f\"prediction{str(yearidx_target[1])[-4:]}.csv\", index_col=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c68f9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (read_preds == read_preds2).all().all(), \"Predictions from loop folder different than in the 'predictions'.\"\n",
    "assert (preds_argmax_df_old == read_preds).all().all(), \"Predictions from loaded model not equal to the saved predictions.\"\n",
    "assert (preds_argmax_df_new == read_preds).all().all(), \"Predictions from loaded model not equal to the saved predictions.\"\n",
    "assert preds_argmax_df_new.equals(read_preds), \"Predictions from loaded model not equal to the saved predictions.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b881b407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99e4ee5c",
   "metadata": {},
   "source": [
    "## Randomize Feature and test significance of difference of monthly PF returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad529cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dca729",
   "metadata": {},
   "source": [
    "### Compare balanced accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0f1105",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assuming we have all predictions from permutation in a dataframe, like so:\n",
    "preds_permut = pd.read_csv(exp_dir/\"all_pred.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e27dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_permut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5931b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Original preds\n",
    "preds_orig = pd.read_csv(exp_dir/\"all_pred.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45096b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d704cb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calc. difference in bal_accuracy. Significant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62e3dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25b5065",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert dm.data.iloc[-len(preds_orig):].equals(dm.data[dm.data[\"date\"] > \"2008\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213adbfc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_true = dm.y[-len(preds_orig):].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3226d92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0baf705",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_orig.iloc[33000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bedc93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds_orig[\"pred\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a68cebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (len(y_true) == len(y_true) == len(y_true)):\n",
    "    print(\"NOT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7876f11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_true == y_true).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e9407d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b1d34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_accuracy_score(y_true, preds_permut[\"pred\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1e50fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_accuracy_score(y_true, preds_orig[\"pred\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9a964b",
   "metadata": {},
   "source": [
    "### Compare monthly returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9123b870",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dm.X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8066ccc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "yearidx_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca58eb9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load original PF returns.\n",
    "pd.read_parquet(Path(r\"C:\\Users\\Mathiass\\Documents\\Projects\\master-thesis\\data\")/\"final_df_call_cao_small.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5707cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad3e493",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.y[-len(preds_orig):].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d4dab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.y[234731:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ffa6fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62b4976",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dataframe_image as dfi\n",
    "from tqdm import tqdm\n",
    "\n",
    "def collect_preds(exp_dir: Path) -> None:\n",
    "    \"\"\"Copies all predictions????.csv to a 'predictions' folder within the\n",
    "    experiment_directory.\"\"\"\n",
    "    preds_dir = exp_dir/\"predictions\"\n",
    "    preds_dir.mkdir(exist_ok=True, parents=True)\n",
    "    # For all objects in exp_dir.\n",
    "    for dir in exp_dir.iterdir():\n",
    "        if dir.is_dir() and dir.name != \"predictions\":\n",
    "            # See https://docs.python.org/3/library/fnmatch.html#module-fnmatch\n",
    "            # for filename pattern matching below.\n",
    "            for file in dir.glob(\"prediction[1,2]???.csv\"): #[1,2]??? for years 1995,..,2000,..\n",
    "                # If files do not exist in 'predictions' folder yet\n",
    "                if not (preds_dir/(file.name)).is_file():\n",
    "                    print(f\"Copy file: '{file.relative_to(Path.cwd())}'\"\n",
    "                        f\"to '{preds_dir.relative_to(Path.cwd())}'\")\n",
    "                    try:\n",
    "                        shutil.copy2(file, preds_dir)\n",
    "                    except shutil.SameFileError:\n",
    "                        print(\"Source and Destination are the same file...\")\n",
    "                else:\n",
    "                    print(f\"File {file.name} already exists in '{preds_dir.name}' folder.\")\n",
    "\n",
    "\n",
    "def concat_and_save_preds(exp_dir: Path) -> pd.DataFrame:\n",
    "    \"\"\"Read prediction????.csv files from the 'predictions' folder in the experiment\n",
    "    directory and return the concatenated pandas dataframe.\n",
    "    \n",
    "    Also, make sure years of 'prediction????.csv' are read in ascending order\n",
    "    and consecutively, i.e. 2009, 2010, 2011, ... and not 2009, 2011, ... .\n",
    "    \"\"\"\n",
    "    preds_dir = exp_dir/\"predictions\"\n",
    "    preds = []\n",
    "    prev_year = 0 # to make sure that files are read from lowest years to top years.\n",
    "    for idx, file in enumerate(sorted(preds_dir.glob(\"*.csv\"), reverse=False)): \n",
    "        # MUST BE reverse=False <=> ascending!, so that years are read in order ->2003->2004, etc.\n",
    "        if not idx: # For first year (equiv. to: if idx == 0).\n",
    "            year = int(file.stem[-4:]) \n",
    "            assert year > prev_year, \"ERROR: year is not a positive integer\"\n",
    "            prev_year = year\n",
    "        else: # For remaining years: must be consecutive, i.e. 2010, 2011, etc.\n",
    "            year = int(file.stem[-4:]) \n",
    "            assert year == prev_year + 1, \"ERROR: year is not succeeding previous year\"\n",
    "            prev_year = year\n",
    "        pred_df = pd.read_csv(file)\n",
    "        preds.append(pred_df)\n",
    "    # Return concatenated dataframe.\n",
    "    preds_concat_df = pd.concat(preds).reset_index(drop=True)\n",
    "    preds_concat_df.to_csv(exp_dir/\"all_pred.csv\")\n",
    "    return preds_concat_df\n",
    "\n",
    "\n",
    "def check_month_years(dic, dates):\n",
    "    \"\"\"Checks whether all end of month indeces in the dictionary 'dic'\n",
    "    are in the correct year. Also, checks whether all indeces are in\n",
    "    consecutive order. 31.12.2019[31.01.2020,......,31.12.2020, 31.01.2021]\n",
    "    \n",
    "    The last month of the eom indeces overlaps with the first entry in\n",
    "    the next year.\n",
    "\n",
    "    ---\n",
    "    Example:\n",
    "        If a year has 12 months in the data, the end of month indeces should \n",
    "        have length 13. The first index is the first \"row\" of the year, \n",
    "        the last index is the first row of the next year.\n",
    "    \"\"\"\n",
    "    for year in dic.keys():\n",
    "        len_dic = len(dic[year])\n",
    "        for idx, eom_idx in enumerate(dic[year]):\n",
    "            # Special case: last eom_idx is first eom_idx of next year.\n",
    "            if idx == len_dic - 1: #idx uses zero indexing.\n",
    "                if int(year) != dates[eom_idx-1].year or (idx)!= dates[eom_idx-1].month:\n",
    "                    return False\n",
    "            elif int(year) != dates[eom_idx].year or (idx+1) != dates[eom_idx].month:\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "# Checks whether rows with id of 0 correspond to start of new years.\n",
    "def check_eoy(concat_df: pd.DataFrame, eoy_indeces: np.ndarray):\n",
    "    \"\"\"Checks whether start of year (eoy indeces) rows correspond to id 0 in \n",
    "    concatenated predictions.\n",
    "    \n",
    "    \"\"\"\n",
    "    id_eq_zero = np.where(concat_df.loc[:, \"id\"] == 0)[0] #np.where returns tuple\n",
    "    for idx_idx, idx in enumerate(id_eq_zero):\n",
    "        if concat_df.iloc[idx][\"index\"] != eoy_indeces[idx_idx]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_and_check_min_max_pred(concat_df: pd.DataFrame, labelfn_exp: str):\n",
    "    \"\"\"Checks whether the predictions contain at least one of the smallest and\n",
    "    at least one of the largest class in each month (so that we can form Long-\n",
    "    Short Portfolios.)\n",
    "\n",
    "    Arguments: \n",
    "        concat_df:      Dataframe with option returns and the direction prediction. \n",
    "        labelfn_exp:    The label_fn of the experiment. Should be a string 'binary' \n",
    "                        or 'multi{number of classes}'.\n",
    "    Returns:\n",
    "        max_real:       Max realized prediction over all the data.\n",
    "        min_real:       Min realized prediction over all the data.\n",
    "        \"\"\"\n",
    "    classes = sorted(concat_df[\"pred\"].unique(), reverse=False) #ascending order\n",
    "    # Min pred value theoretically.\n",
    "    min_theor = 0\n",
    "    if labelfn_exp==\"binary\":\n",
    "        max_theor = 1 \n",
    "    else: #multi3, multi5, multi10 -> take (3, 5, 10) - 1\n",
    "        max_theor = int(labelfn_exp[5:]) - 1 # 3 classes -> 0, 1, 2\n",
    "    assert len(classes) == max_theor + 1, \"At least one class is not predicted at all.\"\n",
    "    assert classes[0] == min_theor and classes[-1] == max_theor, \"List 'classes' is not sorted in ascending order.\"\n",
    "    # Min pred value realized per month.\n",
    "    min_real_series = concat_df.groupby(\"date\")[\"pred\"].min()\n",
    "    min_real = min_real_series.min()\n",
    "    # print(\"Min prediction realized is:\", min_real)\n",
    "    assert min_theor == min_real, (\n",
    "        \"Not a single month has the prediction of the theoretical minimum class.\")\n",
    "    months_no_min = min_real_series[min_real_series != min_real].count()\n",
    "    print(f\"Number of months where Short class {min_real} is not predicted:\", \n",
    "            months_no_min, \"out of\", f\"{len(min_real_series)}.\")\n",
    "    # Max pred value realized per month.\n",
    "    max_real_series = concat_df.groupby(\"date\")[\"pred\"].max()\n",
    "    max_real = max_real_series.max()\n",
    "    # max_real_series[max_real_series != max_real].index.strftime(\"%Y-%m-%d\").to_list()\n",
    "    assert max_theor == max_real, (\n",
    "        \"Not a single month has the prediction of the theoretical maximum class.\")\n",
    "    months_no_max = max_real_series[max_real_series != max_real].count()\n",
    "    print(f\"Number of months where Long class {max_real} is not predicted:\", \n",
    "            months_no_max, \"out of\", f\"{len(max_real_series)}.\")\n",
    "    return max_real, min_real, classes\n",
    "\n",
    "\n",
    "def various_tests(agg_dict: dict, concat_df: pd.DataFrame, col_list: list, classes: list, class_ignore: dict):\n",
    "    \"\"\"Perform various sanity checks on our monthly aggregated results in agg_dict.\"\"\"\n",
    "    # Test1: Compare agg_dict with agg_dict2, calculated via 'weighted_avg' function \n",
    "    # and not via 'np.average'. They should yield the same (up to small precision).\n",
    "    agg_dict2 = {}\n",
    "    for c in tqdm(classes):\n",
    "        agg_df = concat_df.groupby(\"date\").aggregate(weighted_means_by_column2, col_list, f\"weights_{c}\")\n",
    "        agg_dict2[f\"class{c}\"] = agg_df\n",
    "    for key in agg_dict.keys():\n",
    "        pd.testing.assert_frame_equal(agg_dict[key], agg_dict2[key])\n",
    "    print(\"Test1: Successful! Weighted_avg function seems to yield the same as np.average.\")\n",
    "\n",
    "    # COPY CRUCIAL HERE! Otherwise, input df will be altered...\n",
    "    agg_dict_copy = agg_dict.copy() #copy because we drop class_ignore months for each class.\n",
    "    concat_df_copy = concat_df.copy()\n",
    "    # Drop 'class_ignore' rows:\n",
    "    for c in classes:\n",
    "        agg_dict_copy[f\"class{c}\"] = agg_dict_copy[f\"class{c}\"].drop(class_ignore[f\"class{c}\"])\n",
    "    # Test2: Check whether first and last month aggregation yield same as first \n",
    "    # and last entries of agg_dict_copy for each class.\n",
    "    for c in classes:\n",
    "        concat_df_copy_c = concat_df_copy[~concat_df_copy[\"date\"].isin(class_ignore[f\"class{c}\"])]\n",
    "        first_month = concat_df_copy_c.loc[concat_df_copy_c[\"date\"] == concat_df_copy_c[\"date\"].iloc[0]]\n",
    "        last_month = concat_df_copy_c.loc[concat_df_copy_c[\"date\"] == concat_df_copy_c[\"date\"].iloc[-1]]\n",
    "        for k in col_list:\n",
    "            assert np.average(first_month[k], weights=first_month[f\"weights_{c}\"]) == agg_dict_copy[f\"class{c}\"].iloc[0][k]\n",
    "            assert np.average(last_month[k], weights=last_month[f\"weights_{c}\"]) == agg_dict_copy[f\"class{c}\"].iloc[-1][k]\n",
    "            assert (weighted_avg(first_month, k, f\"weights_{c}\") - agg_dict_copy[f\"class{c}\"].iloc[0][k]) < 0.0001\n",
    "            assert (weighted_avg(last_month, k, f\"weights_{c}\") - agg_dict_copy[f\"class{c}\"].iloc[-1][k]) < 0.0001\n",
    "    print(\"Test2: Successful! First and last month individual aggregation (of non-to-ignore months) yield the same \"\n",
    "         \"as first and last entries of the aggregated dataframe for the respective class.\")\n",
    "\n",
    "    # Test3: If \"pred\" column in aggregated df's corresponds to class in each row (month).\n",
    "    for c in classes:\n",
    "        assert (agg_dict_copy[f\"class{c}\"][\"pred\"] == c).all(), \"Aggregated 'pred' is not equal to the class in at least one month.\"\n",
    "    print(\"Test3: Successful! Aggregated 'pred' column is equal to the class in each month.\")\n",
    "    # Test4: If short and low portfolios are aggregated correctly.\n",
    "    assert ((agg_dict_copy[f\"class{classes[0]}\"][\"if_long_short\"] == -1).all() and\n",
    "            (agg_dict_copy[f\"class{classes[-1]}\"][\"if_long_short\"] == 1).all()), (\"Long \"\n",
    "            \"or short portfolio aggregation does not yield 1 or -1 in 'if_long_short' column.\")\n",
    "    print(\"Test4: Successful! Both the lowest class and the highest class corrrespond \"\n",
    "        \"to -1 and 1 in the column 'if_long_short', respectively.\")\n",
    "    # Test5: Check if one-hot encoding columns correspond to 'preds' and 'if_long_short'.\n",
    "    for c in classes:\n",
    "        for k in classes:\n",
    "            if c == k:\n",
    "                assert (agg_dict_copy[f\"class{c}\"][f\"weights_{k}\"] == 1).all()\n",
    "                assert (agg_dict_copy[f\"class{c}\"][\"pred\"] == k).all()\n",
    "                if c==classes[0]:\n",
    "                    assert (agg_dict_copy[f\"class{c}\"][\"if_long_short\"] == -1).all()\n",
    "                elif c==classes[-1]:\n",
    "                    assert (agg_dict_copy[f\"class{c}\"][\"if_long_short\"] == 1).all()\n",
    "                else:\n",
    "                    assert (agg_dict_copy[f\"class{c}\"][\"pred\"] == k).all()\n",
    "            else:\n",
    "                assert (agg_dict_copy[f\"class{c}\"][f\"weights_{k}\"] == 0).all()\n",
    "    print(\"Test5: Successful! Check whether one-hot encoding columns make sense \"\n",
    "        \"with the columns 'preds' and 'if_long_short'.\")\n",
    "\n",
    "\n",
    "# Weighted average functions used to aggreagte portfolios. We use np.average.\n",
    "def weighted_means_by_column(x, cols, w):\n",
    "    \"\"\" This takes a DataFrame and averages each data column (cols)\n",
    "        while weighting observations by column w.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.Series([np.average(x[c], weights=x[w] ) for c in cols], cols)\n",
    "    except ZeroDivisionError:\n",
    "        series = pd.Series(0, cols) # set all values to 0 for those months with no prediction.\n",
    "        return series\n",
    "\n",
    "\n",
    "# Only used for testing:\n",
    "def weighted_avg(df, values, weights):\n",
    "    if df[weights].sum() == 0:\n",
    "        raise ZeroDivisionError\n",
    "    return sum(df[values] * df[weights]) / df[weights].sum()\n",
    "\n",
    "def weighted_means_by_column2(x, cols, w):\n",
    "    \"\"\" This takes a DataFrame and averages each data column (cols)\n",
    "        while weighting observations by column w.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.Series([weighted_avg(x, c, weights=w) for c in cols], cols)\n",
    "    except ZeroDivisionError:\n",
    "        series = pd.Series(0, cols) # set all values to 0 for those months with no prediction.\n",
    "        return series\n",
    "# ---\n",
    "\n",
    "\n",
    "def export_dfi(perfstats: pd.DataFrame, path: str) -> None:\n",
    "    \"\"\"dfi tries exporting the dataframe with Google Chrome first. On Linux\n",
    "    this can fail, so then it tries exporting with table_conversion=matplotlib.\"\"\"\n",
    "    try:\n",
    "        dfi.export(perfstats, path)\n",
    "        return\n",
    "    except OSError:\n",
    "        print(\"Exporting performance stats via chrome failed. Trying with \"\n",
    "            \"table conversion='matplotlib'...\")\n",
    "    try:\n",
    "        dfi.export(perfstats, path, table_conversion=\"matplotlib\")\n",
    "        return\n",
    "    except OSError as err:\n",
    "        raise OSError(\"Try different dataframe .png exporter.\") from err\n",
    "\n",
    "\n",
    "def get_class_ignore_dates(concat_df: pd.DataFrame, classes: list) -> dict:\n",
    "    \"\"\"For each class get months where there was no prediction for it at all.\n",
    "    \n",
    "        Returns:\n",
    "            class_ignore (dict): DatetimeIndeces for each class in a dictionary.\n",
    "    \"\"\"\n",
    "    class_ignore = {}\n",
    "    for c in classes:\n",
    "        sum_onehot = concat_df.groupby(\"date\")[f\"weights_{c}\"].sum()\n",
    "        nr_months_noclass = sum_onehot[sum_onehot==0].count()\n",
    "        months_noclass = sum_onehot[sum_onehot==0].index #Datetimeindex of months.\n",
    "        if c == classes[0]: #short class, save month indeces to exlude.\n",
    "            if not nr_months_noclass:\n",
    "                print(f\"Short Class {c} was predicted in every month.\")\n",
    "            else:\n",
    "                print(f\"Short Class {c}, was not predicted in the following {nr_months_noclass} months:\", \n",
    "                months_noclass.strftime(\"%Y-%m-%d\").tolist())\n",
    "        elif c == classes[-1]: #long class, save month indeces to exclude.\n",
    "            if not nr_months_noclass:\n",
    "                print(f\"Short Class {c} was predicted in every month.\")\n",
    "            else:\n",
    "                print(f\"Long Class {c} was not predicted in the following {nr_months_noclass} months:\", \n",
    "                months_noclass.strftime(\"%Y-%m-%d\").tolist())\n",
    "        else: #remaining classes, just print info.\n",
    "            if not nr_months_noclass:\n",
    "                print(f\"Class {c} was predicted in every month.\")\n",
    "            else:\n",
    "                print(f\"Class {c}, was not predicted in the following {nr_months_noclass} months:\", \n",
    "                months_noclass.strftime(\"%Y-%m-%d\").tolist())\n",
    "        class_ignore[f\"class{c}\"] = months_noclass\n",
    "    return class_ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5ee4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(preds_concat_df: pd.DataFrame, exp_dir: Path, datapath: Path):\n",
    "    # Get path where datasets reside:\n",
    "    print(\"Concat the dataframe with the respective option data...\")\n",
    "    df_small = pd.read_parquet(datapath/\"final_df_call_cao_small.parquet\")\n",
    "    print(datapath)\n",
    "    dates = df_small[\"date\"]\n",
    "    # Get args from experiment.\n",
    "    # Alternatively: Load json with json.load() and convert dict/list to df.\n",
    "    args_exp = pd.read_json(exp_dir/\"args.json\", typ=\"series\")\n",
    "    # Get start of year index and all end of month indeces. Load with args that \n",
    "    # were used in the actual experiment.\n",
    "    eoy_indeces, eom_indeces = YearMonthEndIndeces(\n",
    "                                dates=dates, \n",
    "                                init_train_length=args_exp[\"init_train_length\"],\n",
    "                                val_length=args_exp[\"val_length\"],\n",
    "                                test_length=args_exp[\"test_length\"]\n",
    "                                ).get_indeces()\n",
    "    # Slice df_small to prediction period.\n",
    "    # Get first month of first year of eventual predictions.\n",
    "    preds_start_idx = list(eom_indeces.values())[0][0]\n",
    "    df_small = df_small.iloc[preds_start_idx:]\n",
    "    # Make sure df_small and preds_concat_df are of same length.\n",
    "    assert len(preds_concat_df) == len(df_small), (\"length of prediction dataframe \"\n",
    "                                    \"is not equal the sliced option return dataframe\")\n",
    "    # Align indeces with preds_concat_df, but dont drop old index.\n",
    "    df_small = df_small.reset_index(drop=False)\n",
    "    # Concatenate option return data and predictions.\n",
    "    concat_df = pd.concat([df_small, preds_concat_df], axis=1)\n",
    "    # Checks whether rows with id of 0 correspond to start of new years.\n",
    "    assert check_eoy(concat_df, eoy_indeces), (\"Id 0 and eoy indeces do not match.\")\n",
    "    # Set df_small index back to main index.\n",
    "    concat_df = concat_df.set_index(\"index\", drop=True)\n",
    "    print(\"Done.\")\n",
    "\n",
    "    # Create single weight column 'if_long_short' with -1 for lowest and 1 for \n",
    "    # highest predicted class. Rest is 0.\n",
    "    print(\"Create weight columns for each class...\")\n",
    "    max_pred, min_pred, classes = get_and_check_min_max_pred(concat_df, args_exp[\"label_fn\"])\n",
    "    # 1.5x faster than pd.map...\n",
    "    condlist = [concat_df[\"pred\"] == min_pred, concat_df[\"pred\"] == max_pred]\n",
    "    choicelist = [-1, 1]\n",
    "    no_alloc_value = 0\n",
    "    concat_df[\"if_long_short\"] = np.select(condlist, choicelist, no_alloc_value)\n",
    "    # Create separate weight columns for each class in concat_df.\n",
    "    for c in classes:\n",
    "        condlist = [concat_df[\"pred\"] == c]\n",
    "        choicelist = [1]\n",
    "        no_alloc_value = 0\n",
    "        concat_df[f\"weights_{c}\"] = np.select(condlist, choicelist, no_alloc_value)\n",
    "\n",
    "    # Only calculate weighted average for numerical columns (have to drop 'date').\n",
    "    col_list = [val for val in concat_df.columns.tolist() if \"date\" not in val]\n",
    "    print(\"Done.\")\n",
    "    # Aggregate and collect all portfolios in a dictionary with key 'class0', 'class1', etc.\n",
    "    print(\"Aggregate for each class and collect the dataframes...\")\n",
    "    agg_dict = {}\n",
    "    for c in classes:\n",
    "        agg_df = concat_df.groupby(\"date\").aggregate(weighted_means_by_column, col_list, f\"weights_{c}\")\n",
    "        agg_dict[f\"class{c}\"] = agg_df\n",
    "    print(\"Done.\")\n",
    "    \n",
    "    print(\"Which classes were not predicted at all in a respective month?...\")\n",
    "    # For each class print out months where no prediction was allocated for that class, \n",
    "    # and save these indeces for short and long class to later ignore the returns of \n",
    "    # these months.\n",
    "    class_ignore = get_class_ignore_dates(concat_df, classes) #returns dict\n",
    "    print(\"Done.\")\n",
    "    \n",
    "    # Perform various tests to check our calculations.\n",
    "    test_concat = concat_df.copy()\n",
    "    test_agg_dict = agg_dict.copy()\n",
    "    print(\"Sanity test the aggregated results...\")\n",
    "    various_tests(agg_dict, concat_df, col_list, classes, class_ignore)\n",
    "    print(\"Done.\")\n",
    "    # Make sure tests did not alter dataframes.\n",
    "    pd.testing.assert_frame_equal(test_concat, concat_df)\n",
    "    for c in classes:\n",
    "        pd.testing.assert_frame_equal(test_agg_dict[f\"class{c}\"], agg_dict[f\"class{c}\"])\n",
    "\n",
    "#     print(\"Save each dataframe in the 'portfolios' subfolder...\")\n",
    "    # Save all aggregated dataframes per class to 'portfolios' subfolder within the \n",
    "    # experiment directory 'exp_dir'.\n",
    "#     pf_dir = exp_dir/\"portfolios\"\n",
    "#     try: # raise error if 'portfolio' folder exists already\n",
    "#         pf_dir.mkdir(exist_ok=False, parents=False) # raise error if parents are missing.\n",
    "#         for class_c, df in agg_dict.items():\n",
    "#             df.to_csv(pf_dir/f\"{class_c}.csv\")\n",
    "#     except FileExistsError as err: # from 'exist_ok' -> portfolios folder already exists, do nothing.\n",
    "#         raise FileExistsError(\"Directory 'portfolios' already exists. Will not \"\n",
    "#         \"touch folder and exit code.\") from err\n",
    "#     print(\"Done.\")\n",
    "\n",
    "    print(\"Create Long Short Portfolio while ignoring months where one side \"\n",
    "        \"is not allocated...\")\n",
    "    # Long-Short PF (highest class (long) - lowest class (short))\n",
    "    short_class = classes[0] #should be 0\n",
    "    assert short_class == 0, \"Class of short portfolio not 0. Check why.\"\n",
    "    long_class = classes[-1] #should be 2 for binary, 3 for 'multi3', etc.\n",
    "    print(f\"Subtract Short portfolio (class {short_class}) from Long portfolio \"\n",
    "            f\"(class {long_class}) and save to long{long_class}short{short_class}.csv...\")\n",
    "    # Subtract short from long portfolio.\n",
    "    long_df = agg_dict[f\"class{long_class}\"].copy() #deep copy to not change original agg_dict\n",
    "    short_df = agg_dict[f\"class{short_class}\"].copy() #deep copy to not change original agg_dict\n",
    "    months_no_inv = class_ignore[f\"class{long_class}\"].union(class_ignore[f\"class{short_class}\"]) #union of months to set to 0.\n",
    "    long_df.loc[months_no_inv, :] = 0\n",
    "    short_df.loc[months_no_inv, :] = 0\n",
    "    long_short_df = long_df - short_df #months that are 0 in both dfs stay 0 everywhere.\n",
    "    assert ((long_short_df.drop(months_no_inv)[\"pred\"] == (long_class - short_class)).all() and #'pred' should be long_class - short_class\n",
    "            (long_short_df.drop(months_no_inv)[\"if_long_short\"] == 2).all()) #'if_long_short' should be 2 (1 - (-1) = 2)\n",
    "    # Drop one-hot \"weight\" columns here.\n",
    "    cols_to_keep = [col for col in long_short_df.columns.tolist() if \"weight\" not in col]\n",
    "    long_short_df = long_short_df[cols_to_keep]\n",
    "    return long_short_df[\"option_ret\"]\n",
    "#     long_short_df.to_csv(pf_dir/f\"long{long_class}short{short_class}.csv\")\n",
    "    print(\"Done.\")\n",
    "    print(\"All done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13d0bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68171262",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(r\"C:\\Users\\Mathiass\\Documents\\Projects\\master-thesis\\data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7824257",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_df_permuted = aggregate(preds_orig, exp_dir, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0d7a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_df_permuted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10141eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_orig = pd.read_csv(exp_dir/\"portfolios\"/\"long4short0.csv\")[\"option_ret\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75c4e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = ret_orig - ls_df_permuted.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01f672a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09396c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.ones_like(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2c9c74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc33cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcdbb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ols = sm.OLS(y, X) #long_short_return regressed on X.\n",
    "# ols_result = ols.fit()\n",
    "ols_result = ols.fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": 5}, use_t=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28bd272",
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6dfcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e3845a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707db81d",
   "metadata": {},
   "source": [
    "### For large samples t-test is similar to z-test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d790ade9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P value for t score\n",
    "import scipy.stats\n",
    "\n",
    "#find p-value for two-tailed test\n",
    "scipy.stats.t.sf(abs(-0.486), df=165)*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd716246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zscore\n",
    "import scipy.stats as st\n",
    "(1 - st.norm.cdf(abs(-0.425)))*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715e3811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is z value significant at the 10% level?\n",
    "(ols_result.summary2().tables[1].z > 1.645) | (ols_result.summary2().tables[1].z < -1.645)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ede799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is z value significant at the 5% level?\n",
    "(ols_result.summary2().tables[1].z > 1.96) | (ols_result.summary2().tables[1].z < -1.96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a04f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is z value significant at the 1% level?\n",
    "(ols_result.summary2().tables[1].z > 2.58) | (ols_result.summary2().tables[1].z < -2.58)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3687e763",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65b1a62d",
   "metadata": {},
   "source": [
    "### Permute features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0049e1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a617132a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original small/medium/big dataset.\n",
    "df_orig = pd.read_parquet(Path(r\"C:\\Users\\Mathiass\\Documents\\Projects\\master-thesis\\data\")/\"final_df_call_cao_small.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d43686b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_orig.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1540aadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = df.columns.tolist()\n",
    "features_list.remove(\"date\")\n",
    "features_list.remove(\"option_ret\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b4faf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"moneyness\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11937213",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df[\"moneyness\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc29b51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"moneyness\"] > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c91f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"impl_volatility\"] == 2.860694]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17af790",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa500546",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d20b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"moneyness\"] = np.random.permutation(df[\"moneyness\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed6afe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2736bd73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"moneyness\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984c151c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"moneyness\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d74c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"moneyness\"].plot.hist(bins=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01936ae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df[df[\"moneyness\"] > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7610aca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884fbc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55bc65a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b520bb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d3363c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 300/300 [00:00<00:00, 293171.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(a):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16901750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2a4b6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a89e225",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('masterthesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "137ad5de30c222602b906d427f317b23725154a9d2ac1dd9f95e9d3b5697fcc3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
