{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "082587ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1161467",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback, TuneReportCheckpointCallback\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune import CLIReporter\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff29d44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3151c880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineer(data):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    data: pandas.DataFrame that must have specific columns.\n",
    "\n",
    "    \"\"\"\n",
    "    # Bid-Ask spread: (Ask - Bid) / Ask\n",
    "    data[\"best_bid\"] = (data[\"best_offer\"] - data[\"best_bid\"]) / (data[\"best_offer\"])\n",
    "    data = data.rename(columns={\"best_bid\": \"ba_spread_option\"}).drop([\"best_offer\"], axis=1)\n",
    "\n",
    "    # Gamma: multiply by spotprice and divide by 100\n",
    "    data[\"gamma\"] = data[\"gamma\"] * data[\"spotprice\"] / 100 #following Bali et al. (2021)\n",
    "\n",
    "    # Theta: scale by spotprice\n",
    "    data[\"theta\"] = data[\"theta\"] / data[\"spotprice\"] #following Bali et al. (2021)\n",
    "\n",
    "    # Vega: scale by spotprice\n",
    "    data[\"vega\"] = data[\"vega\"] / data[\"spotprice\"] #following Bali et al. (2021)\n",
    "\n",
    "    # Time to Maturity: cale by number of days in year: 365\n",
    "    data[\"days_to_exp\"] = data[\"days_to_exp\"] / 365\n",
    "\n",
    "    # Moneyness: Strike / Spot (K / S)\n",
    "    data[\"strike_price\"] = data[\"strike_price\"] / data[\"spotprice\"] # K / S\n",
    "    data = data.rename(columns={\"strike_price\": \"moneyness\"})\n",
    "\n",
    "    # Forward Price ratio: Forward / Spot\n",
    "    data[\"forwardprice\"] = data[\"forwardprice\"] / data[\"spotprice\"]\n",
    "\n",
    "    # Drop redundant/ unimportant columns\n",
    "    data = data.drop([\"cfadj\", \"days_no_trading\", \"spotprice\", \"adj_spot\"], axis=1)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebd83d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary y label generator\n",
    "def binary_categorize(y):\n",
    "    \"\"\"\n",
    "    Input: continuous target variable \n",
    "\n",
    "    Output: 1 for positive returns, \n",
    "            0 for negative returns\n",
    "    \"\"\"\n",
    "    if y > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "# multiclass y label generator\n",
    "def multi_categorize(y):\n",
    "    \"\"\"\n",
    "    Input: continuous target variable\n",
    "    CAREFUL: classes have to be between [0, C) for F.crossentropyloss.\n",
    "    \n",
    "    Output: multi class\n",
    "    \"\"\"\n",
    "    if y > 0.05:\n",
    "        return 2\n",
    "    elif y < -0.05:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4eaea3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVSplitter:\n",
    "    \"\"\" Generator for data splits\n",
    "    Args:\n",
    "    dates: pandas.Series of datetime,\n",
    "    init_train_length: int,\n",
    "    val_length: int\n",
    "    \"\"\"\n",
    "    def __init__(self, dates, init_train_length=10, val_length=2, test_length=1):\n",
    "        # find indeces where years change (will ignore last year end in dates)\n",
    "        self.val_length = val_length\n",
    "        self.test_length = test_length\n",
    "        self.eoy_idx =  np.where((dates.dt.year.diff() == 1))[0]\n",
    "        self.eoy_idx = np.append(self.eoy_idx, len(dates)) #append end of year of last year in dates\n",
    "\n",
    "        assert init_train_length + val_length + test_length <= len(self.eoy_idx) + 1, \\\n",
    "        \"defined train and val are larger than number of years in dataset\"\n",
    "        assert init_train_length > 0, \"init_train_length must be strictly greater than 0\"\n",
    "\n",
    "        # align the 4th idx to be the end of the 5th year...\n",
    "        self.train_start_idx = init_train_length - 1\n",
    "\n",
    "        self.train_eoy = self.eoy_idx[self.train_start_idx:-(val_length+test_length)]\n",
    "        self.val_eoy = self.eoy_idx[self.train_start_idx + val_length:-test_length]\n",
    "        # For generate_idx():\n",
    "        self.test_eoy = self.eoy_idx[self.train_start_idx + val_length + test_length:]\n",
    "\n",
    "    def generate(self):\n",
    "        for i in range(len(self.eoy_idx) - (self.train_start_idx + self.val_length)):\n",
    "            yield (list(range(self.train_eoy[i])),\n",
    "                   list(range(self.train_eoy[i], self.val_eoy[i])))\n",
    "\n",
    "    def generate_idx(self):\n",
    "        for i in range(len(self.eoy_idx) - (self.train_start_idx + self.val_length \n",
    "                        + self.test_length)):\n",
    "            yield ({\"train\": self.train_eoy[i], \n",
    "                    \"val\": self.val_eoy[i], \n",
    "                    \"test\": self.test_eoy[i]}\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "787b79c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from disk\n",
    "path = Path(r\"C:\\Users\\Mathiass\\OneDrive - UniversitÃ¤t ZÃ¼rich UZH\\Documents\\mt_literature\\data\")\n",
    "\n",
    "class Dataset():\n",
    "    def __init__(self, path=path, year_idx=0, dataset=\"small\", init_train_length=23, val_length=2, label_fn=\"binary\"):\n",
    "        if dataset == \"small\":\n",
    "            self.data = pd.read_parquet(path/\"final_df_filledmean_small.parquet\")\n",
    "        elif dataset == \"big\":\n",
    "            self.data = pd.read_parquet(path/\"final_df_filledmean.parquet\")\n",
    "        else:\n",
    "            raise ValueError(\"Specify dataset as either 'small' or 'big'\")\n",
    "\n",
    "        # get splits\n",
    "        splitter = CVSplitter(self.data[\"date\"], init_train_length=init_train_length, \n",
    "                                val_length=val_length, test_length=1)\n",
    "        eoy_indeces = list(splitter.generate_idx())\n",
    "        self.eoy_train = eoy_indeces[year_idx][\"train\"]\n",
    "        self.eoy_val = eoy_indeces[year_idx][\"val\"]\n",
    "        self.eoy_test = eoy_indeces[year_idx][\"test\"]\n",
    "        \n",
    "        # Truncate data\n",
    "        self.data = self.data.iloc[:self.eoy_test]\n",
    "        assert len(self.data) == self.eoy_test, \"length of data is not equal to eoy_test\"\n",
    "            \n",
    "        # feature engineer data\n",
    "        self.data = feature_engineer(self.data)\n",
    "        \n",
    "        # create y\n",
    "        self.y = self.data[\"option_ret\"]\n",
    "        # make classification problem\n",
    "        if label_fn == \"binary\":\n",
    "            self.y = self.y.apply(binary_categorize)\n",
    "        elif label_fn == \"multi\":\n",
    "            self.y = self.y.apply(multi_categorize)\n",
    "        else:\n",
    "            raise ValueError(\"Specify label_fn as either 'binary' or 'multi'\")\n",
    "        # create X\n",
    "        self.X = self.data.drop([\"option_ret\"], axis=1)\n",
    "        \n",
    "        # save dates and drop\n",
    "        self.dates = self.X[\"date\"]\n",
    "        self.X = self.X.drop([\"date\"], axis=1)\n",
    "        \n",
    "#         # to torch Tensor\n",
    "#         self.X = torch.from_numpy(self.X.values).float() #-> will be standardized in setup, so do it there.\n",
    "#         self.y = torch.from_numpy(self.y.values)\n",
    "\n",
    "        # to numpy\n",
    "        self.X = self.X.values #-> will be standardized in setup, so do it there.\n",
    "        self.y = self.y.values\n",
    "    \n",
    "        ############################### setup #########################################################\n",
    "        # train\n",
    "        # self.X_train = self.X[self.dates < self.hparams.start_val]\n",
    "        self.X_train = self.X[:self.eoy_train]\n",
    "        self.y_train = self.y[:len(self.X_train)]\n",
    "        \n",
    "        #val\n",
    "        # mask = (self.dates >= self.hparams.start_val) & (self.dates < self.hparams.start_test)\n",
    "        # self.X_val = self.X[mask]\n",
    "        self.X_val = self.X[self.eoy_train:self.eoy_val]\n",
    "        self.y_val = self.y[len(self.X_train):len(self.X_train)+len(self.X_val)]\n",
    "        \n",
    "        # test\n",
    "        self.X_test = self.X[self.eoy_val:self.eoy_test]\n",
    "        self.y_test = self.y[-len(self.X_test):]\n",
    "        \n",
    "        assert (len(self.X_train)+len(self.X_val)+len(self.X_test)) == len(self.data), \\\n",
    "            \"sum of X train, val, test is not equal length of dataset\"\n",
    "        assert (len(self.y_train)+len(self.y_val)+len(self.y_test) == len(self.data)), \\\n",
    "        \"sum of y train, val, test is not equal to length of dataset\"\n",
    "        \n",
    "#         #standardize X_train\n",
    "#         mean = torch.mean(self.X_train, axis=0)\n",
    "#         std = torch.std(self.X_train, axis=0)\n",
    "        \n",
    "#         # Standardize X_train, X_val and X_test with mean/std from X_train\n",
    "#         self.X_train = (self.X_train - mean) / std\n",
    "#         self.X_val = (self.X_val - mean) / std\n",
    "#         self.X_test = (self.X_test - mean) / std\n",
    "\n",
    "        # Save variables\n",
    "        # input dim\n",
    "        self.input_dim = self.X_train.shape[1]\n",
    "        # number of classes\n",
    "        self.num_classes = len(np.unique(self.y_train))\n",
    "#         class weights\n",
    "        self.class_weights = len(self.y_train) / np.unique(self.y_train, return_counts=True)[1]\n",
    "        \n",
    "        print(\"*****************************************************************************************\")\n",
    "        print(\"Current dataset information:\")\n",
    "        print(\"---\")\n",
    "        print(\"class_weights:\", self.class_weights)\n",
    "        print(\"---\")\n",
    "        print(f\"# of input data: {len(self.data)} with shape: {self.data.shape}\")\n",
    "        print(f\"# of training samples: {len(self.y_train)} with X_train of shape: {self.X_train.shape}\")\n",
    "        print(f\"# of validation samples: {len(self.y_val)} with X_val of shape: {self.X_val.shape}\")\n",
    "        print(f\"# of test samples: {len(self.y_test)} with X_test of shape: {self.X_test.shape}\")\n",
    "        print(\"---\")\n",
    "        print(f\"train start date: \", self.dates.iloc[0].strftime(\"%Y-%m-%d\"), \n",
    "              \", train end date: \", self.dates.iloc[:self.eoy_train].iloc[-1].strftime(\"%Y-%m-%d\"))\n",
    "        print(f\"val start date: \", self.dates.iloc[self.eoy_train:self.eoy_val].iloc[0].strftime(\"%Y-%m-%d\"), \n",
    "              \", val end date: \", self.dates.iloc[self.eoy_train:self.eoy_val].iloc[-1].strftime(\"%Y-%m-%d\"))\n",
    "        print(f\"test start date: \", self.dates.iloc[self.eoy_val:self.eoy_test].iloc[0].strftime(\"%Y-%m-%d\"), \n",
    "              \", test end date: \", self.dates.iloc[self.eoy_val:self.eoy_test].iloc[-1].strftime(\"%Y-%m-%d\"))\n",
    "        print(\"*****************************************************************************************\")\n",
    "        \n",
    "    def get_datasets(self):\n",
    "        return self.X_train, self.X_val, self.X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67adb0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_lr_tune(config, data):\n",
    "    # needed for reproducibility, will seed trainer (init of weights in NN?)\n",
    "    pl.seed_everything(42, workers=True)\n",
    "\n",
    "    data = data\n",
    "    \n",
    "    clf = make_pipeline(StandardScaler(), \n",
    "                        SGDClassifier(\n",
    "                        max_iter=config[\"num_epochs\"],\n",
    "                        alpha=config[\"alpha\"],\n",
    "#                         epsilon=config[\"epsilon\"],\n",
    "                        tol=1e-3,\n",
    "                        verbose=3,\n",
    "                        n_jobs=-1,\n",
    "                        ))\n",
    "    \n",
    "    clf.fit(data.X_train, data.y_train)\n",
    "    \n",
    "    y_pred = clf.predict(data.X_val)\n",
    "    \n",
    "    val_bal_acc = balanced_accuracy_score(data.y_val, y_pred)\n",
    "    \n",
    "    tune.report(val_bal_acc=val_bal_acc)\n",
    "\n",
    "    return {\"val_bal_acc\": val_bal_acc}\n",
    "\n",
    "\n",
    "def lr_tune():\n",
    "    \n",
    "    # Example parameters to tune from SGDClassifier\n",
    "    config = {\n",
    "                \"alpha\": tune.choice([1e-4, 1e-1, 1]),\n",
    "                \"num_epochs\": tune.choice([100, 1000]),\n",
    "#               \"epsilon\": [0.01, 0.1\n",
    "             }\n",
    "\n",
    "    scheduler = ASHAScheduler(\n",
    "        max_t=1000,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "\n",
    "    reporter = CLIReporter(\n",
    "        parameter_columns=[\"alpha\"],\n",
    "        metric_columns=[\"val_bal_acc\"])\n",
    "    \n",
    "    data = Dataset()\n",
    "\n",
    "    train_fn_with_parameters = tune.with_parameters(inner_lr_tune,\n",
    "                                                    data=data,\n",
    "#                                                     args=args,\n",
    "#                                                     year_idx=year_idx,\n",
    "#                                                     ckpt_path=ckpt_path,\n",
    "                                                   )\n",
    "    resources_per_trial = {\"cpu\": 8, \"gpu\": 0}\n",
    "\n",
    "    # Set logging directory for tune.run\n",
    "    log_dir = \"./logs/tune/lr_loops\"\n",
    "    time = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "#     # name = time+\"_\"+string_from_config(config) # config into path \n",
    "#     # CAREFUL: will give error if directory path is too large\n",
    "#     train_year_end = 1996 + args.init_train_length + year_idx - 1\n",
    "#     val_year_end = train_year_end + args.val_length\n",
    "#     years = f\"train{train_year_end}_val{val_year_end}\"\n",
    "    name = time\n",
    "\n",
    "#     # save config space as .json\n",
    "#     summary_path = Path.cwd()/log_dir/time\n",
    "#     summary_path.mkdir(exist_ok=True, parents=True)\n",
    "#     with open(summary_path/\"config.json\", 'w') as f:\n",
    "#         json.dump(serialize_config(config), fp=f, indent=3)\n",
    "\n",
    "#     # save args to json\n",
    "#     args_dict = serialize_args(args.__dict__) #functions are not serializable\n",
    "#     with open(summary_path/'args.json', 'w') as f:\n",
    "#         json.dump(args_dict, f, indent=3)\n",
    "\n",
    "\n",
    "    analysis = tune.run(train_fn_with_parameters,\n",
    "        local_dir=log_dir,\n",
    "        resources_per_trial=resources_per_trial,\n",
    "        metric=\"val_bal_acc\",\n",
    "        mode=\"max\",\n",
    "        config=config,\n",
    "        num_samples=10,\n",
    "        scheduler=scheduler,\n",
    "        progress_reporter=reporter,\n",
    "        name=name,\n",
    "        fail_fast=True, # stop all trials as soon as any trial errors\n",
    "#         keep_checkpoints_num=1, # only keep best checkpoint\n",
    "#         checkpoint_score_attr=\"min-val_loss\",\n",
    "        )\n",
    "\n",
    "#     print(\"Best hyperparameters found were: \", analysis.best_config)\n",
    "    \n",
    "#     best_last_trial = analysis.get_best_trial(\"val_bal_acc\", \"max\", \"last\") #change \"last\" to \"all\" for global min\n",
    "# #     print(\"Best trial among last epoch config: {}\".format(best_last_trial.config))\n",
    "# #     print(\"Best trial >>last epoch<< validation loss: {}\".format(\n",
    "# #         best_last_trial.last_result[\"val_loss\"]))\n",
    "#     print(\"Best trial >>last epoch<< validation balanced accuracy: {}\".format(\n",
    "#         best_last_trial.last_result[\"val_bal_acc\"]))\n",
    "    \n",
    "\n",
    "#     best_result_per_trial_df = analysis.dataframe(metric=\"val_loss\", \n",
    "#                                             mode=\"min\").sort_values(\"val_loss\")\n",
    "#     # save df to folder?\n",
    "#     best_result = best_result_per_trial_df.iloc[0, :].to_dict() #take best values of best trial\n",
    "\n",
    "#     # test prediction: best checkpoint out of all trials\n",
    "#     best_trial = analysis.get_best_trial(\"val_loss\", \"min\", scope=\"all\")\n",
    "#     if args.refit:\n",
    "#         config = best_trial.config\n",
    "#         ckpt_path = Path(analysis.get_best_checkpoint(best_trial).get_internal_representation()[1], \"checkpoint\")\n",
    "    \n",
    "#     if not args.no_predict:\n",
    "#         best_path = Path(analysis.get_best_checkpoint(best_trial).get_internal_representation()[1], \"checkpoint\")\n",
    "#         print(f\"Loading model from path at {ckpt_path}\")\n",
    "#         model = FFN.load_from_checkpoint(best_path)\n",
    "#         dm = MyDataModule_Loop(\n",
    "#             path=args.path_data,\n",
    "#             year_idx=year_idx,\n",
    "#             dataset=args.dataset,\n",
    "#             batch_size=1, #any number, doesnt matter for predict\n",
    "#             init_train_length=args.init_train_length,\n",
    "#             val_length=args.val_length,\n",
    "#             label_fn=args.label_fn,\n",
    "#             # config=model.hparams.config, # so that config is not hyperparam search again\n",
    "#         )\n",
    "#         trainer = pl.Trainer(\n",
    "#             deterministic=True,\n",
    "#             gpus=args.gpus_per_trial,\n",
    "#         )\n",
    "#         preds = trainer.predict(model=model, datamodule=dm)\n",
    "#         preds_argmax = preds[0].argmax(dim=1).numpy() # assumes batchsize is whole testset\n",
    "#         preds_argmax_df = pd.DataFrame(preds_argmax, columns=[\"pred\"])\n",
    "#         test_year_end = val_year_end + args.test_length\n",
    "#         # prediction path\n",
    "#         save_to_dir = Path(Path.cwd(),log_dir, name, f\"prediction{test_year_end}.csv\")\n",
    "#         preds_argmax_df.to_csv(save_to_dir, index_label=\"id\")\n",
    "        \n",
    "#     return best_result, summary_path, ckpt_path, config \n",
    "    #dictionary of best metrics and config, path\n",
    "    return analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76a9ce35",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************************************************************\n",
      "Current dataset information:\n",
      "---\n",
      "class_weights: [1.56424512 2.77227938]\n",
      "---\n",
      "# of input data: 3823386 with shape: (3823386, 17)\n",
      "# of training samples: 2780818 with X_train of shape: (2780818, 15)\n",
      "# of validation samples: 647992 with X_val of shape: (647992, 15)\n",
      "# of test samples: 394576 with X_test of shape: (394576, 15)\n",
      "---\n",
      "train start date:  1996-01-31 , train end date:  2018-12-31\n",
      "val start date:  2019-01-31 , val end date:  2020-12-31\n",
      "test start date:  2021-01-31 , test end date:  2021-11-30\n",
      "*****************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-14 21:23:07,753\tWARNING function_runner.py:603 -- Function checkpointing is disabled. This may result in unexpected behavior when using checkpointing features or certain schedulers. To enable, set the train function arguments to be `func(config, checkpoint_dir=None)`.\n",
      "2022-07-14 21:23:07,888\tWARNING tune.py:668 -- Tune detects GPUs, but no trials are using GPUs. To enable trials to use GPUs, set tune.run(resources_per_trial={'gpu': 1}...) which allows Tune to expose 1 GPU to each trial. You can also override `Trainable.default_resource_request` if using the Trainable API.\n",
      "2022-07-14 21:23:08,102\tERROR syncer.py:147 -- Log sync requires rsync to be installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-07-14 21:23:08 (running for 00:00:00.22)\n",
      "Memory usage on this node: 8.6/15.9 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 512.000: None | Iter 256.000: None | Iter 128.000: None | Iter 64.000: None | Iter 32.000: None | Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 8.0/8 CPUs, 0/1 GPUs, 0.0/5.62 GiB heap, 0.0/2.81 GiB objects\n",
      "Result logdir: C:\\Users\\Mathiass\\Documents\\Projects\\master-thesis\\notebooks\\logs\\tune\\lr_loops\\20220714212301\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+---------------------------+----------+-----------------+---------+\n",
      "| Trial name                | status   | loc             |   alpha |\n",
      "|---------------------------+----------+-----------------+---------|\n",
      "| inner_lr_tune_6421b_00000 | RUNNING  | 127.0.0.1:16244 |  1      |\n",
      "| inner_lr_tune_6421b_00001 | PENDING  |                 |  0.0001 |\n",
      "| inner_lr_tune_6421b_00002 | PENDING  |                 |  0.1    |\n",
      "| inner_lr_tune_6421b_00003 | PENDING  |                 |  1      |\n",
      "| inner_lr_tune_6421b_00004 | PENDING  |                 |  1      |\n",
      "| inner_lr_tune_6421b_00005 | PENDING  |                 |  0.0001 |\n",
      "| inner_lr_tune_6421b_00006 | PENDING  |                 |  0.1    |\n",
      "| inner_lr_tune_6421b_00007 | PENDING  |                 |  0.1    |\n",
      "| inner_lr_tune_6421b_00008 | PENDING  |                 |  0.1    |\n",
      "| inner_lr_tune_6421b_00009 | PENDING  |                 |  0.0001 |\n",
      "+---------------------------+----------+-----------------+---------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 1\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.00, NNZs: 15, Bias: -0.999998, T: 2780818, Avg. loss: 0.721456\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 0.60 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 2\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.00, NNZs: 15, Bias: -0.999999, T: 5561636, Avg. loss: 0.721429\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 1.16 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 3\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.00, NNZs: 15, Bias: -1.000000, T: 8342454, Avg. loss: 0.721429\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 1.72 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 4\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.00, NNZs: 15, Bias: -0.999999, T: 11123272, Avg. loss: 0.721429\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 2.28 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 5\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.00, NNZs: 15, Bias: -1.000000, T: 13904090, Avg. loss: 0.721428\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 2.89 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 6\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.00, NNZs: 15, Bias: -1.000000, T: 16684908, Avg. loss: 0.721428\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 3.51 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Convergence after 6 epochs took 3.51 seconds\n",
      "Result for inner_lr_tune_6421b_00000:\n",
      "  date: 2022-07-14_21-23-18\n",
      "  done: false\n",
      "  experiment_id: b810ce6045034f21a639a212ecdb9f5f\n",
      "  hostname: Mathias\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  pid: 16244\n",
      "  time_since_restore: 5.039806842803955\n",
      "  time_this_iter_s: 5.039806842803955\n",
      "  time_total_s: 5.039806842803955\n",
      "  timestamp: 1657826598\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 6421b_00000\n",
      "  val_bal_acc: 0.5\n",
      "  warmup_time: 0.00400090217590332\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-07-14 21:23:18 (running for 00:00:10.63)\n",
      "Memory usage on this node: 9.4/15.9 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 512.000: None | Iter 256.000: None | Iter 128.000: None | Iter 64.000: None | Iter 32.000: None | Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: 0.5\n",
      "Resources requested: 8.0/8 CPUs, 0/1 GPUs, 0.0/5.62 GiB heap, 0.0/2.81 GiB objects\n",
      "Current best trial: 6421b_00000 with val_bal_acc=0.5 and parameters={'alpha': 1}\n",
      "Result logdir: C:\\Users\\Mathiass\\Documents\\Projects\\master-thesis\\notebooks\\logs\\tune\\lr_loops\\20220714212301\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+---------------------------+----------+-----------------+---------+---------------+\n",
      "| Trial name                | status   | loc             |   alpha |   val_bal_acc |\n",
      "|---------------------------+----------+-----------------+---------+---------------|\n",
      "| inner_lr_tune_6421b_00000 | RUNNING  | 127.0.0.1:16244 |  1      |           0.5 |\n",
      "| inner_lr_tune_6421b_00001 | PENDING  |                 |  0.0001 |               |\n",
      "| inner_lr_tune_6421b_00002 | PENDING  |                 |  0.1    |               |\n",
      "| inner_lr_tune_6421b_00003 | PENDING  |                 |  1      |               |\n",
      "| inner_lr_tune_6421b_00004 | PENDING  |                 |  1      |               |\n",
      "| inner_lr_tune_6421b_00005 | PENDING  |                 |  0.0001 |               |\n",
      "| inner_lr_tune_6421b_00006 | PENDING  |                 |  0.1    |               |\n",
      "| inner_lr_tune_6421b_00007 | PENDING  |                 |  0.1    |               |\n",
      "| inner_lr_tune_6421b_00008 | PENDING  |                 |  0.1    |               |\n",
      "| inner_lr_tune_6421b_00009 | PENDING  |                 |  0.0001 |               |\n",
      "+---------------------------+----------+-----------------+---------+---------------+\n",
      "\n",
      "\n",
      "Result for inner_lr_tune_6421b_00000:\n",
      "  date: 2022-07-14_21-23-18\n",
      "  done: true\n",
      "  experiment_id: b810ce6045034f21a639a212ecdb9f5f\n",
      "  experiment_tag: 0_alpha=1,num_epochs=100\n",
      "  hostname: Mathias\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  pid: 16244\n",
      "  time_since_restore: 5.039806842803955\n",
      "  time_this_iter_s: 5.039806842803955\n",
      "  time_total_s: 5.039806842803955\n",
      "  timestamp: 1657826598\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 6421b_00000\n",
      "  val_bal_acc: 0.5\n",
      "  warmup_time: 0.00400090217590332\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 1\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.11, NNZs: 15, Bias: -0.977964, T: 2780818, Avg. loss: 0.842909\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 0.67 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 2\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.08, NNZs: 15, Bias: -0.991425, T: 5561636, Avg. loss: 0.735787\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 1.34 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 3\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.04, NNZs: 15, Bias: -0.998152, T: 8342454, Avg. loss: 0.729913\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 1.90 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 4\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.03, NNZs: 15, Bias: -0.994118, T: 11123272, Avg. loss: 0.727450\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 2.46 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 5\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.02, NNZs: 15, Bias: -0.999943, T: 13904090, Avg. loss: 0.726093\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 3.04 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 6\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.01, NNZs: 15, Bias: -0.996204, T: 16684908, Avg. loss: 0.725264\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 3.59 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 7\n",
      "== Status ==\n",
      "Current time: 2022-07-14 21:23:23 (running for 00:00:15.73)\n",
      "Memory usage on this node: 9.9/15.9 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 512.000: None | Iter 256.000: None | Iter 128.000: None | Iter 64.000: None | Iter 32.000: None | Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: 0.5\n",
      "Resources requested: 8.0/8 CPUs, 0/1 GPUs, 0.0/5.62 GiB heap, 0.0/2.81 GiB objects\n",
      "Current best trial: 6421b_00000 with val_bal_acc=0.5 and parameters={'alpha': 1}\n",
      "Result logdir: C:\\Users\\Mathiass\\Documents\\Projects\\master-thesis\\notebooks\\logs\\tune\\lr_loops\\20220714212301\n",
      "Number of trials: 10/10 (8 PENDING, 1 RUNNING, 1 TERMINATED)\n",
      "+---------------------------+------------+-----------------+---------+---------------+\n",
      "| Trial name                | status     | loc             |   alpha |   val_bal_acc |\n",
      "|---------------------------+------------+-----------------+---------+---------------|\n",
      "| inner_lr_tune_6421b_00001 | RUNNING    | 127.0.0.1:16244 |  0.0001 |               |\n",
      "| inner_lr_tune_6421b_00002 | PENDING    |                 |  0.1    |               |\n",
      "| inner_lr_tune_6421b_00003 | PENDING    |                 |  1      |               |\n",
      "| inner_lr_tune_6421b_00004 | PENDING    |                 |  1      |               |\n",
      "| inner_lr_tune_6421b_00005 | PENDING    |                 |  0.0001 |               |\n",
      "| inner_lr_tune_6421b_00006 | PENDING    |                 |  0.1    |               |\n",
      "| inner_lr_tune_6421b_00007 | PENDING    |                 |  0.1    |               |\n",
      "| inner_lr_tune_6421b_00008 | PENDING    |                 |  0.1    |               |\n",
      "| inner_lr_tune_6421b_00009 | PENDING    |                 |  0.0001 |               |\n",
      "| inner_lr_tune_6421b_00000 | TERMINATED | 127.0.0.1:16244 |  1      |           0.5 |\n",
      "+---------------------------+------------+-----------------+---------+---------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.01, NNZs: 15, Bias: -0.992594, T: 19465726, Avg. loss: 0.724676\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 4.17 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 8\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.01, NNZs: 15, Bias: -0.997153, T: 22246544, Avg. loss: 0.724236\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 4.71 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 9\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.01, NNZs: 15, Bias: -1.001932, T: 25027362, Avg. loss: 0.723913\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 5.25 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 10\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.01, NNZs: 15, Bias: -0.996797, T: 27808180, Avg. loss: 0.723644\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 5.80 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Convergence after 10 epochs took 5.80 seconds\n",
      "Result for inner_lr_tune_6421b_00001:\n",
      "  date: 2022-07-14_21-23-25\n",
      "  done: false\n",
      "  experiment_id: b810ce6045034f21a639a212ecdb9f5f\n",
      "  hostname: Mathias\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  pid: 16244\n",
      "  time_since_restore: 7.183361291885376\n",
      "  time_this_iter_s: 7.183361291885376\n",
      "  time_total_s: 7.183361291885376\n",
      "  timestamp: 1657826605\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 6421b_00001\n",
      "  val_bal_acc: 0.5\n",
      "  warmup_time: 0.00400090217590332\n",
      "  \n",
      "Result for inner_lr_tune_6421b_00001:\n",
      "  date: 2022-07-14_21-23-25\n",
      "  done: true\n",
      "  experiment_id: b810ce6045034f21a639a212ecdb9f5f\n",
      "  experiment_tag: 1_alpha=0.0001,num_epochs=100\n",
      "  hostname: Mathias\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  pid: 16244\n",
      "  time_since_restore: 7.183361291885376\n",
      "  time_this_iter_s: 7.183361291885376\n",
      "  time_total_s: 7.183361291885376\n",
      "  timestamp: 1657826605\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 6421b_00001\n",
      "  val_bal_acc: 0.5\n",
      "  warmup_time: 0.00400090217590332\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 1\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.00, NNZs: 15, Bias: -0.999981, T: 2780818, Avg. loss: 0.721657\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 0.74 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 2\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.00, NNZs: 15, Bias: -0.999993, T: 5561636, Avg. loss: 0.721443\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 1.37 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 3\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.00, NNZs: 15, Bias: -0.999998, T: 8342454, Avg. loss: 0.721437\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 2.01 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 4\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.00, NNZs: 15, Bias: -0.999995, T: 11123272, Avg. loss: 0.721434\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 2.60 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 5\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.00, NNZs: 15, Bias: -0.999999, T: 13904090, Avg. loss: 0.721433\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 3.19 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 6\n",
      "== Status ==\n",
      "Current time: 2022-07-14 21:23:30 (running for 00:00:22.98)\n",
      "Memory usage on this node: 9.7/15.9 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 512.000: None | Iter 256.000: None | Iter 128.000: None | Iter 64.000: None | Iter 32.000: None | Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: 0.5\n",
      "Resources requested: 8.0/8 CPUs, 0/1 GPUs, 0.0/5.62 GiB heap, 0.0/2.81 GiB objects\n",
      "Current best trial: 6421b_00000 with val_bal_acc=0.5 and parameters={'alpha': 1}\n",
      "Result logdir: C:\\Users\\Mathiass\\Documents\\Projects\\master-thesis\\notebooks\\logs\\tune\\lr_loops\\20220714212301\n",
      "Number of trials: 10/10 (7 PENDING, 1 RUNNING, 2 TERMINATED)\n",
      "+---------------------------+------------+-----------------+---------+---------------+\n",
      "| Trial name                | status     | loc             |   alpha |   val_bal_acc |\n",
      "|---------------------------+------------+-----------------+---------+---------------|\n",
      "| inner_lr_tune_6421b_00002 | RUNNING    | 127.0.0.1:16244 |  0.1    |               |\n",
      "| inner_lr_tune_6421b_00003 | PENDING    |                 |  1      |               |\n",
      "| inner_lr_tune_6421b_00004 | PENDING    |                 |  1      |               |\n",
      "| inner_lr_tune_6421b_00005 | PENDING    |                 |  0.0001 |               |\n",
      "| inner_lr_tune_6421b_00006 | PENDING    |                 |  0.1    |               |\n",
      "| inner_lr_tune_6421b_00007 | PENDING    |                 |  0.1    |               |\n",
      "| inner_lr_tune_6421b_00008 | PENDING    |                 |  0.1    |               |\n",
      "| inner_lr_tune_6421b_00009 | PENDING    |                 |  0.0001 |               |\n",
      "| inner_lr_tune_6421b_00000 | TERMINATED | 127.0.0.1:16244 |  1      |           0.5 |\n",
      "| inner_lr_tune_6421b_00001 | TERMINATED | 127.0.0.1:16244 |  0.0001 |           0.5 |\n",
      "+---------------------------+------------+-----------------+---------+---------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.00, NNZs: 15, Bias: -0.999996, T: 16684908, Avg. loss: 0.721432\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 3.78 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Convergence after 6 epochs took 3.78 seconds\n",
      "Result for inner_lr_tune_6421b_00002:\n",
      "  date: 2022-07-14_21-23-31\n",
      "  done: false\n",
      "  experiment_id: b810ce6045034f21a639a212ecdb9f5f\n",
      "  hostname: Mathias\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  pid: 16244\n",
      "  time_since_restore: 5.183724880218506\n",
      "  time_this_iter_s: 5.183724880218506\n",
      "  time_total_s: 5.183724880218506\n",
      "  timestamp: 1657826611\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 6421b_00002\n",
      "  val_bal_acc: 0.5\n",
      "  warmup_time: 0.00400090217590332\n",
      "  \n",
      "Result for inner_lr_tune_6421b_00002:\n",
      "  date: 2022-07-14_21-23-31\n",
      "  done: true\n",
      "  experiment_id: b810ce6045034f21a639a212ecdb9f5f\n",
      "  experiment_tag: 2_alpha=0.1000,num_epochs=1000\n",
      "  hostname: Mathias\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  pid: 16244\n",
      "  time_since_restore: 5.183724880218506\n",
      "  time_this_iter_s: 5.183724880218506\n",
      "  time_total_s: 5.183724880218506\n",
      "  timestamp: 1657826611\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 6421b_00002\n",
      "  val_bal_acc: 0.5\n",
      "  warmup_time: 0.00400090217590332\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 1\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.00, NNZs: 15, Bias: -0.999998, T: 2780818, Avg. loss: 0.721456\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 0.69 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 2\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.00, NNZs: 15, Bias: -0.999999, T: 5561636, Avg. loss: 0.721429\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 1.41 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 3\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.00, NNZs: 15, Bias: -1.000000, T: 8342454, Avg. loss: 0.721429\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 2.04 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 4\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.00, NNZs: 15, Bias: -0.999999, T: 11123272, Avg. loss: 0.721429\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 2.68 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 5\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.00, NNZs: 15, Bias: -1.000000, T: 13904090, Avg. loss: 0.721428\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 3.25 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 6\n",
      "== Status ==\n",
      "Current time: 2022-07-14 21:23:36 (running for 00:00:28.18)\n",
      "Memory usage on this node: 10.1/15.9 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 512.000: None | Iter 256.000: None | Iter 128.000: None | Iter 64.000: None | Iter 32.000: None | Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: 0.5\n",
      "Resources requested: 8.0/8 CPUs, 0/1 GPUs, 0.0/5.62 GiB heap, 0.0/2.81 GiB objects\n",
      "Current best trial: 6421b_00000 with val_bal_acc=0.5 and parameters={'alpha': 1}\n",
      "Result logdir: C:\\Users\\Mathiass\\Documents\\Projects\\master-thesis\\notebooks\\logs\\tune\\lr_loops\\20220714212301\n",
      "Number of trials: 10/10 (6 PENDING, 1 RUNNING, 3 TERMINATED)\n",
      "+---------------------------+------------+-----------------+---------+---------------+\n",
      "| Trial name                | status     | loc             |   alpha |   val_bal_acc |\n",
      "|---------------------------+------------+-----------------+---------+---------------|\n",
      "| inner_lr_tune_6421b_00003 | RUNNING    | 127.0.0.1:16244 |  1      |               |\n",
      "| inner_lr_tune_6421b_00004 | PENDING    |                 |  1      |               |\n",
      "| inner_lr_tune_6421b_00005 | PENDING    |                 |  0.0001 |               |\n",
      "| inner_lr_tune_6421b_00006 | PENDING    |                 |  0.1    |               |\n",
      "| inner_lr_tune_6421b_00007 | PENDING    |                 |  0.1    |               |\n",
      "| inner_lr_tune_6421b_00008 | PENDING    |                 |  0.1    |               |\n",
      "| inner_lr_tune_6421b_00009 | PENDING    |                 |  0.0001 |               |\n",
      "| inner_lr_tune_6421b_00000 | TERMINATED | 127.0.0.1:16244 |  1      |           0.5 |\n",
      "| inner_lr_tune_6421b_00001 | TERMINATED | 127.0.0.1:16244 |  0.0001 |           0.5 |\n",
      "| inner_lr_tune_6421b_00002 | TERMINATED | 127.0.0.1:16244 |  0.1    |           0.5 |\n",
      "+---------------------------+------------+-----------------+---------+---------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.00, NNZs: 15, Bias: -1.000000, T: 16684908, Avg. loss: 0.721428\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 3.86 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Convergence after 6 epochs took 3.86 seconds\n",
      "Result for inner_lr_tune_6421b_00003:\n",
      "  date: 2022-07-14_21-23-36\n",
      "  done: false\n",
      "  experiment_id: b810ce6045034f21a639a212ecdb9f5f\n",
      "  hostname: Mathias\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  pid: 16244\n",
      "  time_since_restore: 5.253567457199097\n",
      "  time_this_iter_s: 5.253567457199097\n",
      "  time_total_s: 5.253567457199097\n",
      "  timestamp: 1657826616\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 6421b_00003\n",
      "  val_bal_acc: 0.5\n",
      "  warmup_time: 0.00400090217590332\n",
      "  \n",
      "Result for inner_lr_tune_6421b_00003:\n",
      "  date: 2022-07-14_21-23-36\n",
      "  done: true\n",
      "  experiment_id: b810ce6045034f21a639a212ecdb9f5f\n",
      "  experiment_tag: 3_alpha=1,num_epochs=100\n",
      "  hostname: Mathias\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  pid: 16244\n",
      "  time_since_restore: 5.253567457199097\n",
      "  time_this_iter_s: 5.253567457199097\n",
      "  time_total_s: 5.253567457199097\n",
      "  timestamp: 1657826616\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 6421b_00003\n",
      "  val_bal_acc: 0.5\n",
      "  warmup_time: 0.00400090217590332\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 1\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.00, NNZs: 15, Bias: -0.999998, T: 2780818, Avg. loss: 0.721456\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 0.71 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 2\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.00, NNZs: 15, Bias: -0.999999, T: 5561636, Avg. loss: 0.721429\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 1.35 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 3\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.00, NNZs: 15, Bias: -1.000000, T: 8342454, Avg. loss: 0.721429\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 1.92 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 4\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.00, NNZs: 15, Bias: -0.999999, T: 11123272, Avg. loss: 0.721429\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 2.47 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 5\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.00, NNZs: 15, Bias: -1.000000, T: 13904090, Avg. loss: 0.721428\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 3.02 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 6\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.00, NNZs: 15, Bias: -1.000000, T: 16684908, Avg. loss: 0.721428\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 3.56 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Convergence after 6 epochs took 3.56 seconds\n",
      "Result for inner_lr_tune_6421b_00004:\n",
      "  date: 2022-07-14_21-23-41\n",
      "  done: false\n",
      "  experiment_id: b810ce6045034f21a639a212ecdb9f5f\n",
      "  hostname: Mathias\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  pid: 16244\n",
      "  time_since_restore: 4.930108070373535\n",
      "  time_this_iter_s: 4.930108070373535\n",
      "  time_total_s: 4.930108070373535\n",
      "  timestamp: 1657826621\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 6421b_00004\n",
      "  val_bal_acc: 0.5\n",
      "  warmup_time: 0.00400090217590332\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-07-14 21:23:41 (running for 00:00:33.42)\n",
      "Memory usage on this node: 9.6/15.9 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 512.000: None | Iter 256.000: None | Iter 128.000: None | Iter 64.000: None | Iter 32.000: None | Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: 0.5\n",
      "Resources requested: 8.0/8 CPUs, 0/1 GPUs, 0.0/5.62 GiB heap, 0.0/2.81 GiB objects\n",
      "Current best trial: 6421b_00000 with val_bal_acc=0.5 and parameters={'alpha': 1}\n",
      "Result logdir: C:\\Users\\Mathiass\\Documents\\Projects\\master-thesis\\notebooks\\logs\\tune\\lr_loops\\20220714212301\n",
      "Number of trials: 10/10 (5 PENDING, 1 RUNNING, 4 TERMINATED)\n",
      "+---------------------------+------------+-----------------+---------+---------------+\n",
      "| Trial name                | status     | loc             |   alpha |   val_bal_acc |\n",
      "|---------------------------+------------+-----------------+---------+---------------|\n",
      "| inner_lr_tune_6421b_00004 | RUNNING    | 127.0.0.1:16244 |  1      |           0.5 |\n",
      "| inner_lr_tune_6421b_00005 | PENDING    |                 |  0.0001 |               |\n",
      "| inner_lr_tune_6421b_00006 | PENDING    |                 |  0.1    |               |\n",
      "| inner_lr_tune_6421b_00007 | PENDING    |                 |  0.1    |               |\n",
      "| inner_lr_tune_6421b_00008 | PENDING    |                 |  0.1    |               |\n",
      "| inner_lr_tune_6421b_00009 | PENDING    |                 |  0.0001 |               |\n",
      "| inner_lr_tune_6421b_00000 | TERMINATED | 127.0.0.1:16244 |  1      |           0.5 |\n",
      "| inner_lr_tune_6421b_00001 | TERMINATED | 127.0.0.1:16244 |  0.0001 |           0.5 |\n",
      "| inner_lr_tune_6421b_00002 | TERMINATED | 127.0.0.1:16244 |  0.1    |           0.5 |\n",
      "| inner_lr_tune_6421b_00003 | TERMINATED | 127.0.0.1:16244 |  1      |           0.5 |\n",
      "+---------------------------+------------+-----------------+---------+---------------+\n",
      "\n",
      "\n",
      "Result for inner_lr_tune_6421b_00004:\n",
      "  date: 2022-07-14_21-23-41\n",
      "  done: true\n",
      "  experiment_id: b810ce6045034f21a639a212ecdb9f5f\n",
      "  experiment_tag: 4_alpha=1,num_epochs=100\n",
      "  hostname: Mathias\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  pid: 16244\n",
      "  time_since_restore: 4.930108070373535\n",
      "  time_this_iter_s: 4.930108070373535\n",
      "  time_total_s: 4.930108070373535\n",
      "  timestamp: 1657826621\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 6421b_00004\n",
      "  val_bal_acc: 0.5\n",
      "  warmup_time: 0.00400090217590332\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 1\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.11, NNZs: 15, Bias: -0.977964, T: 2780818, Avg. loss: 0.842909\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 0.65 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 2\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.08, NNZs: 15, Bias: -0.991425, T: 5561636, Avg. loss: 0.735787\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 1.29 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 3\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.04, NNZs: 15, Bias: -0.998152, T: 8342454, Avg. loss: 0.729913\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 1.85 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 4\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.03, NNZs: 15, Bias: -0.994118, T: 11123272, Avg. loss: 0.727450\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 2.41 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 5\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.02, NNZs: 15, Bias: -0.999943, T: 13904090, Avg. loss: 0.726093\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 2.97 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 6\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.01, NNZs: 15, Bias: -0.996204, T: 16684908, Avg. loss: 0.725264\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 3.52 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 7\n",
      "== Status ==\n",
      "Current time: 2022-07-14 21:23:46 (running for 00:00:38.51)\n",
      "Memory usage on this node: 10.2/15.9 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 512.000: None | Iter 256.000: None | Iter 128.000: None | Iter 64.000: None | Iter 32.000: None | Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: 0.5\n",
      "Resources requested: 8.0/8 CPUs, 0/1 GPUs, 0.0/5.62 GiB heap, 0.0/2.81 GiB objects\n",
      "Current best trial: 6421b_00000 with val_bal_acc=0.5 and parameters={'alpha': 1}\n",
      "Result logdir: C:\\Users\\Mathiass\\Documents\\Projects\\master-thesis\\notebooks\\logs\\tune\\lr_loops\\20220714212301\n",
      "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
      "+---------------------------+------------+-----------------+---------+---------------+\n",
      "| Trial name                | status     | loc             |   alpha |   val_bal_acc |\n",
      "|---------------------------+------------+-----------------+---------+---------------|\n",
      "| inner_lr_tune_6421b_00005 | RUNNING    | 127.0.0.1:16244 |  0.0001 |               |\n",
      "| inner_lr_tune_6421b_00006 | PENDING    |                 |  0.1    |               |\n",
      "| inner_lr_tune_6421b_00007 | PENDING    |                 |  0.1    |               |\n",
      "| inner_lr_tune_6421b_00008 | PENDING    |                 |  0.1    |               |\n",
      "| inner_lr_tune_6421b_00009 | PENDING    |                 |  0.0001 |               |\n",
      "| inner_lr_tune_6421b_00000 | TERMINATED | 127.0.0.1:16244 |  1      |           0.5 |\n",
      "| inner_lr_tune_6421b_00001 | TERMINATED | 127.0.0.1:16244 |  0.0001 |           0.5 |\n",
      "| inner_lr_tune_6421b_00002 | TERMINATED | 127.0.0.1:16244 |  0.1    |           0.5 |\n",
      "| inner_lr_tune_6421b_00003 | TERMINATED | 127.0.0.1:16244 |  1      |           0.5 |\n",
      "| inner_lr_tune_6421b_00004 | TERMINATED | 127.0.0.1:16244 |  1      |           0.5 |\n",
      "+---------------------------+------------+-----------------+---------+---------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.01, NNZs: 15, Bias: -0.992594, T: 19465726, Avg. loss: 0.724676\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 4.06 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 8\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.01, NNZs: 15, Bias: -0.997153, T: 22246544, Avg. loss: 0.724236\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 4.61 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 9\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.01, NNZs: 15, Bias: -1.001932, T: 25027362, Avg. loss: 0.723913\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 5.18 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 10\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.01, NNZs: 15, Bias: -0.996797, T: 27808180, Avg. loss: 0.723644\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 5.74 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Convergence after 10 epochs took 5.74 seconds\n",
      "Result for inner_lr_tune_6421b_00005:\n",
      "  date: 2022-07-14_21-23-48\n",
      "  done: false\n",
      "  experiment_id: b810ce6045034f21a639a212ecdb9f5f\n",
      "  hostname: Mathias\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  pid: 16244\n",
      "  time_since_restore: 7.103130578994751\n",
      "  time_this_iter_s: 7.103130578994751\n",
      "  time_total_s: 7.103130578994751\n",
      "  timestamp: 1657826628\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 6421b_00005\n",
      "  val_bal_acc: 0.5\n",
      "  warmup_time: 0.00400090217590332\n",
      "  \n",
      "Result for inner_lr_tune_6421b_00005:\n",
      "  date: 2022-07-14_21-23-48\n",
      "  done: true\n",
      "  experiment_id: b810ce6045034f21a639a212ecdb9f5f\n",
      "  experiment_tag: 5_alpha=0.0001,num_epochs=1000\n",
      "  hostname: Mathias\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  pid: 16244\n",
      "  time_since_restore: 7.103130578994751\n",
      "  time_this_iter_s: 7.103130578994751\n",
      "  time_total_s: 7.103130578994751\n",
      "  timestamp: 1657826628\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 6421b_00005\n",
      "  val_bal_acc: 0.5\n",
      "  warmup_time: 0.00400090217590332\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 1\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.00, NNZs: 15, Bias: -0.999981, T: 2780818, Avg. loss: 0.721657\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 0.65 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 2\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.00, NNZs: 15, Bias: -0.999993, T: 5561636, Avg. loss: 0.721443\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 1.31 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 3\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.00, NNZs: 15, Bias: -0.999998, T: 8342454, Avg. loss: 0.721437\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 1.89 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 4\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.00, NNZs: 15, Bias: -0.999995, T: 11123272, Avg. loss: 0.721434\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 2.44 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 5\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.00, NNZs: 15, Bias: -0.999999, T: 13904090, Avg. loss: 0.721433\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 2.99 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 6\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.00, NNZs: 15, Bias: -0.999996, T: 16684908, Avg. loss: 0.721432\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 3.54 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Convergence after 6 epochs took 3.54 seconds\n",
      "Result for inner_lr_tune_6421b_00006:\n",
      "  date: 2022-07-14_21-23-53\n",
      "  done: false\n",
      "  experiment_id: b810ce6045034f21a639a212ecdb9f5f\n",
      "  hostname: Mathias\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  pid: 16244\n",
      "  time_since_restore: 4.928755521774292\n",
      "  time_this_iter_s: 4.928755521774292\n",
      "  time_total_s: 4.928755521774292\n",
      "  timestamp: 1657826633\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 6421b_00006\n",
      "  val_bal_acc: 0.5\n",
      "  warmup_time: 0.00400090217590332\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-07-14 21:23:53 (running for 00:00:45.56)\n",
      "Memory usage on this node: 9.7/15.9 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 512.000: None | Iter 256.000: None | Iter 128.000: None | Iter 64.000: None | Iter 32.000: None | Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: 0.5\n",
      "Resources requested: 8.0/8 CPUs, 0/1 GPUs, 0.0/5.62 GiB heap, 0.0/2.81 GiB objects\n",
      "Current best trial: 6421b_00000 with val_bal_acc=0.5 and parameters={'alpha': 1}\n",
      "Result logdir: C:\\Users\\Mathiass\\Documents\\Projects\\master-thesis\\notebooks\\logs\\tune\\lr_loops\\20220714212301\n",
      "Number of trials: 10/10 (3 PENDING, 1 RUNNING, 6 TERMINATED)\n",
      "+---------------------------+------------+-----------------+---------+---------------+\n",
      "| Trial name                | status     | loc             |   alpha |   val_bal_acc |\n",
      "|---------------------------+------------+-----------------+---------+---------------|\n",
      "| inner_lr_tune_6421b_00006 | RUNNING    | 127.0.0.1:16244 |  0.1    |           0.5 |\n",
      "| inner_lr_tune_6421b_00007 | PENDING    |                 |  0.1    |               |\n",
      "| inner_lr_tune_6421b_00008 | PENDING    |                 |  0.1    |               |\n",
      "| inner_lr_tune_6421b_00009 | PENDING    |                 |  0.0001 |               |\n",
      "| inner_lr_tune_6421b_00000 | TERMINATED | 127.0.0.1:16244 |  1      |           0.5 |\n",
      "| inner_lr_tune_6421b_00001 | TERMINATED | 127.0.0.1:16244 |  0.0001 |           0.5 |\n",
      "| inner_lr_tune_6421b_00002 | TERMINATED | 127.0.0.1:16244 |  0.1    |           0.5 |\n",
      "| inner_lr_tune_6421b_00003 | TERMINATED | 127.0.0.1:16244 |  1      |           0.5 |\n",
      "| inner_lr_tune_6421b_00004 | TERMINATED | 127.0.0.1:16244 |  1      |           0.5 |\n",
      "| inner_lr_tune_6421b_00005 | TERMINATED | 127.0.0.1:16244 |  0.0001 |           0.5 |\n",
      "+---------------------------+------------+-----------------+---------+---------------+\n",
      "\n",
      "\n",
      "Result for inner_lr_tune_6421b_00006:\n",
      "  date: 2022-07-14_21-23-53\n",
      "  done: true\n",
      "  experiment_id: b810ce6045034f21a639a212ecdb9f5f\n",
      "  experiment_tag: 6_alpha=0.1000,num_epochs=100\n",
      "  hostname: Mathias\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  pid: 16244\n",
      "  time_since_restore: 4.928755521774292\n",
      "  time_this_iter_s: 4.928755521774292\n",
      "  time_total_s: 4.928755521774292\n",
      "  timestamp: 1657826633\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 6421b_00006\n",
      "  val_bal_acc: 0.5\n",
      "  warmup_time: 0.00400090217590332\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 1\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.00, NNZs: 15, Bias: -0.999981, T: 2780818, Avg. loss: 0.721657\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 0.78 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 2\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.00, NNZs: 15, Bias: -0.999993, T: 5561636, Avg. loss: 0.721443\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 1.47 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 3\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.00, NNZs: 15, Bias: -0.999998, T: 8342454, Avg. loss: 0.721437\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 2.07 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 4\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.00, NNZs: 15, Bias: -0.999995, T: 11123272, Avg. loss: 0.721434\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 2.67 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 5\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.00, NNZs: 15, Bias: -0.999999, T: 13904090, Avg. loss: 0.721433\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 3.23 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 6\n",
      "== Status ==\n",
      "Current time: 2022-07-14 21:23:58 (running for 00:00:50.65)\n",
      "Memory usage on this node: 10.1/15.9 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 512.000: None | Iter 256.000: None | Iter 128.000: None | Iter 64.000: None | Iter 32.000: None | Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: 0.5\n",
      "Resources requested: 8.0/8 CPUs, 0/1 GPUs, 0.0/5.62 GiB heap, 0.0/2.81 GiB objects\n",
      "Current best trial: 6421b_00000 with val_bal_acc=0.5 and parameters={'alpha': 1}\n",
      "Result logdir: C:\\Users\\Mathiass\\Documents\\Projects\\master-thesis\\notebooks\\logs\\tune\\lr_loops\\20220714212301\n",
      "Number of trials: 10/10 (2 PENDING, 1 RUNNING, 7 TERMINATED)\n",
      "+---------------------------+------------+-----------------+---------+---------------+\n",
      "| Trial name                | status     | loc             |   alpha |   val_bal_acc |\n",
      "|---------------------------+------------+-----------------+---------+---------------|\n",
      "| inner_lr_tune_6421b_00007 | RUNNING    | 127.0.0.1:16244 |  0.1    |               |\n",
      "| inner_lr_tune_6421b_00008 | PENDING    |                 |  0.1    |               |\n",
      "| inner_lr_tune_6421b_00009 | PENDING    |                 |  0.0001 |               |\n",
      "| inner_lr_tune_6421b_00000 | TERMINATED | 127.0.0.1:16244 |  1      |           0.5 |\n",
      "| inner_lr_tune_6421b_00001 | TERMINATED | 127.0.0.1:16244 |  0.0001 |           0.5 |\n",
      "| inner_lr_tune_6421b_00002 | TERMINATED | 127.0.0.1:16244 |  0.1    |           0.5 |\n",
      "| inner_lr_tune_6421b_00003 | TERMINATED | 127.0.0.1:16244 |  1      |           0.5 |\n",
      "| inner_lr_tune_6421b_00004 | TERMINATED | 127.0.0.1:16244 |  1      |           0.5 |\n",
      "| inner_lr_tune_6421b_00005 | TERMINATED | 127.0.0.1:16244 |  0.0001 |           0.5 |\n",
      "| inner_lr_tune_6421b_00006 | TERMINATED | 127.0.0.1:16244 |  0.1    |           0.5 |\n",
      "+---------------------------+------------+-----------------+---------+---------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.00, NNZs: 15, Bias: -0.999996, T: 16684908, Avg. loss: 0.721432\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 3.81 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Convergence after 6 epochs took 3.81 seconds\n",
      "Result for inner_lr_tune_6421b_00007:\n",
      "  date: 2022-07-14_21-23-58\n",
      "  done: false\n",
      "  experiment_id: b810ce6045034f21a639a212ecdb9f5f\n",
      "  hostname: Mathias\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  pid: 16244\n",
      "  time_since_restore: 5.260483980178833\n",
      "  time_this_iter_s: 5.260483980178833\n",
      "  time_total_s: 5.260483980178833\n",
      "  timestamp: 1657826638\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 6421b_00007\n",
      "  val_bal_acc: 0.5\n",
      "  warmup_time: 0.00400090217590332\n",
      "  \n",
      "Result for inner_lr_tune_6421b_00007:\n",
      "  date: 2022-07-14_21-23-58\n",
      "  done: true\n",
      "  experiment_id: b810ce6045034f21a639a212ecdb9f5f\n",
      "  experiment_tag: 7_alpha=0.1000,num_epochs=100\n",
      "  hostname: Mathias\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  pid: 16244\n",
      "  time_since_restore: 5.260483980178833\n",
      "  time_this_iter_s: 5.260483980178833\n",
      "  time_total_s: 5.260483980178833\n",
      "  timestamp: 1657826638\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 6421b_00007\n",
      "  val_bal_acc: 0.5\n",
      "  warmup_time: 0.00400090217590332\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 1\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.00, NNZs: 15, Bias: -0.999981, T: 2780818, Avg. loss: 0.721657\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 0.66 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 2\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.00, NNZs: 15, Bias: -0.999993, T: 5561636, Avg. loss: 0.721443\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 1.28 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 3\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.00, NNZs: 15, Bias: -0.999998, T: 8342454, Avg. loss: 0.721437\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 1.88 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 4\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.00, NNZs: 15, Bias: -0.999995, T: 11123272, Avg. loss: 0.721434\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 2.46 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 5\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.00, NNZs: 15, Bias: -0.999999, T: 13904090, Avg. loss: 0.721433\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 3.06 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 6\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.00, NNZs: 15, Bias: -0.999996, T: 16684908, Avg. loss: 0.721432\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 3.68 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Convergence after 6 epochs took 3.68 seconds\n",
      "== Status ==\n",
      "Current time: 2022-07-14 21:24:03 (running for 00:00:55.97)\n",
      "Memory usage on this node: 9.9/15.9 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 512.000: None | Iter 256.000: None | Iter 128.000: None | Iter 64.000: None | Iter 32.000: None | Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: 0.5\n",
      "Resources requested: 8.0/8 CPUs, 0/1 GPUs, 0.0/5.62 GiB heap, 0.0/2.81 GiB objects\n",
      "Current best trial: 6421b_00000 with val_bal_acc=0.5 and parameters={'alpha': 1}\n",
      "Result logdir: C:\\Users\\Mathiass\\Documents\\Projects\\master-thesis\\notebooks\\logs\\tune\\lr_loops\\20220714212301\n",
      "Number of trials: 10/10 (1 PENDING, 1 RUNNING, 8 TERMINATED)\n",
      "+---------------------------+------------+-----------------+---------+---------------+\n",
      "| Trial name                | status     | loc             |   alpha |   val_bal_acc |\n",
      "|---------------------------+------------+-----------------+---------+---------------|\n",
      "| inner_lr_tune_6421b_00008 | RUNNING    | 127.0.0.1:16244 |  0.1    |               |\n",
      "| inner_lr_tune_6421b_00009 | PENDING    |                 |  0.0001 |               |\n",
      "| inner_lr_tune_6421b_00000 | TERMINATED | 127.0.0.1:16244 |  1      |           0.5 |\n",
      "| inner_lr_tune_6421b_00001 | TERMINATED | 127.0.0.1:16244 |  0.0001 |           0.5 |\n",
      "| inner_lr_tune_6421b_00002 | TERMINATED | 127.0.0.1:16244 |  0.1    |           0.5 |\n",
      "| inner_lr_tune_6421b_00003 | TERMINATED | 127.0.0.1:16244 |  1      |           0.5 |\n",
      "| inner_lr_tune_6421b_00004 | TERMINATED | 127.0.0.1:16244 |  1      |           0.5 |\n",
      "| inner_lr_tune_6421b_00005 | TERMINATED | 127.0.0.1:16244 |  0.0001 |           0.5 |\n",
      "| inner_lr_tune_6421b_00006 | TERMINATED | 127.0.0.1:16244 |  0.1    |           0.5 |\n",
      "| inner_lr_tune_6421b_00007 | TERMINATED | 127.0.0.1:16244 |  0.1    |           0.5 |\n",
      "+---------------------------+------------+-----------------+---------+---------------+\n",
      "\n",
      "\n",
      "Result for inner_lr_tune_6421b_00008:\n",
      "  date: 2022-07-14_21-24-03\n",
      "  done: false\n",
      "  experiment_id: b810ce6045034f21a639a212ecdb9f5f\n",
      "  hostname: Mathias\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  pid: 16244\n",
      "  time_since_restore: 5.064195156097412\n",
      "  time_this_iter_s: 5.064195156097412\n",
      "  time_total_s: 5.064195156097412\n",
      "  timestamp: 1657826643\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 6421b_00008\n",
      "  val_bal_acc: 0.5\n",
      "  warmup_time: 0.00400090217590332\n",
      "  \n",
      "Result for inner_lr_tune_6421b_00008:\n",
      "  date: 2022-07-14_21-24-03\n",
      "  done: true\n",
      "  experiment_id: b810ce6045034f21a639a212ecdb9f5f\n",
      "  experiment_tag: 8_alpha=0.1000,num_epochs=100\n",
      "  hostname: Mathias\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  pid: 16244\n",
      "  time_since_restore: 5.064195156097412\n",
      "  time_this_iter_s: 5.064195156097412\n",
      "  time_total_s: 5.064195156097412\n",
      "  timestamp: 1657826643\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 6421b_00008\n",
      "  val_bal_acc: 0.5\n",
      "  warmup_time: 0.00400090217590332\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 1\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.11, NNZs: 15, Bias: -0.977964, T: 2780818, Avg. loss: 0.842909\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 0.65 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 2\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.08, NNZs: 15, Bias: -0.991425, T: 5561636, Avg. loss: 0.735787\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 1.29 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 3\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.04, NNZs: 15, Bias: -0.998152, T: 8342454, Avg. loss: 0.729913\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 1.87 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 4\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.03, NNZs: 15, Bias: -0.994118, T: 11123272, Avg. loss: 0.727450\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 2.47 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 5\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.02, NNZs: 15, Bias: -0.999943, T: 13904090, Avg. loss: 0.726093\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 3.08 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 6\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.01, NNZs: 15, Bias: -0.996204, T: 16684908, Avg. loss: 0.725264\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 3.64 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 7\n",
      "== Status ==\n",
      "Current time: 2022-07-14 21:24:08 (running for 00:01:01.11)\n",
      "Memory usage on this node: 10.3/15.9 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 512.000: None | Iter 256.000: None | Iter 128.000: None | Iter 64.000: None | Iter 32.000: None | Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: 0.5\n",
      "Resources requested: 8.0/8 CPUs, 0/1 GPUs, 0.0/5.62 GiB heap, 0.0/2.81 GiB objects\n",
      "Current best trial: 6421b_00000 with val_bal_acc=0.5 and parameters={'alpha': 1}\n",
      "Result logdir: C:\\Users\\Mathiass\\Documents\\Projects\\master-thesis\\notebooks\\logs\\tune\\lr_loops\\20220714212301\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+---------------------------+------------+-----------------+---------+---------------+\n",
      "| Trial name                | status     | loc             |   alpha |   val_bal_acc |\n",
      "|---------------------------+------------+-----------------+---------+---------------|\n",
      "| inner_lr_tune_6421b_00009 | RUNNING    | 127.0.0.1:16244 |  0.0001 |               |\n",
      "| inner_lr_tune_6421b_00000 | TERMINATED | 127.0.0.1:16244 |  1      |           0.5 |\n",
      "| inner_lr_tune_6421b_00001 | TERMINATED | 127.0.0.1:16244 |  0.0001 |           0.5 |\n",
      "| inner_lr_tune_6421b_00002 | TERMINATED | 127.0.0.1:16244 |  0.1    |           0.5 |\n",
      "| inner_lr_tune_6421b_00003 | TERMINATED | 127.0.0.1:16244 |  1      |           0.5 |\n",
      "| inner_lr_tune_6421b_00004 | TERMINATED | 127.0.0.1:16244 |  1      |           0.5 |\n",
      "| inner_lr_tune_6421b_00005 | TERMINATED | 127.0.0.1:16244 |  0.0001 |           0.5 |\n",
      "| inner_lr_tune_6421b_00006 | TERMINATED | 127.0.0.1:16244 |  0.1    |           0.5 |\n",
      "| inner_lr_tune_6421b_00007 | TERMINATED | 127.0.0.1:16244 |  0.1    |           0.5 |\n",
      "| inner_lr_tune_6421b_00008 | TERMINATED | 127.0.0.1:16244 |  0.1    |           0.5 |\n",
      "+---------------------------+------------+-----------------+---------+---------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.01, NNZs: 15, Bias: -0.992594, T: 19465726, Avg. loss: 0.724676\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 4.24 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 8\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.01, NNZs: 15, Bias: -0.997153, T: 22246544, Avg. loss: 0.724236\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 4.82 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 9\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.01, NNZs: 15, Bias: -1.001932, T: 25027362, Avg. loss: 0.723913\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 5.46 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m -- Epoch 10\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Norm: 0.01, NNZs: 15, Bias: -0.996797, T: 27808180, Avg. loss: 0.723644\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Total training time: 6.08 seconds.\n",
      "\u001b[2m\u001b[36m(inner_lr_tune pid=16244)\u001b[0m Convergence after 10 epochs took 6.08 seconds\n",
      "Result for inner_lr_tune_6421b_00009:\n",
      "  date: 2022-07-14_21-24-11\n",
      "  done: false\n",
      "  experiment_id: b810ce6045034f21a639a212ecdb9f5f\n",
      "  hostname: Mathias\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  pid: 16244\n",
      "  time_since_restore: 7.5065248012542725\n",
      "  time_this_iter_s: 7.5065248012542725\n",
      "  time_total_s: 7.5065248012542725\n",
      "  timestamp: 1657826651\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 6421b_00009\n",
      "  val_bal_acc: 0.5\n",
      "  warmup_time: 0.00400090217590332\n",
      "  \n",
      "Result for inner_lr_tune_6421b_00009:\n",
      "  date: 2022-07-14_21-24-11\n",
      "  done: true\n",
      "  experiment_id: b810ce6045034f21a639a212ecdb9f5f\n",
      "  experiment_tag: 9_alpha=0.0001,num_epochs=100\n",
      "  hostname: Mathias\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  pid: 16244\n",
      "  time_since_restore: 7.5065248012542725\n",
      "  time_this_iter_s: 7.5065248012542725\n",
      "  time_total_s: 7.5065248012542725\n",
      "  timestamp: 1657826651\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 6421b_00009\n",
      "  val_bal_acc: 0.5\n",
      "  warmup_time: 0.00400090217590332\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-07-14 21:24:11 (running for 00:01:03.62)\n",
      "Memory usage on this node: 9.4/15.9 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 512.000: None | Iter 256.000: None | Iter 128.000: None | Iter 64.000: None | Iter 32.000: None | Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: 0.5\n",
      "Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/5.62 GiB heap, 0.0/2.81 GiB objects\n",
      "Current best trial: 6421b_00000 with val_bal_acc=0.5 and parameters={'alpha': 1}\n",
      "Result logdir: C:\\Users\\Mathiass\\Documents\\Projects\\master-thesis\\notebooks\\logs\\tune\\lr_loops\\20220714212301\n",
      "Number of trials: 10/10 (10 TERMINATED)\n",
      "+---------------------------+------------+-----------------+---------+---------------+\n",
      "| Trial name                | status     | loc             |   alpha |   val_bal_acc |\n",
      "|---------------------------+------------+-----------------+---------+---------------|\n",
      "| inner_lr_tune_6421b_00000 | TERMINATED | 127.0.0.1:16244 |  1      |           0.5 |\n",
      "| inner_lr_tune_6421b_00001 | TERMINATED | 127.0.0.1:16244 |  0.0001 |           0.5 |\n",
      "| inner_lr_tune_6421b_00002 | TERMINATED | 127.0.0.1:16244 |  0.1    |           0.5 |\n",
      "| inner_lr_tune_6421b_00003 | TERMINATED | 127.0.0.1:16244 |  1      |           0.5 |\n",
      "| inner_lr_tune_6421b_00004 | TERMINATED | 127.0.0.1:16244 |  1      |           0.5 |\n",
      "| inner_lr_tune_6421b_00005 | TERMINATED | 127.0.0.1:16244 |  0.0001 |           0.5 |\n",
      "| inner_lr_tune_6421b_00006 | TERMINATED | 127.0.0.1:16244 |  0.1    |           0.5 |\n",
      "| inner_lr_tune_6421b_00007 | TERMINATED | 127.0.0.1:16244 |  0.1    |           0.5 |\n",
      "| inner_lr_tune_6421b_00008 | TERMINATED | 127.0.0.1:16244 |  0.1    |           0.5 |\n",
      "| inner_lr_tune_6421b_00009 | TERMINATED | 127.0.0.1:16244 |  0.0001 |           0.5 |\n",
      "+---------------------------+------------+-----------------+---------+---------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-14 21:24:11,618\tINFO tune.py:747 -- Total run time: 63.87 seconds (63.61 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "analysis = lr_tune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf603e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=)\u001b[0m 2022-07-14 21:24:11,680\tINFO context.py:67 -- Exec'ing worker with command: \"C:\\Users\\Mathiass\\Anaconda3\\envs\\masterthesis\\python.exe\" C:\\Users\\Mathiass\\Anaconda3\\envs\\masterthesis\\lib\\site-packages\\ray\\workers/default_worker.py --node-ip-address=127.0.0.1 --node-manager-port=57337 --object-store-name=tcp://127.0.0.1:58479 --raylet-name=tcp://127.0.0.1:63917 --redis-address=None --storage=None --temp-dir=C:\\Users\\Mathiass\\AppData\\Local\\Temp\\ray --metrics-agent-port=61969 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --gcs-address=127.0.0.1:62460 --redis-password=5241590000000000 --startup-token=8 --runtime-env-hash=137120697\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_bal_acc</th>\n",
       "      <th>time_this_iter_s</th>\n",
       "      <th>done</th>\n",
       "      <th>timesteps_total</th>\n",
       "      <th>episodes_total</th>\n",
       "      <th>training_iteration</th>\n",
       "      <th>trial_id</th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>date</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>...</th>\n",
       "      <th>pid</th>\n",
       "      <th>hostname</th>\n",
       "      <th>node_ip</th>\n",
       "      <th>time_since_restore</th>\n",
       "      <th>timesteps_since_restore</th>\n",
       "      <th>iterations_since_restore</th>\n",
       "      <th>warmup_time</th>\n",
       "      <th>config/alpha</th>\n",
       "      <th>config/num_epochs</th>\n",
       "      <th>logdir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>5.039807</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6421b_00000</td>\n",
       "      <td>b810ce6045034f21a639a212ecdb9f5f</td>\n",
       "      <td>2022-07-14_21-23-18</td>\n",
       "      <td>1657826598</td>\n",
       "      <td>...</td>\n",
       "      <td>16244</td>\n",
       "      <td>Mathias</td>\n",
       "      <td>127.0.0.1</td>\n",
       "      <td>5.039807</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004001</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>100</td>\n",
       "      <td>C:\\Users\\Mathiass\\Documents\\Projects\\master-th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5</td>\n",
       "      <td>7.183361</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6421b_00001</td>\n",
       "      <td>b810ce6045034f21a639a212ecdb9f5f</td>\n",
       "      <td>2022-07-14_21-23-25</td>\n",
       "      <td>1657826605</td>\n",
       "      <td>...</td>\n",
       "      <td>16244</td>\n",
       "      <td>Mathias</td>\n",
       "      <td>127.0.0.1</td>\n",
       "      <td>7.183361</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>100</td>\n",
       "      <td>C:\\Users\\Mathiass\\Documents\\Projects\\master-th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5</td>\n",
       "      <td>5.183725</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6421b_00002</td>\n",
       "      <td>b810ce6045034f21a639a212ecdb9f5f</td>\n",
       "      <td>2022-07-14_21-23-31</td>\n",
       "      <td>1657826611</td>\n",
       "      <td>...</td>\n",
       "      <td>16244</td>\n",
       "      <td>Mathias</td>\n",
       "      <td>127.0.0.1</td>\n",
       "      <td>5.183725</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004001</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>C:\\Users\\Mathiass\\Documents\\Projects\\master-th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.5</td>\n",
       "      <td>5.253567</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6421b_00003</td>\n",
       "      <td>b810ce6045034f21a639a212ecdb9f5f</td>\n",
       "      <td>2022-07-14_21-23-36</td>\n",
       "      <td>1657826616</td>\n",
       "      <td>...</td>\n",
       "      <td>16244</td>\n",
       "      <td>Mathias</td>\n",
       "      <td>127.0.0.1</td>\n",
       "      <td>5.253567</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004001</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>100</td>\n",
       "      <td>C:\\Users\\Mathiass\\Documents\\Projects\\master-th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5</td>\n",
       "      <td>4.930108</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6421b_00004</td>\n",
       "      <td>b810ce6045034f21a639a212ecdb9f5f</td>\n",
       "      <td>2022-07-14_21-23-41</td>\n",
       "      <td>1657826621</td>\n",
       "      <td>...</td>\n",
       "      <td>16244</td>\n",
       "      <td>Mathias</td>\n",
       "      <td>127.0.0.1</td>\n",
       "      <td>4.930108</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004001</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>100</td>\n",
       "      <td>C:\\Users\\Mathiass\\Documents\\Projects\\master-th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.5</td>\n",
       "      <td>7.103131</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6421b_00005</td>\n",
       "      <td>b810ce6045034f21a639a212ecdb9f5f</td>\n",
       "      <td>2022-07-14_21-23-48</td>\n",
       "      <td>1657826628</td>\n",
       "      <td>...</td>\n",
       "      <td>16244</td>\n",
       "      <td>Mathias</td>\n",
       "      <td>127.0.0.1</td>\n",
       "      <td>7.103131</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1000</td>\n",
       "      <td>C:\\Users\\Mathiass\\Documents\\Projects\\master-th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.5</td>\n",
       "      <td>4.928756</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6421b_00006</td>\n",
       "      <td>b810ce6045034f21a639a212ecdb9f5f</td>\n",
       "      <td>2022-07-14_21-23-53</td>\n",
       "      <td>1657826633</td>\n",
       "      <td>...</td>\n",
       "      <td>16244</td>\n",
       "      <td>Mathias</td>\n",
       "      <td>127.0.0.1</td>\n",
       "      <td>4.928756</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004001</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>100</td>\n",
       "      <td>C:\\Users\\Mathiass\\Documents\\Projects\\master-th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.5</td>\n",
       "      <td>5.260484</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6421b_00007</td>\n",
       "      <td>b810ce6045034f21a639a212ecdb9f5f</td>\n",
       "      <td>2022-07-14_21-23-58</td>\n",
       "      <td>1657826638</td>\n",
       "      <td>...</td>\n",
       "      <td>16244</td>\n",
       "      <td>Mathias</td>\n",
       "      <td>127.0.0.1</td>\n",
       "      <td>5.260484</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004001</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>100</td>\n",
       "      <td>C:\\Users\\Mathiass\\Documents\\Projects\\master-th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.5</td>\n",
       "      <td>5.064195</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6421b_00008</td>\n",
       "      <td>b810ce6045034f21a639a212ecdb9f5f</td>\n",
       "      <td>2022-07-14_21-24-03</td>\n",
       "      <td>1657826643</td>\n",
       "      <td>...</td>\n",
       "      <td>16244</td>\n",
       "      <td>Mathias</td>\n",
       "      <td>127.0.0.1</td>\n",
       "      <td>5.064195</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004001</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>100</td>\n",
       "      <td>C:\\Users\\Mathiass\\Documents\\Projects\\master-th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.5</td>\n",
       "      <td>7.506525</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6421b_00009</td>\n",
       "      <td>b810ce6045034f21a639a212ecdb9f5f</td>\n",
       "      <td>2022-07-14_21-24-11</td>\n",
       "      <td>1657826651</td>\n",
       "      <td>...</td>\n",
       "      <td>16244</td>\n",
       "      <td>Mathias</td>\n",
       "      <td>127.0.0.1</td>\n",
       "      <td>7.506525</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>100</td>\n",
       "      <td>C:\\Users\\Mathiass\\Documents\\Projects\\master-th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   val_bal_acc  time_this_iter_s   done  timesteps_total  episodes_total  \\\n",
       "0          0.5          5.039807  False              NaN             NaN   \n",
       "1          0.5          7.183361  False              NaN             NaN   \n",
       "2          0.5          5.183725  False              NaN             NaN   \n",
       "3          0.5          5.253567  False              NaN             NaN   \n",
       "4          0.5          4.930108  False              NaN             NaN   \n",
       "5          0.5          7.103131  False              NaN             NaN   \n",
       "6          0.5          4.928756  False              NaN             NaN   \n",
       "7          0.5          5.260484  False              NaN             NaN   \n",
       "8          0.5          5.064195  False              NaN             NaN   \n",
       "9          0.5          7.506525  False              NaN             NaN   \n",
       "\n",
       "   training_iteration     trial_id                     experiment_id  \\\n",
       "0                   1  6421b_00000  b810ce6045034f21a639a212ecdb9f5f   \n",
       "1                   1  6421b_00001  b810ce6045034f21a639a212ecdb9f5f   \n",
       "2                   1  6421b_00002  b810ce6045034f21a639a212ecdb9f5f   \n",
       "3                   1  6421b_00003  b810ce6045034f21a639a212ecdb9f5f   \n",
       "4                   1  6421b_00004  b810ce6045034f21a639a212ecdb9f5f   \n",
       "5                   1  6421b_00005  b810ce6045034f21a639a212ecdb9f5f   \n",
       "6                   1  6421b_00006  b810ce6045034f21a639a212ecdb9f5f   \n",
       "7                   1  6421b_00007  b810ce6045034f21a639a212ecdb9f5f   \n",
       "8                   1  6421b_00008  b810ce6045034f21a639a212ecdb9f5f   \n",
       "9                   1  6421b_00009  b810ce6045034f21a639a212ecdb9f5f   \n",
       "\n",
       "                  date   timestamp  ...    pid  hostname    node_ip  \\\n",
       "0  2022-07-14_21-23-18  1657826598  ...  16244   Mathias  127.0.0.1   \n",
       "1  2022-07-14_21-23-25  1657826605  ...  16244   Mathias  127.0.0.1   \n",
       "2  2022-07-14_21-23-31  1657826611  ...  16244   Mathias  127.0.0.1   \n",
       "3  2022-07-14_21-23-36  1657826616  ...  16244   Mathias  127.0.0.1   \n",
       "4  2022-07-14_21-23-41  1657826621  ...  16244   Mathias  127.0.0.1   \n",
       "5  2022-07-14_21-23-48  1657826628  ...  16244   Mathias  127.0.0.1   \n",
       "6  2022-07-14_21-23-53  1657826633  ...  16244   Mathias  127.0.0.1   \n",
       "7  2022-07-14_21-23-58  1657826638  ...  16244   Mathias  127.0.0.1   \n",
       "8  2022-07-14_21-24-03  1657826643  ...  16244   Mathias  127.0.0.1   \n",
       "9  2022-07-14_21-24-11  1657826651  ...  16244   Mathias  127.0.0.1   \n",
       "\n",
       "  time_since_restore  timesteps_since_restore  iterations_since_restore  \\\n",
       "0           5.039807                        0                         1   \n",
       "1           7.183361                        0                         1   \n",
       "2           5.183725                        0                         1   \n",
       "3           5.253567                        0                         1   \n",
       "4           4.930108                        0                         1   \n",
       "5           7.103131                        0                         1   \n",
       "6           4.928756                        0                         1   \n",
       "7           5.260484                        0                         1   \n",
       "8           5.064195                        0                         1   \n",
       "9           7.506525                        0                         1   \n",
       "\n",
       "   warmup_time  config/alpha  config/num_epochs  \\\n",
       "0     0.004001        1.0000                100   \n",
       "1     0.004001        0.0001                100   \n",
       "2     0.004001        0.1000               1000   \n",
       "3     0.004001        1.0000                100   \n",
       "4     0.004001        1.0000                100   \n",
       "5     0.004001        0.0001               1000   \n",
       "6     0.004001        0.1000                100   \n",
       "7     0.004001        0.1000                100   \n",
       "8     0.004001        0.1000                100   \n",
       "9     0.004001        0.0001                100   \n",
       "\n",
       "                                              logdir  \n",
       "0  C:\\Users\\Mathiass\\Documents\\Projects\\master-th...  \n",
       "1  C:\\Users\\Mathiass\\Documents\\Projects\\master-th...  \n",
       "2  C:\\Users\\Mathiass\\Documents\\Projects\\master-th...  \n",
       "3  C:\\Users\\Mathiass\\Documents\\Projects\\master-th...  \n",
       "4  C:\\Users\\Mathiass\\Documents\\Projects\\master-th...  \n",
       "5  C:\\Users\\Mathiass\\Documents\\Projects\\master-th...  \n",
       "6  C:\\Users\\Mathiass\\Documents\\Projects\\master-th...  \n",
       "7  C:\\Users\\Mathiass\\Documents\\Projects\\master-th...  \n",
       "8  C:\\Users\\Mathiass\\Documents\\Projects\\master-th...  \n",
       "9  C:\\Users\\Mathiass\\Documents\\Projects\\master-th...  \n",
       "\n",
       "[10 rows x 21 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis.dataframe(metric=\"val_bal_acc\", mode=\"max\").sort_values(\"val_bal_acc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9742c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inner_lr_tune_6421b_00000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis.get_best_trial(\"val_bal_acc\", \"max\", \"last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8135a1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2022-07-14 16:54:58,923\tWARNING worker.py:1404 -- Warning: The actor ImplicitFunc is very large (74 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5dc231",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e4d97d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1561b72c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad63cd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caeac47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347192a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de3eb82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054bf8bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d796fe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c2b1bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0e5bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3f277e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517c8d2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
