{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6207475a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from pytorch_lightning import seed_everything\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torch.nn import functional as F\n",
    "import torchmetrics\n",
    "\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score #equal to torchmetrics.accuracy(average=\"macro\")\n",
    "\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback, TuneReportCheckpointCallback\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune import CLIReporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d341a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca88da6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in tune: will seed hyperparam search space\n",
    "seed_everything(42, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73e517c",
   "metadata": {},
   "outputs": [],
   "source": [
    "r\"C:\\Users\\Mathiass\\OneDrive - Universit채t Z체rich UZH\\Documents\\mt_literature\\data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b7c858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path\n",
    "path_data = pathlib.Path(r\"C:\\Users\\Mathiass\\OneDrive - Universit채t Z체rich UZH\\Documents\\mt_literature\\data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a240229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineer(data):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    data: pandas.DataFrame that must have specific columns.\n",
    "\n",
    "    \"\"\"\n",
    "    # Bid-Ask spread: (Ask - Bid) / Ask\n",
    "    data[\"best_bid\"] = (data[\"best_offer\"] - data[\"best_bid\"]) / (data[\"best_offer\"])\n",
    "    data = data.rename(columns={\"best_bid\": \"ba_spread_option\"}).drop([\"best_offer\"], axis=1)\n",
    "\n",
    "    # Gamma: multiply by spotprice and divide by 100\n",
    "    data[\"gamma\"] = data[\"gamma\"] * data[\"spotprice\"] / 100 #following Bali et al. (2021)\n",
    "\n",
    "    # Theta: scale by spotprice\n",
    "    data[\"theta\"] = data[\"theta\"] / data[\"spotprice\"] #following Bali et al. (2021)\n",
    "\n",
    "    # Vega: scale by spotprice\n",
    "    data[\"vega\"] = data[\"vega\"] / data[\"spotprice\"] #following Bali et al. (2021)\n",
    "\n",
    "    # Time to Maturity: cale by number of days in year: 365\n",
    "    data[\"days_to_exp\"] = data[\"days_to_exp\"] / 365\n",
    "\n",
    "    # Moneyness: Strike / Spot (K / S)\n",
    "    data[\"strike_price\"] = data[\"strike_price\"] / data[\"spotprice\"] # K / S\n",
    "    data = data.rename(columns={\"strike_price\": \"moneyness\"})\n",
    "\n",
    "    # Forward Price ratio: Forward / Spot\n",
    "    data[\"forwardprice\"] = data[\"forwardprice\"] / data[\"spotprice\"]\n",
    "\n",
    "    # Drop redundant/ unimportant columns\n",
    "    data = data.drop([\"cfadj\", \"days_no_trading\", \"spotprice\", \"adj_spot\"], axis=1)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc52b89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiclass y label function\n",
    "def binary_categorize(y):\n",
    "    if y > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d36bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiclass y label function\n",
    "def multi_categorize(y):\n",
    "    if y > 0.05:\n",
    "        return 2\n",
    "    elif y < -0.05:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98076a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataModule(pl.LightningDataModule):\n",
    "    def __init__(self,\n",
    "                 path,\n",
    "                 dataset: str,\n",
    "#                  batch_size: int, \n",
    "                 start_val: str, \n",
    "                 start_test: str,\n",
    "                 label_fn: str,\n",
    "                 config: dict,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=[\"path\"])\n",
    "        \n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "        \n",
    "        # read data from disk\n",
    "        if dataset == \"small\":\n",
    "            self.data = pd.read_parquet(path/\"final_df_filledmean_small.parquet\")\n",
    "        elif dataset == \"big\":\n",
    "            self.data = pd.read_parquet(path/\"final_df_filledmean.parquet\")\n",
    "        else:\n",
    "            raise ValueError(\"Specify dataset as either 'small' or 'big'\")\n",
    "            \n",
    "        # feature engineer data\n",
    "        self.data = feature_engineer(self.data)\n",
    "        \n",
    "        # create y\n",
    "        self.y = self.data[\"option_ret\"]\n",
    "        # make classification problem\n",
    "        if label_fn == \"binary\":\n",
    "            self.y = self.y.apply(binary_categorize)\n",
    "        elif label_fn == \"multi\":\n",
    "            self.y = self.y.apply(multi_categorize)\n",
    "        else:\n",
    "            raise ValueError(\"Specify label_fn as either 'binary' or 'multi'\")\n",
    "        # create X\n",
    "        self.X = self.data.drop([\"option_ret\"], axis=1)\n",
    "        \n",
    "        # save dates and drop\n",
    "        self.dates = self.X[\"date\"]\n",
    "        self.X = self.X.drop([\"date\"], axis=1)\n",
    "        \n",
    "        # to torch Tensor\n",
    "        self.X = torch.from_numpy(self.X.values).float() #-> will be standardized in setup, so do it there.\n",
    "        self.y = torch.from_numpy(self.y.values)\n",
    "        \n",
    "    def setup(self, stage: str = None):\n",
    "        # train\n",
    "        self.X_train = self.X[self.dates < self.hparams.start_val]\n",
    "        self.y_train = self.y[:len(self.X_train)]\n",
    "        \n",
    "        #val\n",
    "        mask = (self.dates >= self.hparams.start_val) & (self.dates < self.hparams.start_test)\n",
    "        self.X_val = self.X[mask]\n",
    "        self.y_val = self.y[len(self.X_train):len(self.X_train)+len(self.X_val)]\n",
    "        \n",
    "        # test\n",
    "        self.X_test = self.X[self.dates >= self.hparams.start_test]\n",
    "        self.y_test = self.y[-len(self.X_test):]\n",
    "        \n",
    "        assert (np.sum(len(self.X_train)+len(self.X_val)+len(self.X_test)) == len(self.data)), \"sum of samples of splits\\\n",
    "        is not equal length of dataset\"\n",
    "        \n",
    "        #standardize X_train\n",
    "        mean = torch.mean(self.X_train, axis=0)\n",
    "        std = torch.std(self.X_train, axis=0)\n",
    "        \n",
    "        # Standardize X_train, X_val and X_test with mean/std from X_train\n",
    "        self.X_train = (self.X_train - mean) / std\n",
    "        self.X_val = (self.X_val - mean) / std\n",
    "        self.X_test = (self.X_test - mean) / std\n",
    "\n",
    "        # Save variables to pass to model class\n",
    "        # input dim\n",
    "        self.input_dim = self.X_train.shape[1]\n",
    "        # number of classes\n",
    "        self.num_classes = len(self.y_train.unique())\n",
    "        # class weights\n",
    "        self.class_weights = len(self.y_train) / self.y_train.unique(return_counts=True)[1]\n",
    "\n",
    "        print(\"class_weights:\", self.class_weights)\n",
    "        print(\"device of class_weights:\", self.class_weights.device)\n",
    "        print(\"---\")\n",
    "        print(f\"# of input data: {len(self.data)} with shape: {self.data.shape}\")\n",
    "        print(f\"# of training samples: {len(self.y_train)} with X_train of shape: {self.X_train.shape}\")\n",
    "        print(f\"# of validation samples: {len(self.y_val)} with X_val of shape: {self.X_val.shape}\")\n",
    "        print(f\"# of test samples: {len(self.y_test)} with X_test of shape: {self.X_test.shape}\")\n",
    "        print(f\"train start date: \", self.dates[self.dates < self.hparams.start_val].iloc[0].strftime(\"%Y-%m-%d\"), \n",
    "              \", train end date: \", self.dates[self.dates < self.hparams.start_val].iloc[-1].strftime(\"%Y-%m-%d\"))\n",
    "        print(f\"val start date: \", self.dates[mask].iloc[0].strftime(\"%Y-%m-%d\"), \n",
    "              \", val end date: \", self.dates[mask].iloc[-1].strftime(\"%Y-%m-%d\"))\n",
    "        print(f\"test start date: \", self.dates[self.dates >= self.hparams.start_test].iloc[0].strftime(\"%Y-%m-%d\"), \n",
    "              \", test end date: \", self.dates[self.dates >= self.hparams.start_test].iloc[-1].strftime(\"%Y-%m-%d\"))\n",
    "        print(\"---\")\n",
    "\n",
    "    def example(self):\n",
    "        \"\"\"Returns a random training example.\"\"\"        \n",
    "        idx = np.random.randint(0, len(self.X_train))\n",
    "        x, y = self.X_train[idx], self.y_train[idx]\n",
    "        return (x, y)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        dataset = TensorDataset(self.X_train, self.y_train)\n",
    "        return DataLoader(dataset, batch_size=self.batch_size,\n",
    "                         num_workers=4,\n",
    "                         pin_memory=True,\n",
    "                         )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        dataset = TensorDataset(self.X_val, self.y_val)\n",
    "        return DataLoader(dataset, batch_size=self.batch_size,\n",
    "                         num_workers=4,\n",
    "                         pin_memory=True,\n",
    "                         )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        dataset = TensorDataset(self.X_test, self.y_test)\n",
    "        return DataLoader(dataset, batch_size=self.batch_size,\n",
    "                         num_workers=4,\n",
    "                         pin_memory=True,\n",
    "                         )\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):\n",
    "        parser = parent_parser.add_argument_group(\"DataModule\")\n",
    "        parser.add_argument(\"--dataset\", type=str, default=\"small\")\n",
    "        # parser.add_argument(\"--path\", type=str, help=\"path to folder that contains the data\")\n",
    "        parser.add_argument(\"--batch_size\", type=int, default=512)\n",
    "        parser.add_argument(\"--start_val\", type=str, default=\"2014\")\n",
    "        parser.add_argument(\"--start_test\", type=str, default=\"2016\")\n",
    "        parser.add_argument(\"--label_fn\", type=str, default=\"binary\")\n",
    "\n",
    "        return parent_parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c562f306",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                # dm,\n",
    "                input_dim: int,\n",
    "                num_classes: int,\n",
    "                class_weights: torch.Tensor,\n",
    "                no_class_weights: bool,\n",
    "#                 hidden_dim: int,\n",
    "#                 learning_rate: float,\n",
    "                 config: dict,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        # Init variables are saved, so that model can be reloaded cleanly if necessary\n",
    "#         self.save_hyperparameters(ignore=[\"class_weights\"])\n",
    "        self.save_hyperparameters()\n",
    "        #ignore \"dm\" is crucial if dm is passed\n",
    "        \n",
    "        if config is not None:\n",
    "            self.hidden_dim = config[\"hidden_dim\"]\n",
    "            self.learning_rate = config[\"lr\"]\n",
    "        else:\n",
    "            self.hidden_dim = hidden_dim\n",
    "            self.learning_rate = learning_rate\n",
    "        \n",
    "        middle_layer = []\n",
    "        for i in range(3):\n",
    "            middle_layer.append(nn.Linear(self.hidden_dim, self.hidden_dim))\n",
    "#             middle_layer.append(nn.BatchNorm1d(self.hidden_dim))\n",
    "            middle_layer.append(nn.ReLU(inplace=True))\n",
    "            middle_layer.append(nn.Dropout())\n",
    "        \n",
    "        \n",
    "        #model\n",
    "        self.first = nn.Sequential(nn.Linear(input_dim, self.hidden_dim), nn.ReLU())\n",
    "#         self.middle = nn.Sequential(*[nn.Sequential(nn.Linear(self.hidden_dim, self.hidden_dim), nn.ReLU()) for i in range(3)])\n",
    "        self.middle = nn.Sequential(*middle_layer)\n",
    "        self.last = nn.Linear(self.hidden_dim, num_classes)\n",
    "        \n",
    "        #sample weights\n",
    "        if not self.hparams.no_class_weights:\n",
    "            self.class_weights = class_weights\n",
    "            self.class_weights = self.class_weights.cuda() # Move to cuda, otherwise mismatch of devices # in train/val\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "#         print(\"---\")\n",
    "#         print(\"class_weights:\", self.class_weights)\n",
    "#         print(\"device of class_weights:\", self.class_weights.device)\n",
    "#         print(\"device of class:\", self.device)\n",
    "#         print(\"---\")\n",
    "\n",
    "        #metrics\n",
    "        self.train_acc = torchmetrics.Accuracy()\n",
    "        self.train_bal_acc = torchmetrics.Accuracy(\n",
    "        num_classes=num_classes, average=\"macro\") # should be equal to sklearn bal. acc.\n",
    "\n",
    "        self.val_acc = torchmetrics.Accuracy()\n",
    "        self.val_bal_acc= torchmetrics.Accuracy(\n",
    "            num_classes=num_classes, average=\"macro\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first(x)\n",
    "        x = self.middle(x)\n",
    "        x = self.last(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x) #logits\n",
    "        \n",
    "        loss = F.cross_entropy(y_hat, y, weight=self.class_weights)\n",
    "        self.log(\"loss/loss\", loss, on_step=True, on_epoch=False, prog_bar=True)\n",
    "        \n",
    "        self.train_acc(y_hat, y)\n",
    "        self.log(\"acc/train\", self.train_acc, on_step=False, on_epoch=True)\n",
    "        \n",
    "        self.train_bal_acc(y_hat, y)\n",
    "        self.log(\"bal_acc/train\", self.train_bal_acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x) #logits\n",
    "        \n",
    "#         self.log(\"hp_metric\", torch.mean(y_hat.argmax(dim=-1).float()).item(), prog_bar=True) # average prediction class\n",
    "        self.log(\"mean_pred\", torch.mean(y_hat.argmax(dim=-1).float()).item(), prog_bar=True)\n",
    "        \n",
    "        loss = F.cross_entropy(y_hat, y, weight=self.class_weights)\n",
    "        self.log(\"loss/val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        self.val_acc(y_hat, y)\n",
    "        self.log(\"acc/val\", self.val_acc, on_step=False, on_epoch=True)\n",
    "        \n",
    "        self.val_bal_acc(y_hat, y)\n",
    "        self.log(\"bal_acc/val\", self.val_bal_acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return {\"val_loss\": loss}\n",
    "    \n",
    "    def on_train_start(self):\n",
    "        self.st_total = time.time()\n",
    "\n",
    "    def on_train_epoch_start(self):\n",
    "        self.st = time.time()\n",
    "        self.steps = self.global_step\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        elapsed = time.time() - self.st\n",
    "        steps_done = self.global_step - self.steps\n",
    "        self.log(\"time/step\", elapsed / steps_done)\n",
    "\n",
    "    def on_train_end(self):\n",
    "        elapsed = time.time() - self.st_total\n",
    "        print(f\"Total Training Time: {time.strftime('%H:%M:%S', time.gmtime(elapsed))}\")\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y, weight=self.class_weights)\n",
    "\n",
    "        self.log(\"loss/test_loss\", loss, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):\n",
    "        parser = parent_parser.add_argument_group(\"FFN\")\n",
    "        parser.add_argument(\"--no_class_weights\", action='store_true')\n",
    "        parser.add_argument(\"--hidden_dim\", type=int, default=100)\n",
    "        parser.add_argument(\"-lr\", \"--learning_rate\", type=float, default=1e-2)\n",
    "\n",
    "        return parent_parser\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16bf8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FFN(\n",
    "input_dim=15,\n",
    "num_classes=2,\n",
    "class_weights=None,\n",
    "no_class_weights=True,\n",
    "#     hidden_dim=HIDDEN_DIM,\n",
    "#     learning_rate=LEARNING_RATE,\n",
    "config={\"hidden_dim\": 50, \"lr\": 1e-2},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc41658f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8151d245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune_callback = TuneReportCallback(\n",
    "#     {\n",
    "#         \"val_loss\": \"loss/val_loss\",\n",
    "#         \"val_bal_acc\": \"bal_acc/val\",\n",
    "#         \"mean_pred\": \"mean_pred\"\n",
    "#     },\n",
    "#     on=\"validation_end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e134539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_callback = ModelCheckpoint(\n",
    "#         monitor=\"loss/val_loss\",\n",
    "#         save_top_k=1,\n",
    "#         mode=\"min\",\n",
    "#         filename='epoch={epoch}-val_loss={loss/val_loss:.3f}-val_bacc={bal_acc/val:.4f}',\n",
    "#         auto_insert_metric_name=False,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37e86e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_callback = EarlyStopping(\n",
    "        monitor=\"loss/val_loss\", \n",
    "        mode=\"min\", \n",
    "        patience=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3eeccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_callback = TuneReportCheckpointCallback(\n",
    "    metrics={\n",
    "        \"loss\": \"loss/loss\",\n",
    "        \"mean_pred\": \"mean_pred\",\n",
    "        \"val_loss\": \"loss/val_loss\",\n",
    "        \"val_bal_acc\": \"bal_acc/val\"\n",
    "    },\n",
    "    filename=\"checkpoint\",\n",
    "    on=\"validation_end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98283591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mnist_tune(config, max_epochs, num_gpus, checkpoint_dir=None):\n",
    "#     data_dir = os.path.expanduser(data_dir)\n",
    "    \n",
    "    # will seed trainer (init of weights in NN?)\n",
    "    seed_everything(42, workers=True)\n",
    "    \n",
    "    print(\"------\")\n",
    "    print(np.random.uniform(0, 100, size=1).item())\n",
    "    print(\"------\")\n",
    "    \n",
    "    dm = MyDataModule(\n",
    "    path=path_data, \n",
    "    dataset=\"small\",\n",
    "#     batch_size=BATCH_SIZE, \n",
    "    start_val=\"1997\", \n",
    "    start_test=\"1998\",\n",
    "    label_fn=\"binary\",\n",
    "    config=config,\n",
    "    )\n",
    "    \n",
    "    dm.setup()\n",
    "    \n",
    "    model = FFN(\n",
    "    input_dim=dm.input_dim,\n",
    "    num_classes=dm.num_classes,\n",
    "    class_weights=dm.class_weights,\n",
    "    no_class_weights=False,\n",
    "#     hidden_dim=HIDDEN_DIM,\n",
    "#     learning_rate=LEARNING_RATE,\n",
    "    config=config,\n",
    "    )\n",
    "    \n",
    "    print(model)\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        deterministic=True,\n",
    "        max_epochs=max_epochs,\n",
    "        gpus=num_gpus,\n",
    "        logger=pl.loggers.TensorBoardLogger(\n",
    "        save_dir=tune.get_trial_dir(), name=\"\", version=\".\"),\n",
    "        enable_progress_bar=True,\n",
    "        callbacks=[\n",
    "#                    checkpoint_callback, \n",
    "                   early_stop_callback,\n",
    "                   tune_callback, \n",
    "                  ],\n",
    "        enable_checkpointing=False,\n",
    "    )\n",
    "    \n",
    "    trainer.fit(model, datamodule=dm)\n",
    "    \n",
    "#     print(checkpoint_callback.best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a9a471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_mnist_asha(num_samples=2, max_epochs=5, gpus_per_trial=1,):\n",
    "    config = {\n",
    "        \"hidden_dim\": tune.choice([50, 100]),\n",
    "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"batch_size\": tune.choice([256]),\n",
    "#         \"hidden_dim\": tune.choice([32]),\n",
    "#         \"lr\": tune.choice([1e-2]),\n",
    "#         \"batch_size\": tune.choice([512]),\n",
    "    }\n",
    "\n",
    "    scheduler = ASHAScheduler(\n",
    "        max_t=max_epochs,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "\n",
    "    reporter = CLIReporter(\n",
    "        parameter_columns=[\"hidden_dim\", \"lr\", \"batch_size\"],\n",
    "        metric_columns=[\"val_loss\", \"val_bal_acc\", \"mean_pred\", \"training_iteration\"])\n",
    "\n",
    "    train_fn_with_parameters = tune.with_parameters(train_mnist_tune,\n",
    "                                                    max_epochs=max_epochs,\n",
    "                                                    num_gpus=gpus_per_trial,\n",
    "#                                                     data_dir=data_dir,\n",
    "                                                   )\n",
    "    resources_per_trial = {\"cpu\": 1, \"gpu\": gpus_per_trial}\n",
    "    \n",
    "    analysis = tune.run(train_fn_with_parameters,\n",
    "        local_dir=\"./logs\",\n",
    "        resources_per_trial=resources_per_trial,\n",
    "        metric=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        config=config,\n",
    "        num_samples=num_samples,\n",
    "        scheduler=scheduler,\n",
    "        progress_reporter=reporter,\n",
    "        name=\"tune_mnist_asha\",\n",
    "        keep_checkpoints_num=1, # only keep best checkpoint\n",
    "        checkpoint_score_attr=\"min-val_loss\",\n",
    "        )\n",
    "\n",
    "    print(\"Best hyperparameters found were: \", analysis.best_config)\n",
    "    \n",
    "    best_trial = analysis.get_best_trial(\"val_loss\", \"min\", \"last\")\n",
    "    print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print(\"Best trial >>last<< validation loss: {}\".format(\n",
    "        best_trial.last_result[\"val_loss\"]))\n",
    "    print(\"Best trial >>last epoch<< validation accuracy: {}\".format(\n",
    "        best_trial.last_result[\"val_bal_acc\"]))\n",
    "    \n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817fd608",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = tune_mnist_asha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03ce86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = result.get_best_trial(\"val_loss\", \"min\", \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6270e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee66289",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.get_best_config(\"val_loss\", \"min\", \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1757c00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6349f796",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.dataframe(metric=\"val_loss\", mode=\"min\").sort_values(\"val_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24034319",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.best_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7370d9eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed89ad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd0ddf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4217be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290e7c8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b8d229",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = result.dataframe(metric=\"val_loss\", mode=\"min\").sort_values(\"val_loss\").iloc[0, :].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3474be",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c5f579",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1588784",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"2008\"] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1e11d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd108530",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca04bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.update({\"2009\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01bd7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b477de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b339965",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_results = result.results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f592143",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4cdb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.get_best_trial(\"val_loss\", \"min\", scope=\"all\").checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9492237b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.best_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886894cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.best_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05d4302",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result.get_best_trial().checkpoint.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8febd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.get_best_trial(\"val_loss\", \"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4162032d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.get_best_trial(\"val_loss\", \"min\", scope=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad31f9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.get_best_trial(\"val_loss\", \"min\", scope=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0c220f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21cee67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0,lr=0.0245_2022-07-09_16-11-58\\checkpoint_epoch=1-step=302\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfb6488",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.get_best_checkpoint(result.get_best_trial(\"val_loss\", \"min\", scope=\"all\")).get_internal_representation()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f83c7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.get_best_checkpoint(result.get_best_trial(\"val_loss\", \"min\", scope=\"last\")).get_internal_representation()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9fe697",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.get_best_checkpoint(result.get_best_trial(), metric=\"val_loss\", mode=\"min\").get_internal_representation()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d88f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.best_checkpoint.get_internal_representation()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d6f549",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.default_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf581bc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bbd155",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "path = result.get_best_checkpoint(result.get_best_trial(), metric=\"val_loss\", mode=\"min\").get_internal_representation()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32ead85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FFN.load_from_checkpoint(Path(path)/\"checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27efac32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result.get_best_trial(\"val_loss\", \"min\", \"last\").checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e978cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = FFN()\n",
    "# # trainer = Trainer()\n",
    "# from pathlib import Path\n",
    "# path = Path(r\"C:\\Users\\Mathiass\\Documents\\Projects\\master-thesis\\notebooks\\logs\\tune_mnist_asha\\train_mnist_tune_5b9ea_00000_0_batch_size=256,hidden_dim=50,lr=0.0245_2022-07-04_21-20-26\\.\\checkpoints\\epoch=0-val_loss=0.764-val_bacc=0.5235.ckpt\")\n",
    "\n",
    "# # automatically restores model, epoch, step, LR schedulers, apex, etc...\n",
    "# # trainer.fit(model, ckpt_path=path)\n",
    "# model = FFN.load_from_checkpoint(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b66c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_json(r\"C:\\Users\\Mathiass\\Documents\\Projects\\master-thesis\\notebooks\\logs\\tune_mnist_asha\\train_mnist_tune_21877_00000_0_batch_size=256,hidden_dim=50,lr=0.0245_2022-07-05_09-36-06\\result.json\", \n",
    "# lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecae8274",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597279cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe846814",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = MyDataModule(\n",
    "path=path_data, \n",
    "dataset=\"small\",\n",
    "#     batch_size=BATCH_SIZE, \n",
    "start_val=\"1998\", \n",
    "start_test=\"1999\",\n",
    "label_fn=\"binary\",\n",
    "config=result.best_config,\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "deterministic=True,\n",
    "max_epochs=1,\n",
    "gpus=1,\n",
    "# logger=pl.loggers.TensorBoardLogger(\n",
    "# save_dir=tune.get_trial_dir(), name=\"\", version=\".\"), # SPECIFY SAVE_DIR FOR VALIDATION LOGGING -> default: lightning logs\n",
    "enable_progress_bar=True,\n",
    "# callbacks=[\n",
    "#     tune_callback, \n",
    "#     early_stop_callback\n",
    "# ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadfe258",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "val_result = trainer.validate(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16152f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deafe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best hyperparameters found were:  {'hidden_dim': 50, 'lr': 0.024526126311336768, 'batch_size': 256}\n",
    "# Best trial config: {'hidden_dim': 50, 'lr': 0.024526126311336768, 'batch_size': 256}\n",
    "# Best trial >>last<< validation loss: 0.6991798281669617\n",
    "# Best trial >>last epoch<< validation accuracy: 0.5307202339172363"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48faa8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.get_best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084600b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8d0307",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback.best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159662dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trainer = pl.Trainer(\n",
    "#     deterministic=True,\n",
    "#     max_epochs=MAX_EPOCHS,\n",
    "#     gpus=1,\n",
    "#     logger=logger, #=logger or False\n",
    "#     check_val_every_n_epoch=1,\n",
    "#     callbacks=[early_stop_callback, checkpoint_callback], #early stop depends earliest after (patience*check_val_every_n_epoch)\n",
    "#     # enable_checkpointing = False,\n",
    "#     num_sanity_val_steps=2,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002d0fe8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# s_time = time.time()\n",
    "# trainer.fit(model, datamodule=dm)\n",
    "# e_time = time.time()\n",
    "# print(f\"Time to fit: {divmod(e_time - s_time, 60)[0]:2.0f}:{divmod(e_time - s_time, 60)[1]:2.0f}\\\n",
    "#  min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3103b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "init_train = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219c5960",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = np.arange((1996+init_train+idx),(2021)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0139fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "years[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b081dd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f840606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abc():\n",
    "    return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68038d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "globals()[\"abc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c98097e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [\"a\", \"b\", \"c\"]\n",
    "for i, k in enumerate(a):\n",
    "    print(i, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce594cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = iter([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908bb388",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {\"test2009\": 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79572e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb4992d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.update(b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "137ad5de30c222602b906d427f317b23725154a9d2ac1dd9f95e9d3b5697fcc3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
