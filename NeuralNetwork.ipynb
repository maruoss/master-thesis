{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6207475a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from pytorch_lightning import seed_everything\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torch.nn import functional as F\n",
    "import torchmetrics\n",
    "\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca88da6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b7c858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path\n",
    "path_data = pathlib.Path(r\"C:\\Users\\Mathiass\\OneDrive - Universität Zürich UZH\\Documents\\mt_literature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a240229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineer(data):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    data: pandas.DataFrame that must have specific columns.\n",
    "\n",
    "    \"\"\"\n",
    "    # Bid-Ask spread: (Ask - Bid) / Ask\n",
    "    data[\"best_bid\"] = (data[\"best_offer\"] - data[\"best_bid\"]) / (data[\"best_offer\"])\n",
    "    data = data.rename(columns={\"best_bid\": \"ba_spread_option\"}).drop([\"best_offer\"], axis=1)\n",
    "\n",
    "    # Gamma: multiply by spotprice and divide by 100\n",
    "    data[\"gamma\"] = data[\"gamma\"] * data[\"spotprice\"] / 100 #following Bali et al. (2021)\n",
    "\n",
    "    # Theta: scale by spotprice\n",
    "    data[\"theta\"] = data[\"theta\"] / data[\"spotprice\"] #following Bali et al. (2021)\n",
    "\n",
    "    # Vega: scale by spotprice\n",
    "    data[\"vega\"] = data[\"vega\"] / data[\"spotprice\"] #following Bali et al. (2021)\n",
    "\n",
    "    # Time to Maturity: cale by number of days in year: 365\n",
    "    data[\"days_to_exp\"] = data[\"days_to_exp\"] / 365\n",
    "\n",
    "    # Moneyness: Strike / Spot (K / S)\n",
    "    data[\"strike_price\"] = data[\"strike_price\"] / data[\"spotprice\"] # K / S\n",
    "    data = data.rename(columns={\"strike_price\": \"moneyness\"})\n",
    "\n",
    "    # Forward Price ratio: Forward / Spot\n",
    "    data[\"forwardprice\"] = data[\"forwardprice\"] / data[\"spotprice\"]\n",
    "\n",
    "    # Drop redundant/ unimportant columns\n",
    "    data = data.drop([\"cfadj\", \"days_no_trading\", \"spotprice\", \"adj_spot\"], axis=1)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc52b89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiclass y label function\n",
    "def binary_categorize(y):\n",
    "    if y > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d36bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiclass y label function\n",
    "def multi_categorize(y):\n",
    "    if y > 0.05:\n",
    "        return 2\n",
    "    elif y < -0.05:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e3abf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Tensor([1, 2, 3]).type(torch.DoubleTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fd35a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pl.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98076a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataModule(pl.LightningDataModule):\n",
    "    def __init__(self,\n",
    "                 dataset: str,\n",
    "                 path,\n",
    "                 batch_size: int, \n",
    "                 start_val: str, \n",
    "                 start_test: str,\n",
    "                 label_fn: str,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # read data from disk\n",
    "        if dataset == \"small\":\n",
    "            self.data = pd.read_parquet(path/\"final_df_filledmean_small.parquet\")\n",
    "        elif dataset == \"big\":\n",
    "            self.data = pd.read_parquet(path/\"final_df_filledmean.parquet\")\n",
    "        else:\n",
    "            raise ValueError(\"Specify dataset as either 'small' or 'big'\")\n",
    "            \n",
    "        # feature engineer data\n",
    "        self.data = feature_engineer(self.data)\n",
    "        \n",
    "        # create y\n",
    "        self.y = self.data[\"option_ret\"]\n",
    "        # make classification problem\n",
    "        if label_fn == \"binary\":\n",
    "            self.y = self.y.apply(binary_categorize)\n",
    "        elif label_fn == \"multi\":\n",
    "            self.y = self.y.apply(multi_categorize)\n",
    "        else:\n",
    "            raise ValueError(\"Specify label_fn as either 'binary' or 'multi'\")\n",
    "        # create X\n",
    "        self.X = self.data.drop([\"option_ret\"], axis=1)\n",
    "        \n",
    "        # save dates and drop\n",
    "        self.dates = self.X[\"date\"]\n",
    "        self.X = self.X.drop([\"date\"], axis=1)\n",
    "        \n",
    "        # to torch Tensor\n",
    "        self.X = torch.from_numpy(self.X.values).float() #-> will be standardized in setup, so do it there.\n",
    "        self.y = torch.from_numpy(self.y.values)\n",
    "        \n",
    "    def setup(self, stage: str = None):\n",
    "        # train\n",
    "        self.X_train = self.X[self.dates < self.hparams.start_val]\n",
    "        self.y_train = self.y[:len(self.X_train)]\n",
    "        \n",
    "        #val\n",
    "        mask = (self.dates >= self.hparams.start_val) & (self.dates < self.hparams.start_test)\n",
    "        self.X_val = self.X[mask]\n",
    "        self.y_val = self.y[len(self.X_train):len(self.X_train)+len(self.X_val)]\n",
    "        \n",
    "        # test\n",
    "        self.X_test = self.X[self.dates >= self.hparams.start_test]\n",
    "        self.y_test = self.y[-len(self.X_test):]\n",
    "        \n",
    "        assert (np.sum(len(self.X_train)+len(self.X_val)+len(self.X_test)) == len(self.data)), \"sum of samples of splits\\\n",
    "        is not equal length of dataset\"\n",
    "        \n",
    "        #standardize X_train\n",
    "        mean = torch.mean(self.X_train, axis=0)\n",
    "        std = torch.std(self.X_train, axis=0)\n",
    "        \n",
    "        # Standardize X_train, X_val and X_test with mean/std from X_train\n",
    "        self.X_train = (self.X_train - mean) / std\n",
    "        self.X_val = (self.X_val - mean) / std\n",
    "        self.X_test = (self.X_test - mean) / std\n",
    "        \n",
    "        \n",
    "        print(f\"# of input data: {len(self.data)} with shape: {self.data.shape}\")\n",
    "        print(f\"# of training samples: {len(self.y_train)} with X_train of shape: {self.X_train.shape}\")\n",
    "        print(f\"# of validation samples: {len(self.y_val)} with X_val of shape: {self.X_val.shape}\")\n",
    "        print(f\"# of test samples: {len(self.y_test)} with X_test of shape: {self.X_test.shape}\")\n",
    "        print(f\"train start date: \", self.dates[self.dates < self.hparams.start_val].iloc[0].strftime(\"%Y-%m-%d\"), \n",
    "              \", train end date: \", self.dates[self.dates < self.hparams.start_val].iloc[-1].strftime(\"%Y-%m-%d\"))\n",
    "        print(f\"val start date: \", self.dates[mask].iloc[0].strftime(\"%Y-%m-%d\"), \n",
    "              \", val end date: \", self.dates[mask].iloc[-1].strftime(\"%Y-%m-%d\"))\n",
    "        print(f\"test start date: \", self.dates[self.dates >= self.hparams.start_test].iloc[0].strftime(\"%Y-%m-%d\"), \n",
    "              \", test end date: \", self.dates[self.dates >= self.hparams.start_test].iloc[-1].strftime(\"%Y-%m-%d\"))\n",
    "              \n",
    "    def example(self):\n",
    "        \"\"\"Returns a random training example.\"\"\"        \n",
    "        idx = np.random.randint(0, len(self.X_train))\n",
    "        x, y = self.X_train[idx], self.y_train[idx]\n",
    "        return (x, y)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        dataset = TensorDataset(self.X_train, self.y_train)\n",
    "        return DataLoader(dataset, batch_size=self.batch_size,\n",
    "                         num_workers=4,\n",
    "                         pin_memory=True,\n",
    "                         )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        dataset = TensorDataset(self.X_val, self.y_val)\n",
    "        return DataLoader(dataset, batch_size=self.batch_size,\n",
    "                         num_workers=4,\n",
    "                         pin_memory=True,\n",
    "                         )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        dataset = TensorDataset(self.X_test, self.y_test)\n",
    "        return DataLoader(dataset, batch_size=self.batch_size,\n",
    "                         num_workers=4,\n",
    "                         pin_memory=True,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f312eb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.empty(3).random_(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c562f306",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                num_classes,\n",
    "                sample_weight,\n",
    "                input_dim,\n",
    "                hidden_dim,\n",
    "                learning_rate,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters() #init variables are saved, so that model can be reloaded cleanly if necessary\n",
    "        \n",
    "        #model\n",
    "        self.l1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.l2 = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "        #sample weights\n",
    "        if self.hparams.sample_weight:\n",
    "            train_idx = dm.dates < dm.hparams.start_val\n",
    "            self.weight = len(dm.y[train_idx]) / dm.y[train_idx].unique(return_counts=True)[1]\n",
    "            self.weight = self.weight.cuda() #move to cuda, otherwise mismatch of devices in train/val\n",
    "        else:\n",
    "            self.weight = None\n",
    "        print(\"sample_weight:\", self.weight)\n",
    "        print(\"device of sample_weight:\", self.weight.device)\n",
    "        print(\"device of class:\", self.device)\n",
    "        \n",
    "        #metrics\n",
    "        self.train_acc = torchmetrics.Accuracy()\n",
    "        self.train_bal_acc = torchmetrics.Accuracy(num_classes=num_classes, average=\"macro\") #should be equal to sklearn bal. acc.\n",
    "        self.val_acc = torchmetrics.Accuracy()\n",
    "        self.val_bal_acc= torchmetrics.Accuracy(num_classes=num_classes, average=\"macro\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.l2(torch.relu(self.l1(x)))\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x) #logits\n",
    "        \n",
    "        loss = F.cross_entropy(y_hat, y, weight=self.weight)\n",
    "        self.log(\"loss/loss\", loss, on_step=True, on_epoch=False, prog_bar=True)\n",
    "        \n",
    "        self.train_acc(y_hat, y)\n",
    "        self.log(\"accuracy/train\", self.train_acc, on_step=False, on_epoch=True)\n",
    "        \n",
    "        self.train_bal_acc(y_hat, y)\n",
    "        self.log(\"bal_accuracy/train\", self.train_bal_acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x) #logits\n",
    "        \n",
    "#         self.log(\"hp_metric\", torch.mean(y_hat.argmax(dim=-1).float()).item(), prog_bar=True) # average prediction class\n",
    "        self.log(\"mean_pred\", torch.mean(y_hat.argmax(dim=-1).float()).item(), prog_bar=True)\n",
    "        \n",
    "        loss = F.cross_entropy(y_hat, y, weight=self.weight)\n",
    "        self.log(\"loss/val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        self.val_acc(y_hat, y)\n",
    "        self.log(\"accuracy/val\", self.val_acc, on_step=False, on_epoch=True)\n",
    "        \n",
    "        self.val_bal_acc(y_hat, y)\n",
    "        self.log(\"bal_accuracy/val\", self.val_bal_acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return {\"val_loss\": loss}\n",
    "    \n",
    "    def on_train_start(self):\n",
    "        self.st_total = time.time()\n",
    "\n",
    "    def on_train_epoch_start(self):\n",
    "        self.st = time.time()\n",
    "        self.steps = self.global_step\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        elapsed = time.time() - self.st\n",
    "        steps_done = self.global_step - self.steps\n",
    "        self.log(\"time/step\", elapsed / steps_done)\n",
    "\n",
    "    def on_train_end(self):\n",
    "        elapsed = time.time() - self.st_total\n",
    "        print(f\"Total Training Time: {time.strftime('%H:%M:%S', time.gmtime(elapsed))}\")\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y, weight=self.weight)\n",
    "\n",
    "        self.log(\"loss/test_loss\", loss, prog_bar=True)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eca11fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = FFN(2, 15, 100, 0.01)\n",
    "\n",
    "# test2 = MyDataModule(path_data, 512, \"2015\", \"2016\")\n",
    "\n",
    "# test.hparams\n",
    "\n",
    "# test.hparams\n",
    "\n",
    "# string = \"\"\n",
    "# for k, v in test2.hparams.items():\n",
    "#     string += k\n",
    "#     string += str(v)\n",
    "#     string += \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772ea4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "# datamodule params\n",
    "PATH = path_data\n",
    "BATCH_SIZE = 512\n",
    "START_VAL = \"2014\"\n",
    "START_TEST = \"2016\"\n",
    "LABEL_FN = \"binary\"\n",
    "DATASET = \"small\"\n",
    "\n",
    "# model params\n",
    "NUM_CLASSES = 2\n",
    "INPUT_DIM = 15 # number of input features: 15 or 172\n",
    "HIDDEN_DIM = 100 \n",
    "LEARNING_RATE = 1e-4\n",
    "SAMPLE_WEIGHT = True\n",
    "\n",
    "# trainer params\n",
    "MAX_EPOCHS = 100\n",
    "\n",
    "# Checks\n",
    "if (LABEL_FN == \"multi\"):\n",
    "    assert NUM_CLASSES > 2, \"number of classes must be bigger than 2 (LABEL_FN is 'multi')\"\n",
    "elif (LABEL_FN == \"binary\"):\n",
    "    assert NUM_CLASSES == 2, \"number of classes must be 2 (LABEL_FN is 'binary')\"\n",
    "if (DATASET == \"small\"):\n",
    "    assert INPUT_DIM == 15, \"input dim should be 15 as DATASET='small'\"\n",
    "elif (DATASET == \"big\"):\n",
    "    assert INPUT_DIM == 172, \"input dim should be 172 as DATASET='big'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a487dd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = MyDataModule(\n",
    "    dataset=DATASET,\n",
    "    path=PATH, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    start_val=START_VAL, \n",
    "    start_test=START_TEST,\n",
    "    label_fn=LABEL_FN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9225a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FFN(\n",
    "    num_classes=NUM_CLASSES,\n",
    "    sample_weight=SAMPLE_WEIGHT,\n",
    "    input_dim=INPUT_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    learning_rate=LEARNING_RATE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ac0aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_foldername(to_add: dict ={}, to_exclude: list = []):\n",
    "    name = \"\"\n",
    "    for k, v in to_add.items():\n",
    "        if k not in to_exclude:\n",
    "            name += k\n",
    "            name += str(v)\n",
    "            name += \".\"\n",
    "    for k, v in model.hparams.items():\n",
    "        if k not in to_exclude:\n",
    "            name += k\n",
    "            name += str(v)\n",
    "            name += \".\"\n",
    "    for k, v in dm.hparams.items():\n",
    "        if k not in to_exclude:\n",
    "            name += k\n",
    "            name += str(v)\n",
    "            name += \".\"\n",
    "    return name\n",
    "\n",
    "to_add = {\"max_epochs\": MAX_EPOCHS}\n",
    "to_exclude = [\"path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa76cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logging directory\n",
    "log_dir = \"logs\"\n",
    "name = log_foldername(to_add=to_add, to_exclude=to_exclude)\n",
    "version = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "# log_dir = os.path.join(log_dir, tag, datetime.now().strftime(\"%Y%m%d%H%M%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445d3622",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = pl.loggers.TensorBoardLogger(\n",
    "    save_dir= log_dir,\n",
    "    name = name,\n",
    "    version = version\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13d0d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_callback = EarlyStopping(monitor=\"loss/val_loss\", mode=\"min\", patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef1894a",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"loss/val_loss\",\n",
    "    save_top_k= 1,\n",
    "    mode= \"min\",\n",
    "    filename='epoch={epoch}-val_loss={loss/val_loss:.2f}-val_bacc={bal_accuracy/val:.2f}',\n",
    "    auto_insert_metric_name=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159662dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    gpus=1,\n",
    "    logger=logger, #=logger or False\n",
    "    check_val_every_n_epoch=1,\n",
    "    callbacks=[early_stop_callback, checkpoint_callback], #early stop depends earliest after (patience*check_val_every_n_epoch)\n",
    "    # enable_checkpointing = False,\n",
    "    num_sanity_val_steps=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002d0fe8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "s_time = time.time()\n",
    "trainer.fit(model, datamodule=dm)\n",
    "e_time = time.time()\n",
    "print(f\"Time to fit: {divmod(e_time - s_time, 60)[0]:2.0f}:{divmod(e_time - s_time, 60)[1]:2.0f}\\\n",
    " min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41541b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorboard as tb\n",
    "# tb.notebook.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57f851b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reload_ext tensorboard\n",
    "# %tensorboard --logdir=logs/lightning_logs/ --port=6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec9b594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# dm.X_train.shape\n",
    "\n",
    "# dm.X_train\n",
    "\n",
    "# torch.mean(dm.X_train, axis=0)\n",
    "\n",
    "# np.mean(dm.X_train.numpy(), axis=0)\n",
    "\n",
    "# np.std(dm.X_train.numpy(), axis=0)\n",
    "\n",
    "# np.mean(dm.X_train.numpy(), axis=0)\n",
    "\n",
    "# torch.mean(dm.X_train, axis=0)\n",
    "\n",
    "# np.std(dm.X_train.numpy(), axis=0)\n",
    "\n",
    "# torch.std(dm.X_train, axis=0)\n",
    "\n",
    "# (dm.X_val - torch.mean(dm.X_train, axis=0)) / torch.std(dm.X_train, axis=0)\n",
    "\n",
    "# (dm.X_val.numpy() - np.mean(dm.X_train.numpy(), axis=0)) / np.std(dm.X_train.numpy(), axis=0)\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# scaler.fit_transform(dm.X_train.numpy())\n",
    "\n",
    "# scaler.transform(dm.X_val.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd948eb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacc11d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4cd733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def standardize(x : torch.Tensor):\n",
    "#     x = (x.numpy() - np.mean(x.numpy(), axis=0)) / np.std(x.numpy(), axis=0)\n",
    "#     return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c1fb7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2437c06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e96a104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score #equal to torchmetrics.accuracy(average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c60cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "(model(dm.X_val).softmax(dim=1)).argmax(dim=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b00c75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.cross_entropy(model(dm.X_val), dm.y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76f194f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(dm.y_val.numpy(), model(dm.X_val).argmax(dim=1).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff0b7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_accuracy_score(dm.y_val.numpy(), model(dm.X_val).argmax(dim=1).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2272cb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torchmetrics.functional.accuracy(model(dm.X_val), dm.y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b07435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torchmetrics.functional.accuracy(model(dm.X_val), dm.y_val, average=\"macro\", num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb03091",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import ConfusionMatrix\n",
    "confmat = ConfusionMatrix(num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dd5486",
   "metadata": {},
   "outputs": [],
   "source": [
    "confmat(model(dm.X_val), dm.y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777ef9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.y_val.unique(return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48d035e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(dm.X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a97226c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bb6938",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dm.y_val.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c30243b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(dm.X_val).argmax(dim=1).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d6b93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.randint(4, (10, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37b6483",
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f2deaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torch.randn(10, 4) * 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deb3e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(preds).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37191fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.numpy().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac7877c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67b90ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torchmetrics.functional.accuracy(preds, target, num_classes=4, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699d67fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2d10fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_accuracy_score(target, preds.argmax(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6936b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = torchmetrics.functional.accuracy(preds, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2665553c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [{'loss/test_loss': 0.6668015718460083}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae297807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_callback.best_model_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "137ad5de30c222602b906d427f317b23725154a9d2ac1dd9f95e9d3b5697fcc3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
